*数据、样本不均衡**等处理方法

#### **样本不平衡的处理方法**

- 欠采样 - 随机删除观测数量足够多的类，使得两个类别间的相对比例是显著的。虽然这种方法使用起来非常简单，但很有可能被我们删除了的数据包含着预测类的重要信息。

- 过采样 - 对于不平衡的类别，我们使用拷贝现有样本的方法随机增加观测数量。理想情况下这种方法给了我们足够的样本数，但过采样可能导致过拟合训练数据。

- 合成采样（ SMOTE ）-该技术要求我们用合成方法得到不平衡类别的观测，该技术与现有的使用最近邻分类方法很类似。问题在于当一个类别的观测数量极度稀少时该怎么做。比如说，我们想用图片分类问题确定一个稀有物种，但我们可能只有一幅这个稀有物种的图片。

- 在loss方面，采用focal loss等loss进行控制不平衡样本。

  **不平衡类别会造成问题有两个主要原因**：

  - 对于不平衡类别，我们不能得到实时的最优结果，因为模型/算法从来没有充分地考察隐含类。
- 它对验证和测试样本的获取造成了一个问题，因为在一些类观测极少的情况下，很难在类中有代表性。

#### **数据不平衡**

​     翻转，旋转，缩放,裁剪，平移，添加噪声，有监督裁剪，mixup，上下采样，增加不同惩罚

​    主要是为了解决图像细节不足问题（增强特征提取骨干网络的表达能力）

#### **过拟合的解决方法**

- 数据扩充/数据增强/更换小网络/正则化/dropout/batch normalization
- 增加训练数据、减小模型复杂度、正则化,L1/L2正则化、集成学习、早期停止
- 通过特征选择，剔除一些不重要的特征，从而降低模型复杂度

#### **什么是过拟合**

​    过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。

#### **产生过拟合根本原因**

​    观察值与真实值存在偏差, 训练数据不足，数据太少，导致无法描述问题的真实分布, 数据有噪声, 训练模型过度，导致模型非常复杂。

#### **什么是欠拟合**

​    训练的模型在训练集上面的表现很差，在验证集上面的表现也很差

​    原因：训练的模型太简单，最通用的特征模型都没有学习到

####  **K折交叉验证（k-fold cross validation）具体是怎么做的**

​    K折交叉验证用于模型调优，所有的数据都被用来训练，会导致过拟合，K折交叉验证可以缓解过拟合。将数据分为k组，每次从训练集中，抽取出k份中的一份数据作为验证集，剩余数据作为训练集。测试结果采用k组数据的平均值。

#### 如何处理数据中的缺失值

​    可以分为以下 2 种情况：

- **缺失值较多**：直接舍弃该列特征，否则可能会带来较大噪声，从而对结果造成不良影响。

- 缺失值较少

  当缺失值较少（< 10%）时，可以考虑对缺失值进行填充，有几下几种填充策略：

  - 用一个**异常值**填充（比如 0 ），缺失值作为一个特征处理：`data.fillna(0)`
  - 用**均值|条件均值**填充：`data.fillna(data.mean())`
  - 用**相邻数据**填充：`data.fillna(method='pad')`，`data.fillna(method='bfill') `
  - **插值**：`data.interpolate()`
  - **拟合**：简单来说，就是将缺失值也作为一个预测问题来处理：将数据分为正常数据和缺失数据，对有值的数据采用`随机森林`等方法拟合，然后对有缺失值的数据进行预测，用预测的值来填充

### 常用的指标

#### 混淆矩阵与 PR

混淆矩阵表格如下：

| 名称                           | 定义                                      |
| ------------------------------ | ----------------------------------------- |
| True Positive(真正例, `TP`)    | 将正类预测为正类数                        |
| True Negative(真负例, `TN`)    | 将负类预测为负类数                        |
| False Positive(假正例, `FP`)   | 将负类预测为正类数 → 误报 (Type I error)  |
| False Negative(假负例子, `FN`) | 将正类预测为负类数 → 漏报 (Type II error) |

​    查准率 `P` 与查全率 `R` 的计算公式如下：

- 查准率（准确率）`P = TP/(TP+FP)`
- 查全率（召回率）`R = TP/(TP+FN)`

​    **准确率描述了模型有多准**，即在预测为正例的结果中，有多少是真正例；**召回率则描述了模型有多全**，即在为真的样本中，有多少被我们的模型预测为正例。

#### 均方误差和方差、标准差

​    **均方误差（MSE，mean squared error）与均方根误差(RMSE)**：

​    均方误差是预测值与真实值之差的平方和的平均值，即误差平方和的平均数。计算公式形式上接近方差，它的开方叫均方根误差 `RMSE`，均方根误差才和标准差形式上接近。 计算公式如下： $$\frac{1}{n} \sum_{i=1}^{n}[f(x_i)-y_i]^2$$ 在机器学习中均方误差常用作**预测和回归问题的损失函数**，均方误差越小，说明模型预测的越准确，反之则越不准确。

​    **方差(variance)与标准差：**

​    方差是在概率论和统计学中衡量随机变量或一组数据时离散程度的度量，在统计描述和概率分布中各有不同的定义，并有不同的公式。概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。**统计中的方差（样本方差）是样本实际值与实际值的总体平均值之差的平方和的平均值**，即将各个误差之平方（而非取绝对值，使之肯定为正数）相加之后再除以总数。**总体方差**计算公式如下： $$\sigma ^2 = \frac{\sum_{i=1}^{N}(X_{i}-\mu)^2}{N}$$ 公式解析：

1. 因为和样本数无关，所以分母为样本数
2. 累加每个值和均值差值的平方，对应于每个值相对于均值的偏差，对应于离散程度，平方是对离散程度的加剧，同时能让差值总为正数，以符合偏差的概念意义
3. $\sigma$ 的平方表示总体方差，$X$ 表示变量，$\mu $ 表示总体的均值，$N$ 表示总体样本数量。

​    由于方差是数据的平方，与检测值本身相差太大，难以直观的衡量，所以常用方差开根号换算回来，就成了标准差（Standard Deviation）用$\sigma$ 表示，标准差计算公式如下： $$\sigma = \sqrt{\frac{\sum_{i=1}^{N}(X_{i}-\mu)^2}{N}}$$ **3，样本方差**

​    在实际项目中，总体均值难以得到时，应用样本统计量替代总体参数，经校正后，样本方差的计算公式如下：

> 样本方差是指总体各单位变量值与其算术平均数的离差平方的平均数。样本方差的意义是用来估计总体方差（统计术语：样本方差是对总体方差的无偏估计）。

$$\sigma ^2 = \frac{\sum_{i=1}^{n-1}(X_{i}-\overline{x_{i}..x_{n}})^2}{n-1}$$ $\overline{x_{i}..x_{n}}$ 表示样本均值公式分母由总体方差的 `N` 变为了 `n-1`，使得样本方差更能反映总体方差。

#### 如何处理数据中的缺失值

​    可以分为以下 2 种情况：

- **缺失值较多**：直接舍弃该列特征，否则可能会带来较大噪声，从而对结果造成不良影响。

- 缺失值较少

  ：当缺失值较少（

  ```
  < 10%
  ```

  ）时，可以考虑对缺失值进行填充，有几下几种填充策略：

  - 用一个**异常值**填充（比如 0 ），缺失值作为一个特征处理：`data.fillna(0)`
  - 用**均值|条件均值**填充：`data.fillna(data.mean())`
  - 用**相邻数据**填充：`data.fillna(method='pad')`，`data.fillna(method='bfill')`
  - **插值**：`data.interpolate()`
  - **拟合**：简单来说，就是将缺失值也作为一个预测问题来处理：将数据分为正常数据和缺失数据，对有值的数据采用`随机森林`等方法拟合，然后对有缺失值的数据进行预测，用预测的值来填充

#### **AuC，RoC，mAP，Recall，Precision，F1-score**

- 召回率(Recall) = 预测为真实正例 / 所有真实正例样本的个数。

- 准确率(Precision) =预测为真实正例 / 所有被预测为正例样本的个数。

- **其中，Precision：P=TP/(TP+FP) 精准率（查准率），Recall：R=TP/(TP+FN) 召回率（查全率 ）**

- mAP: mean Average Precision, 即各类别AP的平均值，AP: PR曲线下面积，后文会详细讲解，PR曲线: Precision-Recall曲线。

- ROC：全称Receiver Operating Characteristic曲线，常用于评价二分类的优劣。

- AUC：全称Area Under Curve，被定义为ROC曲线下的面积，取值范围在0.5到1之间。

- F1-score：F1值，又称调和平均数，公式(2)和(3)中反应的precision和recall是相互矛盾的，
- 当recall越大时，预测的覆盖率越高，这样precision就会越小，反之亦然，通常，使用F1-score来调和precision和recall。

$$
F 1=\frac{2 *\text { precision }* \text { recall }}{\text { precision }+\text { recall }}
$$

#### 偏差和方差

​    模型误差 = 偏差(Bias) + 方差(Variance) + 不可避免的误差

+ 偏差：
  + 描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据。
+ 方差：
  + 描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散。

#### Bias(偏差)，Error(误差)和Varience(方差)

> [机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？](https://www.zhihu.com/question/27068705)

​    统计领域为我们提供了很多工具来实现机器学习目标，不仅可以解决训练集上的任务，还可以泛化。偏差-方差指标方法是试图对学习算法(模型)的期望泛化错误率进行拆解。`Error = Bias +    Varience`：

- `Error`: 反映的是整个模型的准确度
- `Bias`: 反映的是模型在**样本上的输出与真实值之间的误差**，即模型的准确性，以打靶事件为例，`low bias`，一般就得复杂化模型，表现出来就是点都打在靶心中间，但这样容易过拟合 (overfitting)，过拟合对应下图是 high variance，点很分散。
- `Varience`: 反映的是模型每一次输出的结果与模型输出期望之间的误差，即模型的稳定性，是训练集上训练出来的模型在测试集上的表现，同样以打靶事件为例，`low variance` 对应就是点都打的很集中，但不一定是靶心附近，手很稳，但是瞄的不准。

[![偏差误差与方差](https://github.com/HarleysZhang/2020_algorithm_intern_information/raw/master/images/%E5%81%8F%E5%B7%AE%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE.png)](https://github.com/HarleysZhang/2020_algorithm_intern_information/blob/master/images/偏差误差与方差.png)

#### 偏差与方差公式

​    对测试样本 `x`, 令 $y_{D}$ 为 x 在数据集中的标记，y 为 x 的真实标记， `f(x;D)` 为在训练集 D 上学习到的模型 f 在 x 上的预测输出。

- 训练过程中期望输出与真实标记（标签）的差别称为偏差（`bias`）：$bias^{2}(x) = (\bar{f} - y)^{2}$
- （交叉验证训练模型）使用样本数相同不同训练集训练出来的模型在测试集上产生的`方差`为： $var(x) = E_{D}[(f(x;D) - \bar{f})^{2}] $

#### 导致偏差和方差的原因

> [机器学习中的Bias(偏差)，Error(误差)，和Variance(方差)有什么区别和联系？](https://www.zhihu.com/question/27068705)

- 偏差通常是由于我们对学习算法做了错误的假设，或者模型的复杂度不够；
  - 比如真实模型是一个二次函数，而我们假设模型为一次函数，这就会导致偏差的增大（欠拟合）；
  - 由偏差引起的误差通常在训练误差上就能体现，或者说训练误差主要是由偏差造成的
- 方差通常是由于模型的复杂度相对于训练集过高导致的；
  - 比如真实模型是一个简单的二次函数，而我们假设模型是一个高次函数，这就会导致方差的增大（过拟合）；
  - 由方差引起的误差通常体现在测试误差相对训练误差的增量上。

#### 深度学习中的偏差与方差

- 神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；
- 但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；
- 深度学习的核心工作之一就是**研究如何降低模型的泛化误差**，这类方法统称为`正则化方法`。

#### 交叉验证

​    在训练数据上面，我们可以进行·交叉验证(Cross-Validation)·。一种方法叫做 ·`K-fold Cross Validation` ( K 折交叉验证), K 折交叉验证，初始采样分割成 K 个子样本，一个单独的子样本被保留作为验证模型的数据，其他 K-1 个样本用来训练。交叉验证重复 K 次，每个子样本验证一次，平均 `K` 次的结果或者使用其它结合方式，最终得到一个单一估测。

- 当 `K` 值大的时候， 我们会有更少的 `Bias`(偏差), 更多的 `Variance`。
- 当 `K` 值小的时候， 我们会有更多的 `Bias`(偏差), 更少的 `Variance`。

交叉验证是机器学习当中的概念，一般深度学习不会使用交叉验证方法，原因是深度学习的数据集一般都很大，但是也有例外，`Kaggle` 的一些医疗类比赛，训练集一般只有几千张，由于训练数据很少，用来作为验证集的数据会非常少，因此训练的模型其验证分数可能会有很大波动，直接取决于我们所选择的验证集和训练集，也就是说，验证集的划分方式可能会造成验证分数上存在较大**方差**，无法对模型进行有效评估，同时也无法进行有效的超参数调整（`batch` 设置多少模型最佳收敛）。

​    也有替代方法允许我们使用所有的样本估计平均测试误差，代价是增加了计算量。这些过程是基于在原始数据上随机采样或分离出的不同数据集上重复训练和测试的想法，最常见的是 k-折交叉验证，这种方法将可用训练集花费为 `K` 个分区（一般取 `4`、`5`或者`10`），实例化训练 `k` 个相同模型，将每个模型在 `k-1` 个分区上训练，并在剩下的一个分区做验证，模型的验证分数等于 `k` 个验证分数的平均值。k-折交叉验证的训练集划分方式如下图所示：

[![image](https://user-images.githubusercontent.com/37138671/114163327-a0dfe200-995c-11eb-9a0e-4ce4fe13150c.png)](https://user-images.githubusercontent.com/37138671/114163327-a0dfe200-995c-11eb-9a0e-4ce4fe13150c.png)

​    `k` 折交叉验证的代码实现可以参考《Python深度学习》第三章，在模型训练好后，可通过计算所有 `Epoch` 的 `K` 折验证分数的平均值，并绘制每轮的模型验证指标变化曲线，观察哪个 `Epoch` 后模型不再收敛，从而完成模型调参工作。同时，`K` 折交叉验证方式训练模型会得到 `K`个模型，将这个 `K` 个模型在测试集上的推理结果取平均值或者投票，也是一种 `Ensemble` 方式，可以增强模型泛化性，防止过拟合。

```
# 计算所有轮次中的 K 折验证分数平均值
average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]
```

### **正则化**

#### **正则化的原理**

​    机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作 l1-norm 和l2-norm，中文称作 L1正则化 和 L2正则化，或者 L1范数 和 L2范数。

#### **正则化有哪几种，分别有什么作用？**

​    L0 范数和 L1 范数都能够达到使参数稀疏的目的，但 L0 范数更难优化求解，L1 范数是 L0 范数的最优凸近似，而且它比 L0 范数要容易优化求解。

​    L2 范数不但可以防止过拟合，提高模型的泛化能力，还可以让我们的优化求解变得稳定和快速。L2 范数对大数和 outlier 更敏感。

####  **L0、L1、L2正则化**

+ L0 范数：向量中非0元素的个数。
+ L1 范数 (Lasso Regularization)：向量中各个元素绝对值的和。
+ L2 范数(Ridge Regression)：向量中各元素平方和再求平方根。

#### L0、L1、L2范式

​    一般来说，监督学习可以看做最小化下面的目标函数：

![image-20210705203415456](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705203415456.png)

​    其中，第一项L(yi,f(xi;w)) 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。因为我们的模型是要拟合我们的训练样本的嘛，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但正如上面说言，我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。

​    **正则函数部分**：规则化函数Ω(w)也有很多种选择，一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。然而，不同的选择对参数w的约束不同，取得的效果也不同，但我们在论文中常见的都聚集在：L0范数，L1范数，L2范数，迹范数，Frobenius范数，核范数等。

- L0范数 

​    L0范数是指向量中非0的元素的个数。如果用L0规则化一个参数矩阵W，就是**希望W中大部分元素是零，实现稀疏**。

​    L0范数的应用：

​    1）特征选择：实现特征的自动选择，去除无用特征。*稀疏化可以去掉这些无用特征，将特征对应的权重置为零*。

​    2）可解释性（interpretability）：例如判断某种病的患病率时，最初有1000个特征，建模后参数经过稀疏化，最终只有5个特征的参数是非零的，那么就可以说影响患病率的主要就是这5个特征。

- L1范数

​    L1范数是指向量中各个元素绝对值之和。L1范数也可以实现稀疏，通过将无用特征对应的参数W置为零实现。

​    L1范数：

![image-20210705203454182](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705203454182.png)

​    既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。

- L2范数

​    L2范数是指向量各元素的平方和然后求平方根。，用在回归模型中也称为岭回归（Ridge regression）。

​    L2范数：

![image-20210705203512744](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705203512744.png)

​    L2避免过拟合的原理是：让L2范数的规则项

![image-20210705203520353](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705203520353.png)

​    尽可能小，可以使得W每个元素都很小，接近于零，但是与L1不同的是，不会等于0；这样得到的模型**抗干扰能力强**，参数很小时，即使样本数据x发生很大的变化，模型预测值y的变化也会很有限。

​    L2范数的作用：1）学习理论的角度：从学习理论的角度来说，**L2范数可以防止过拟合，提升模型的泛化能力**。2）优化计算的角度：从优化或者数值计算的角度来说，**L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。**

#### L1 loss

​    L1 我理解成 1 维向量的距离。使用L1损失函数也被叫做最小化绝对误差(Least Abosulote Error)。这 个名称非常的形象。LAE 就是最小化真实值和预测值 之间差值 的绝对值的和。

![image-20210705203806959](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705203806959.png)

#### L2 loss

​    L2 就是二维空间向量的距离。

![image-20210705203831075](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705203831075.png)

#### smooth L1 Loss

![image-20210705203845322](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705203845322.png)

**比较：**

1. L1 loss 在零点不平滑，用的较少；L1loss在零点不平滑，学习慢；
2. Smooth L1 Loss 改善了零点不平滑问题。

​    smooth L1 loss和L1 loss函数的区别在于，L1 loss在0点处导数不唯一，可能影响收敛。smooth L1 loss的解决办法是在0点附近使用平方函数使得它更加平滑。

​    smooth L1 loss让loss function对于离群点更加鲁棒，即：相比于L2损失函数，其对离群点/异常值（outlier）不敏感，梯度变化相对更小，训练时不容易跑飞。

​    Smooth L1 Loss 相比L1修改零点不平滑问题，而且在x较大的时候不像L2对异常值敏感，是一个缓慢变化的loss；

3. L2 loss：对离群点比较敏感，如果feature 是 unbounded的话，需要好好调整学习率，防止出现梯度爆炸的情况。

​    L2loss学习快，因为是平方增长，但是当预测值太大的时候，会在loss中占据主导位置(如真实值为1，预测多次，有一次预测值为100，其余预测为2)。

4. 一般来说，L1正则会制造稀疏的特征，大部分无用特征的权重会被置为0。L2正则会让特征的权重不过大，使得特征的权重比较平均。

#### **L1正则化和L2正则化的作用**

​    L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。

​    L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合。

#### **L1、L2范数，L1趋向于0，但L2不会，为什么？**

​     L1范数更容易产生稀疏的权重，L2范数更容易产生分散的权重

#### **L1、L2正则化区别，为什么稀疏的解好？**

​    L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。

#### **实现参数的稀疏有什么好处吗？**

​    一个好处是可以简化模型，避免过拟合。另一个好处是参数变少可以使整个模型获得更好的可解释性。

### 特征归一化

#### **什么是特征归一化**

​    数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上。

#### **特征归一化的作用**

​     数据归一化后 更容易正确的收敛到最优解、提升模型的精度归一化的另一好处是提高精度、深度学习中数据归一化可以防止模型梯度爆炸。

#### 机器学习项目流程

1. 数学抽象
2. 数据获取
3. 预处理与特征选择
4. 模型训练与调优
5. 模型诊断
6. 模型融合/集成
7. 上线运行

#### 数据清洗与特征处理

> [机器学习中的数据清洗与特征处理综述](https://tech.meituan.com/2015/02/10/machinelearning-data-feature-process.html) 美团的这篇综述文章总结得不错，虽然缺少实例不容易直观理解，但是对于我这个初学者来说也足够了，本章内容几乎都来自于参考文章。

**清洗标注数据**

​    清洗标注数据的方法，主要是是数据采样和样本过滤。

- `数据采样`：对于分类问题：选取正例，负例。对于回归问题，需要采集数据。对于采样得到的文本，根据需要设定样本权重，当模型不能使用全部的数据来训练时，需要对数据进行采样，设定一定的采样率。采样的方法包括随机采样，固定比例采样等方法。
- 样本过滤：1.结合业务情况进行数据的过滤，例如去除crawler抓取，spam，作弊等数据。 - 2.异常点检测，采用异常点检测算法对样本进行分析，常用的异常点检测算法包括 - 偏差检测，例如聚类，最近邻等。

**特征分类**

​    根据不同的分类方法，可以将特征分为：

- `Low level` 特征和 `High level` 特征
- 稳定特征与动态特征。
- 二值特征、连续特征、枚举特征

​    `Low level` 特征是较低级别的特征，主要是原始特征，不需要或者需要很少的人工处理和干预，例如文本中的词向量特征，图像特征中的像素点大小，用户 `id`，商品 id等。High level 特征是经过比较复杂的处理，结合部分业务逻辑或者规则、模型得到的特征，例如人工打分，模型打分等特征，可以用于较复杂的非线性模型。Low level 比较针对性，覆盖面小。长尾样本的预测值主要受 high level 特征影响。 高频样本的预测值主要受 low level 特征影响。

​    `稳定特征` 是变化频率较少的特征，例如评价平均分，团购单价价格等，在较长时间段内数值都不会发生变化。动态特征是更新变化比较频繁的特征，有些甚至是实时计算得到的特征，例如距离特征，2 小时销量等特征。或者叫做实时特征和非实时特征。针对两类特征的不同可以针对性地设计特征存储和更新方式，例如对于稳定特征，可以建入索引，较长时间更新一次，如果做缓存的话，缓存的时间可以较长。对于动态特征，需要实时计算或者准实时地更新数据，如果做缓存的话，缓存过期时间需要设置的较短。

​    `二值特征主要是 0/1 特征`，即特征只取两种值：`0 或者 1`，例如`用户 id 特征`：目前的 id 是否是某个特定的 id，`词向量特征`：某个特定的词是否在文章中出现等等。连续值特征是取值为有理数的特征，特征取值个数不定，例如距离特征，特征取值为是0~正无穷。枚举值特征主要是特征有固定个数个可能值，例如今天周几，只有7个可能值：周1，周2，…，周日。在实际的使用中，我们可能对不同类型的特征进行转换，例如将枚举特征或者连续特征处理为二值特征。枚举特征处理为二值特征技巧：将枚举特征映射为多个特征，每个特征对应一个特定枚举值，例如今天周几，可以把它转换成7个二元特征：今天是否是周一，今天是否是周二，…，今天是否是周日。连续值处理为二值特征方法：先将连续值离散化（后面会介绍如何离散化)，再将离散化后的特征切分为N个二元特征，每个特征代表是否在这个区间内。

**特征处理与分析**

​    对特征进行分类后，需要对特征进行处理，`常用的特征处理方法`如下：

- 特征归一化，离散化，缺省值处理
- 特征降维方法
- 特征选择方法

​    **特征归一化**。在有些算法中，例如线性模型或者距离相关的模型（聚类模型、knn 模型等），特征值的取值范围会对最终的结果产生较大影响，例如输入数据有两种不同的特征，其中的二元特征取值范围 `[0, 1]`，而距离特征取值可能是 [0，正无穷]，两种特征取值范围不一致，导致模型可能会偏向于取值范围较大额特征，为了平衡取值范围不一致的特征，需要对特征进行归一化处理，将特征值取值归一化到 [0,1] 区间，常用的归一化方法包括：

1. `函数归一化`，通过映射函数将特征取值映射到［0，1］区间，例如最大最小值归一化方法，是一种线性的映射。还有通过非线性函数的映射，例如 `log` 函数等。
2. `分维度归一化`，可以使用最大最小归一化方法，但是最大最小值选取的是所属类别的最大最小值，即使用的是局部最大最小值，不是全局的最大最小值。
3. `排序归一化`，不管原来的特征取值是什么样的，将特征按大小排序，根据特征所对应的序给予一个新的值。

​    **离散化**。在上面介绍过连续值的取值空间可能是无穷的，为了便于表示和在模型中处理，需要对连续值特征进行离散化处理。常用的离散化方法包括等值划分和等量划分。

1. `等值划分`，是将特征按照值域进行均分，每一段内的取值等同处理。例如某个特征的取值范围为 [0，10]，我们可以将其划分为10段，[0，1)，[1，2)，…，[9，10)。
2. `等量划分`，是根据样本总数进行均分，每段等量个样本划分为 1 段。例如距离特征，取值范围［0，3000000］，现在需要切分成 10 段，如果按照等比例划分的话，会发现绝大部分样本都在第 1 段中。使用等量划分就会避免这种问题，最终可能的切分是[0，100)，[100，300)，[300，500)，..，[10000，3000000]，前面的区间划分比较密，后面的比较稀疏。

​    **缺省值处理**。有些特征可能因为无法采样或者没有观测值而缺失，例如距离特征，用户可能禁止获取地理位置或者获取地理位置失败，此时需要对这些特征做特殊的处理，赋予一个缺省值。缺省值如何赋予，也有很多种方法。例如`单独表示，众数，平均值等`。

#### 深入理解Batch Normalization批标准化

​    机器学习领域有个很重要的假设：**IID独立同分布假设**，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？**BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。**

​    BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的**激活输入值**（就是那个x=WU+B，U是输入）**随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近**（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这**导致反向传播时低层神经网络的梯度消失**，这是训练深层神经网络收敛越来越慢的**本质原因**，**而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布**，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是**这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。**

​    BN在训练的时候可以根据Mini-Batch里的若干训练实例进行激活数值调整，但是在推理（inference）的过程中，从所有训练实例中获得的统计量来代替Mini-Batch里面m个训练实例获得的均值和方差统计量。

​    BatchNorm为什么NB呢，关键还是效果好。**①不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。** 总而言之，经过这么简单的变换，带来的好处多得很，这也是为何现在BN这么快流行起来的原因。

### 回归

#### **分类和回归的区别**

   定量输出称为回归，或者说是连续变量预测；

   定性输出称为分类，或者说是离散变量预测。

#### **各举例3个模型**

​    **常见分类模型**有感知机、朴素贝叶斯、逻辑回归(LR)、支持向量机(SVM)、神经网络等；

​    **常见回归模型**有线性回归、多项式回归、岭回归（L2正则化）、Lasso回归（L1正则化）等

#### **线性回归和逻辑回归的区别**

**线性回归**：利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。一元线性回归分析：y=ax+by=ax+b，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示。多元线性回归分析：hθ(x)=θ0+θ1x1+...+θnxn，包括两个或两个以上的自变量，并且因变量和自变量是线性关系。

**逻辑回归**：逻辑回归（Logistic Regression）是用于处理因变量为分类变量的回归问题，常见的是二分类或二项分布问题，也可以处理多分类问题，它实际上是属于一种分类方法。

![image-20210619103116071](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210619103116071.png)

**区别**：LR通常用于二分类，使用的是交叉熵损失函数；线性回归用于回归，使用的是均方误差损失函数

#### **怎么优化LR？求解LR?**

梯度下降、极大似然法。

#### Sigmoid与softmax的区别

​    **sigmoid**常用于**二元分类**，将二元输入映射成0和1。

![image-20210705204003487](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705204003487.png)

![image-20210705204014607](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705204014607.png)

​    其实logistic函数也就是经常说的sigmoid函数，它的几何形状也就是一条sigmoid曲线（S型曲线）。A logistic function or logistic curve is a common “S” shape (sigmoid curve). 也就是说，sigmoid把一个值映射到0-1之间。

​    该函数具有如下的特性：当x趋近于负无穷时，y趋近于0；当x趋近于正无穷时，y趋近于1；当x= 0时，y=0.5.

​    **优点：** 1.Sigmoid函数的输出映射在(0,1)之间，单调连续，输出范围有限，优化稳定，可以用作输出层。2.求导容易，处处可导，导数为：f′(x)=f(x)(1−f(x))

​    **缺点：** 1.由于其软饱和性，容易产生梯度消失，导致训练出现问题。2.其输出并不是以0为中心的。

​    **Softmax函数**

![image-20210705204056561](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705204056561.png)

​    Softmax 函数：将激活值与所有神经元的输出值联系在一起，所有神经元的激活值加起来为1。第L层（最后一层）的第j个神经元的激活输出为：

![image-20210706210521043](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210706210521043.png)

```python
def softmax(x):
    shift_x = x - np.max(x)#防止输入增大时输出为nan
    exp_x = np.exp(shift_x)
    return exp_x / np.sum(exp_x)
```



### **SVM—支持向量机**

​    支持向量机（support vector machines, SVM）是一种**二分类**模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；

​    SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。

​    SVM的的学习算法就是求解凸二次规划的最优化算法。

#### **SVM的原理**

​    SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。

​																						**w⋅x+b=0**

​    即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

#### 支持向量机简述

- 支持向量机（Support Vector Machines, SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使其成为实质上的非线性分类器。
- `SVM` 的学习策略是找到最大间隔（两个异类支持向量到超平面的距离之和 $\gamma = \frac{2}{||w}$ 称为“间隔”），可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。
- `SVM` 的最优化算法是求解凸二次规划的最优化算法。

####  **支持向量回归原理(SVR)**

SVR（支持向量回归）是SVM（支持向量机）中的一个重要的应用分支。SVR回归与SVM分类的区别在于，SVR的样本点最终只有一类，它所寻求的最优超平面不是SVM那样使两类或多类样本点分的“最开”，而是使所有的样本点离着超平面的总偏差最小。

#### 拉格朗日乘子法和 KKT 条件

为了方便和好记，就把原来的优化问题写成f(x,y)+λh(x,y)的形式，然后分别对λ,x,y求偏导，并且令偏导为0就行了，和之前得到的方程组是一样的。这种方法叫**拉格朗日乘数法**。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqpYZMyUibVlvicdO6NKUAIws6kkK9rGibicSPjl7T6iaK40xhG1qF6ZWLXgrvgL9ibhEicCUeiaQQjoW4kzw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​    **这个就是 KKT 条件。** 它的含义是这个优化问题的极值点一定满足这组方程组。（不是极值点也可能会满足，但是不会存在某个极值点不满足的情况）它也是原来的优化问题取得极值的必要条件，解出来了极值点之后还是要代入验证的。但是因为约束比较多，情况比较复杂，KKT 条件并不是对于任何情况都是满足的。要满足 KKT条件需要有一些规范性条件（Regularity conditions），就是要求约束条件的质量不能太差，常见的比如：

1. LCQ：如果 h(x)和g(x)都是形如Ax+b的仿射函数，那么极值一定满足 KKT 条件。
2. LICQ：起作用的g(x)函数（即g(x)相当于等式约束的情况）和h(x)函数在极值点处的梯度要线性无关，那么极值一定满足 KKT 条件。
3. Slater条件：如果优化问题是个凸优化问题，且至少存在一个点满足h(x)=0和g(x)=0，极值一定满足 KKT 条件。并且满足强对偶性质（下面会讲）

#### **SVM的核函数了解哪些？为什么要用核函数？**

​    当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。

* **线性核函数** 
$$
  \kappa\left(x, x*{i}\right)=x \cdot x*{i}
$$
  线性核，主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想

* **多项式核函数**

  多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。
  $$
  \kappa\left(x, x*{i}\right)=\left(\left(x \cdot x*{i}\right)+1\right)^{d}
  $$

* **高斯**（**RBF**）**核函数**

  高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数。

$$
\kappa\left(x, x*{i}\right)=\exp \left(-\frac{|| x-x*{i} |^{2}}{\delta^{2}}\right)
$$

+ **sigmoid核函数**

​    采用sigmoid核函数，支持向量机实现的就是一种多层神经网络。

#### 挑选不同的核函数

* 如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；

* 如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；

* 如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。

#### **SVM如何解决线性不可分问题？**

​    间隔最大化，通过引入软间隔、核函数解决线性不可分问题

#### **SVM为什么要对偶(优化复杂度转变，核化)**

①首先是我们有不等式约束方程，这就需要我们写成min max的形式来得到最优解。而这种写成这种形式对x不能求导，所以我们需要转换成max min的形式，这时候，x就在里面了，这样就能对x求导了。而为了满足这种对偶变换成立，就需要满足KKT条件（KKT条件是原问题与对偶问题等价的必要条件，当原问题是凸优化问题时，变为充要条件）。

②对偶问题将原始问题中的约束转为了对偶问题中的等式约束

③方便核函数的引入

④改变了问题的复杂度。由求特征向量w转化为求比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。

### **LR和SVM介绍+区别，什么场景用SVM比较好**

- **相同点**：
  - 第一，LR和SVM都是分类算法；
  - 第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。
  - 第三，LR和SVM都是监督学习算法。第四，LR和SVM都是判别模型。
- **不同点：** 
  - 第一，本质上是其损失函数（loss function）不同。注：lr的损失函数是 cross entropy loss，adaboost的损失函数是expotional loss ,svm是hinge loss，常见的回归模型通常用 均方误差 loss。
  - 第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。
  - 第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。
  - 第四，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。
  - 第五，SVM的损失函数就自带正则！而LR必须另外在损失函数上添加正则项。

### KNN算法

​    `K` 近邻算法（KNN）是一种基本分类和回归方法。`KNN` 算法的核心思想是如果一个样本在特征空间中的 `k` 个最相邻的样本中的大多数属于一个类别，那该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分类样本所属的类别。 如下图：

![image-20210807212001022](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210807212001022.png)

​    在 `KNN` 中，通过计算对象间距离来作为各个对象之间的非相似性指标，避免了对象之间的匹配问题，在这里距离一般使用欧氏距离或曼哈顿距离，公式如下：

![image-20210807212051781](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210807212051781.png)

​    同时，KNN通过依据 k 个对象中占优的类别进行决策，而不是单一的对象类别决策，这两点就是KNN算法的优势。

#### k 值的选取

- `k` 值较小，模型会变得复杂，容易发生过拟合
- `k` 值较大，模型比较简单，容易欠拟合

所以 `k` 值得选取也是一种调参？

KNN中的K值选取对K近邻算法的结果会产生重大影响。如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差（近似误差：可以理解为对现有训练集的训练误差）会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；

如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。

在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法来选择最优的K值。经验规则：k一般低于训练样本数的平方根。

#### KNN算法思路

​    `KNN` 思想就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前 `K` 个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：

1. 计算测试数据与各个训练数据之间的距离；
2. 按照距离的递增关系进行排序；
3. 选取距离最小的 `K` 个点；
4. 确定前 `K` 个点所在类别的出现频率；
5. 返回前 `K` 个点中出现频率最高的类别作为测试数据的预测分类。

### K-Means(K均值)

#### 分类与聚类算法

- 分类简单来说，就是根据文本的特征或属性，划分到已有的类别中。也就是说，这些类别是已知的，通过对已知分类的数据进行训练和学习，找到这些不同类的特征，再对未分类的数据进行分类。
- 聚类，就是你压根不知道数据会分为几类，通过聚类分析将数据或者说用户聚合成几个群体，那就是聚类了。聚类不需要对数据进行训练和学习。
- 分类属于监督学习，聚类属于无监督学习。常见的分类比如决策树分类算法、贝叶斯分类算法等聚类的算法最基本的有系统聚类，K-means 均值聚类。

#### K-mean聚类算法概述

​    聚类的目的是找到每个样本 x 潜在的类别 y，并将同类别 y 的样本 x 放在一起。在聚类问题中，假定训练样本是${x^1,...,x^m}$，每个 $x^i \in R^n$，没有 `y`。`K-means` 算法是将样本聚类成 k 个簇（cluster），算法过程如下：

1. 随机选取 k 个聚类中心（cluster centroids）为 $\mu_1, \mu_1,...,\mu_k \in R^n$。
2. 重复下面过程，直到质心不变或者变化很小：
   - 对于每一个样例 `i` ，计算其所属类别：$$c^i = arg min \underset{j}{min}||x^i - \mu_j||^2$$
   - 对于每一个类 `j`，重新计算该类的质心：$$\mu_j = \frac {\sum_{i=1}^{m}1{c^i = j}x^i} { \sum_{i=1}^{m}1{c^i = j}}$$

​    `K` 是我们事先给定的聚类数，$c^i$ 代表样例 `i` 与 `k` 个类中距离最近的那个类，$c^i$ 的值是 `1` 到 `k` 中的一个。质心 $\mu_j$ 代表我们对属于同一个类的样本中心点的猜测。

```php
选择K个点作为初始质心 

repeat 

    将每个点指派到最近的质心，形成K个簇 

    重新计算每个簇的质心 

until 簇不发生变化或达到最大迭代次数 
```

#### **K-means聚类的原理以及过程？**

​    K-means算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。

#### **K-means聚类怎么衡量相似度的？**

​    欧式距离

#### **余弦相似度距离和欧氏距离的区别？**

**余弦相似度**

​    通过对两个文本分词，`TF-IDF` 算法向量化，利用空间中两个向量的夹角，来判断这两个向量的相似程度：(`计算夹角的余弦，取值 0-1`)

- 当两个向量夹角越大，距离越远，最大距离就是两个向量夹角 180°；
- 夹角越小，距离越近，最小距离就是两个向量夹角 0°，完全重合。
- 夹角越小相似度越高，但由于有可能一个文章的特征向量词特别多导致整个向量维度很高，使得计算的代价太大不适合大数据量的计算。

​    **计算两个向量a、b的夹角余弦：** 我们知道，余弦定理：$cos(\theta) = \frac {a^2+b^2+c^2}{2ab}$ ，由此推得两个向量夹角余弦的计算公式如下： $$cos(\theta) = \frac {ab}{||a|| \times ||b||} = \frac {x_{1}x_{2}+y_1y_2}{\sqrt{x^2_1+y^2_1}\sqrt{x^2_2+y^2_2}}$$ （分子就是两个向量的内积，分母是两个向量的模长乘积）

**欧式距离**

​    在欧几里得空间中，欧式距离其实就是向量空间中两点之间的距离。点 $x = (x_{1}, ..., x_{n})$ 和 $y = (y_{1}, ..., y_{n})$ 之间得欧氏距离计算公式如下： $$d(x,y) = \sqrt {((x_{1}-y_{1})^{2} + (x_{2}-y_{2})^{2} + ... + (x_{n}-y_{n})^{2})}$$

**余弦相似度和欧氏距离的区别**

- 欧式距离和余弦相似度都能度量 `2` 个向量之间的相似度
- 放到向量空间中看，欧式距离衡量`两点之间`的直线距离，而余弦相似度计算的是`两个向量`之间的夹角
- 没有归一化时，欧式距离的范围是 `[0, +∞]`，而余弦相似度的范围是 `[-1, 1]`；余弦距离是计算相似程度，而欧氏距离计算的是相同程度（对应值的相同程度）
- 归一化的情况下，可以将空间想象成一个超球面（三维），欧氏距离就是球面上两点的直线距离，而向量余弦值等价于两点的球面距离，本质是一样。

#### **K值怎么来进行确定？**

​    轮廓系数法和手肘法

#### **简要阐述一下KNN算法和K-Means算法的区别**

​    ①KNN算法是分类算法，分类算法肯定是需要有学习语料，然后通过学习语料的学习之后的模板来匹配我们的测试语料集，将测试语料集合进行按照预先学习的语料模板来分类；

​    ②Kmeans算法是聚类算法，聚类算法与分类算法最大的区别是聚类算法没有学习语料集合。

### **EM聚类算法---最大期望算法**

​    在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。

​    最大期望算法经过两个步骤交替进行计算：

​    第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；

​    第二步是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

### 贝叶斯

#### 先验概率与后验概率

**条件概率**

​    一个事件发生后另一个事件发生的概率。设 A 与 B 为样本空间 Ω 中的两个事件，其中 P(B)>0。那么在事件 B 发生的条件下，事件 A 发生的条件概率为： $$P(A|B) = \frac {P(A\cap B)} {P(B)}$$

**先验概率**

​    事件发生前的概率，可以是基于以往经验/分析，也可以是基于历史数据的统计，甚至可以是人的主观观点给出。一般是**单独**事件概率，如 P(x), P(y)。

**后验概率**

- 事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小（**由果推因**：就是在知道“果”之后，去推测“因”的概率）
- 后验概率和和先验概率的关系可以通过`贝叶斯`公式求得，公式如下： $$P(B_{i}|A) = \frac {P(B_{i}\cdot P(A|B_{i}))}{P(B_{1})\cdot P(A|B_{1}) + P(B_{2})\cdot P(A|B_{2}) }$$

#### 贝叶斯公式

​    贝叶斯公式是建立在条件概率的基础上寻找事件发生的原因（即大事件 `A` 已经发生的条件下，分割中的小事件 `Bi` 的概率），设 `B1,B2,...` 是样本空间 `Ω` 的一个划分，则对任一事件 `A（P(A)>0)`, 有：$$P(B_{i}|A) = \frac {P(A|B_{i})P(B_{i})}{\sum_{j=1}^{n}P(B_{j})P(A|B_{j})}$$

- `Bi` 常被视为导致试验结果A发生的”原因“；
- `P(Bi)(i=1,2,...)` 表示各种原因发生的可能性大小，故称先验概率；
- `P(Bi|A)(i=1,2...)` 则反映当试验产生了结果A之后，再对各种原因概率的新认识，故称后验概率。

#### 后验概率实例

​    假设一个学校里有 `60％` 男生和 `40%` 女生。女生穿裤子的人数和穿裙子的人数相等，所有男生穿裤子。一个人在远处随机看到了一个穿裤子的学生。那么这个学生是女生的概率是多少？

- 使用贝叶斯定理，事件A是看到女生，事件B是看到一个穿裤子的学生。我们所要计算的是 $P(A|B)$。
- $P(A)$ 是忽略其它因素，看到女生的概率，在这里是 40%；
- $P(A')$ 是忽略其它因素，看到不是女生（即看到男生）的概率，在这里是 60%；
- $P(B|A)$ 是女生穿裤子的概率，在这里是 50%；
- $P(B|A')$ 是男生穿裤子的概率，在这里是 100%；
- $P(B)$ 是忽略其它因素，学生穿裤子的概率，$P(B) = P(B|A)P(A) + P(B|A')P(A')$，在这里是 0.5×0.4 + 1×0.6 = 0.8。

​    根据贝叶斯定理，我们计算出后验概率P(A|B): $$P(A|B) = \frac {P(B|A)P(A)}{P(B)} = \frac {0.5\times 0.4} {0.8}$$

#### **极大似然估计（MLE）**

​    在已经得到试验结果（即样本）的情况下，估计满足这个样本分布的参数，将使这个样本出现的概率最大的那个参数Θ作为真参数Θ的估计。在样本固定的情况下，样本出现的概率与参数Θ之间的函数，称为似然函数。

#### 最大后验概率（MAP）

最大后验估计是根据经验数据，获得对难以观察的量的点估计。与最大似然估计不同的是，最大后验估计融入了被估计量的先验分布，即模型参数本身的概率分布。最大后验概率估计其实就是多了一个参数的先验概率，也可以认为最大似然估计就是把先验概率认为是一个定值；后验概率 := 似然 * 先验概率

#### **伯努利分布**

​    伯努利分布(Bernoulli distribution)又名两点分布或0-1分布。

#### **假设检验的基本思想**

​    假设检验的基本思想是小概率反证法思想。小概率思想是指小概率事件(P<0．01或P<0．05)在一次试验中基本上不会发生。反证法思想是先提出假设(检验假设Ho)，再用适当的统计方法确定假设成立的可能性大小，如可能性小，则认为假设不成立，若可能性大，则还不能认为假设不成立。

### **条件随机场**

​    CRF即条件随机场（Conditional Random Fields），是在给定一组输入随机变量条件下另外一组输出随机变量的条件概率分布模型，它是一种判别式的概率无向图模型，既然是判别式，那就是对条件概率分布建模。

### **隐马尔科夫模型（HMM）**

​    隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。

### **降维方法**

​    主成分分析（PCA）、线性判别分析（LDA）、局部线性嵌入（LLE）、LE、SVD

​    **PCA原理和执行步骤**：

​        主成分分析(PCA) 是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。 是将原空间变换到特征向量空间内，数学表示为AX = γX。

​    **LDA算法**：

​        LDA是一种有监督的（supervised）线性降维算法。与PCA保持数据信息不同，核心思想：往线性判别超平面的法向量上投影，是的区分度最大（高内聚，低耦合）。LDA是为了使得降维后的数据点尽可能地容易被区分！

### 树方面的算法

#### **ID3，C4.5和CART树**

①ID3 ---- ID3算法最核心的思想是采用信息增益来选择特征

②C4.5采用信息增益比，用于减少ID3算法的局限（在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性，而这样做有时候是没有意义的）

③CART算法采用gini系数，不仅可以用来分类，也可以解决回归问题。

​    决策树呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策模型进行分类。

#### **决策树的分类**：

​    离散性决策树、连续性决策树。

+ 离散性决策树：离散性决策树，其目标变量是离散的，如性别：男或女等；

+ 连续性决策树：连续性决策树，其目标变量是连续的，如工资、价格、年龄等；

**决策树的优点**：

​    （1）具有可读性，如果给定一个模型，那么过呢据所产生的决策树很容易推理出相应的逻辑表达。

​    （2）分类速度快，能在相对短的时间内能够对大型数据源做出可行且效果良好的结果。

**决策树的缺点**：

​    （1）对未知的测试数据未必有好的分类、泛化能力，即可能发生过拟合现象，此时可采用剪枝或随机森林。

#### 信息量

信息量是通过概率来定义的：如果一件事情的概率很低，那么它的信息量就很大；反之，如果一件事情的概率很高，它的信息量就很低。简而言之，概率小的事件信息量大，因此信息量可以定义如下：

![信息量 = \log \frac{1}{p(x)} .](https://math.jianshu.com/math?formula=%E4%BF%A1%E6%81%AF%E9%87%8F%20%3D%20%5Clog%20%5Cfrac%7B1%7D%7Bp(x)%7D%20.)

下面解释为什么要取倒数再去对数。

（1）先取倒数： ![\frac{1}{p(x)}](https://math.jianshu.com/math?formula=%5Cfrac%7B1%7D%7Bp(x)%7D) 这件事表示：“信息量”和“概率”呈反比；

（2）在取对数：![\log](https://math.jianshu.com/math?formula=%5Clog) 取对数是为了将区间 ![[1,\infty]](https://math.jianshu.com/math?formula=%5B1%2C%5Cinfty%5D) 映射到 ![[0, \infty]](https://math.jianshu.com/math?formula=%5B0%2C%20%5Cinfty%5D)。

再总结一下：![\because p(x) \in [0,1], \therefore \frac{1}{p(x)} \in [1, \infty], \therefore \log \frac{1}{p(x)} \in [0, \infty]](https://math.jianshu.com/math?formula=%5Cbecause%20p(x)%20%5Cin%20%5B0%2C1%5D%2C%20%5Ctherefore%20%5Cfrac%7B1%7D%7Bp(x)%7D%20%5Cin%20%5B1%2C%20%5Cinfty%5D%2C%20%5Ctherefore%20%5Clog%20%5Cfrac%7B1%7D%7Bp(x)%7D%20%5Cin%20%5B0%2C%20%5Cinfty%5D)

#### 信息熵

信息熵是信息量的数学期望。理解了信息量，信息熵的定义式便不难理解。定义如下：

![H(X) = -\sum_{x \in X} p(x)\log p(x)](https://math.jianshu.com/math?formula=H(X)%20%3D%20-%5Csum_%7Bx%20%5Cin%20X%7D%20p(x)%5Clog%20p(x))

- 熵越小表示越“纯”，决策树算法在进行特征选择时的其中**标准之一**就是选择使得通过该特征分类以后的类的熵最小；

#### 条件熵

条件熵的定义为：在 ![X](https://math.jianshu.com/math?formula=X) 给定的条件下，![Y](https://math.jianshu.com/math?formula=Y) 的条件概率分布的熵对 ![X](https://math.jianshu.com/math?formula=X) 的数学期望。

条件熵一定要记住下面的这个定义式，其它的式子都可以由信息熵和条件熵的定义式得出。

![H(Y|X)=\sum_{x\in X} p(x)H(Y|X=x)](https://math.jianshu.com/math?formula=H(Y%7CX)%3D%5Csum_%7Bx%5Cin%20X%7D%20p(x)H(Y%7CX%3Dx))

理解条件熵可以使用决策树进行特征选择的例子：我们期望选择的特征要能将数据的标签尽可能分得比较“纯”一些，特征将数据的标签分得“纯”，则熵就小，信息增益就大。

#### **相对熵**

又称 KL 散度，如果我们对于同一个随机变量 ![X](https://math.jianshu.com/math?formula=X) 有两个单独的概率分布 ![P(X)](https://math.jianshu.com/math?formula=P(X)) 和 ![Q(X)](https://math.jianshu.com/math?formula=Q(X))，使用 KL 散度（Kullback-Leibler (KL) divergence)来衡量这两个分布的差异。差异越大则相对熵越大，差异越小则相对熵越小。

计算公式如下：

![D_{KL}(p||q)=\sum_{i=1}^np(x_i)\log(\frac{p(x_i)}{q(x_i)}) \tag{3.1}](https://math.jianshu.com/math?formula=D_%7BKL%7D(p%7C%7Cq)%3D%5Csum_%7Bi%3D1%7D%5Enp(x_i)%5Clog(%5Cfrac%7Bp(x_i)%7D%7Bq(x_i)%7D)%20%5Ctag%7B3.1%7D)

如何记忆：如果用 ![P](https://math.jianshu.com/math?formula=P) 来描述样本，那么就非常完美（因为 ![P](https://math.jianshu.com/math?formula=P) 认为是真实的情况）。而用 ![Q](https://math.jianshu.com/math?formula=Q) 来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和 ![P](https://math.jianshu.com/math?formula=P) 一样完美的描述。如果我们的 ![Q](https://math.jianshu.com/math?formula=Q) 通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，![Q](https://math.jianshu.com/math?formula=Q) 等价于 ![P](https://math.jianshu.com/math?formula=P)。 即 ![P](https://math.jianshu.com/math?formula=P) 和 ![Q](https://math.jianshu.com/math?formula=Q) 的分布完全一致的时候，KL 散度的值等于 ![0](https://math.jianshu.com/math?formula=0) 。

#### **相对熵公式**

​    KL散度（Kullback-Leibler Divergence）也叫做相对熵，用于度量两个概率分布之间的差异程度。

![image-20210619115053532](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210619115053532.png)

#### 交叉熵

![H(p,q) = \sum_{i=1}^np(x_i)\log \cfrac{1}{q(x_i)}](https://math.jianshu.com/math?formula=H(p%2Cq)%20%3D%20%5Csum_%7Bi%3D1%7D%5Enp(x_i)%5Clog%20%5Ccfrac%7B1%7D%7Bq(x_i)%7D)

通过逻辑回归的损失函数记忆交叉熵。![p(x_i)](https://math.jianshu.com/math?formula=p(x_i)) 认为是类标，是独热编码（也可以认为是概率分布），而 ![q(x_i)](https://math.jianshu.com/math?formula=q(x_i)) 认为是逻辑回归预测的概率分布。

- 交叉熵是对数似然函数的相反数。对数似然的值我们希望它越大越好，交叉熵的值我们希望它越小越好。

#### 相对熵与交叉熵的关系

结论：KL 散度 = 交叉熵 - 熵 。这一点从相对熵的定义式就可以导出。

![image-20210627150104636](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210627150104636.png)



```php
KL损失的输出表示两个概率分布的接近程度。如果预测的概率分布与真实的概率分布相差很远，就会导致很大的损失。如果 KL Divergence 的值为零，则表示概率分布相同。

KL Divergence 与交叉熵损失的关键区别在于它们如何处理预测概率和实际概率。交叉熵根据预测的置信度惩罚模型，而 KL Divergence 则没有。
KL Divergence 仅评估概率分布预测与ground truth分布的不同之处。

应用场景：逼近复杂函数多类分类任务确保预测的分布与训练数据的分布相似
```

```python
input = torch.randn(2, 3, requires_grad=True)`
target = torch.randn(2, 3)`

kl_loss = torch.nn.KLDivLoss(reduction = 'batchmean')`
output = kl_loss(input, target)`
```



### **集成学习（bagging和boosting）bagging和boosting的联系和区别**

​    Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。

**boosting（提升法）**

​    Boosting是一族可将弱学习器提升为强学习器的算法。其工作机制为：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

**boosting的算法过程如下：**

​    对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。

​    进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）

**Bagging（套袋法）**

​    Bagging是指采用Bootstrap（有放回的均匀抽样）的方式从训练数据中抽取部分数据训练多个分类器，每个分类器的权重是一致的，然后通过投票的方式取票数最高的分类结果最为最终结果。

**bagging的算法过程如下：**

​    从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）

​    对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）

​    对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）

区别：

* 样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.Boosting：每一轮的训练集不变(个人觉得这里说的训练集不变是说的总的训练集，对于每个分类器的训练集还是在变化的，毕竟每次都是抽样)，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.
* 样例权重：Bagging：使用均匀取样，每个样例的权重相等Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.
* 预测函数：Bagging：所有预测函数的权重相等.Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.
* 并行计算：Bagging：各个预测函数可以并行生成Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.
  - Bagging + 决策树 = 随机森林  
  - AdaBoost + 决策树 = 提升树
  - Gradient Boosting + 决策树 = GBDT

#### **随机森林的原理**

​    随机森林属于集成学习（Ensemble Learning）中的bagging算法。在集成学习中，主要分为bagging算法和boosting算法。我们先看看这两种方法的特点和区别。

#### **随机森林的随机体现在哪里？**

​    随机森林的随机性体现在每颗树的训练样本是随机的，树中每个节点的分裂属性集合也是随机选择确定的。有了这2个随机的保证，随机森林就不会产生过拟合的现象了。

+ **调参**：一般采用网格搜索法优化超参数组合。这里将调参方法简单归纳为三条：1、分块调参（不同框架参数分开调参）；2、一次调参不超过三个参数；3、逐步缩小参数范围。

### **其他树模型**

**Adaboost与GBDT两者boosting的不同策略是两者的本质区别：**

​    Adaboost强调Adaptive（自适应），通过不断修改样本权重（增大分错样本权重，降低分对样本权重），不断加入弱分类器进行boosting。

​    而GBDT则是旨在不断减少残差（回归），通过不断加入新的树旨在在残差减少（负梯度）的方向上建立一个新的模型。——即损失函数是旨在最快速度降低残差。

   而XGBoost的boosting策略则与GBDT类似，区别在于GBDT旨在通过不断加入新的树最快速度降低残差，而XGBoost则可以人为定义损失函数（可以是最小平方差、logistic loss function、hinge loss function或者人为定义的loss function），只需要知道该loss function对参数的一阶、二阶导数便可以进行boosting，其进一步增大了模型的泛华能力，其贪婪法寻找添加树的结构以及loss function中的损失函数与正则项等一系列策略也使得XGBoost预测更准确。

​    XGBoost的具体策略可参考本专栏的XGBoost详述。 GBDT每一次的计算是都为了减少上一次的残差，进而在残差减少（负梯度）的方向上建立一个新的模型。

​    XGBoost则可以自定义一套损失函数，借助泰勒展开（只需知道损失函数的一阶、二阶导数即可求出损失函数）转换为一元二次函数，得到极值点与对应极值即为所求。

### 激活函数

#### 定义：

​    在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。

#### **激活函数的意义如下**：

①模拟生物神经元特性，接受输入后通过一个阈值模拟神经元的激活和兴奋并产生输出；

②为神经网络引入非线性，增强神经网络的表达能力；

③导出神经网络最后的结果(在输出层时)。

​    **常用的激活函数**？sigmoid，tanh，ReLU, leaky ReLU, PReLU, ELU，random ReLU等。

#### **①sigmoid**

​    我们通常就用其中最常用的logistic函数来代指sigmoid函数：

![image-20210619100625126](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210619100625126.png)

![img](https://tva1.sinaimg.cn/large/006C3FgEly1gr6a7k9qk4j30ge0bawef.jpg)

​    sigmoid函数和阶跃函数非常相似，但是解决了光滑和连续的问题，同时它还成功引入了非线性。由于其值域处在0~1，所以往往被用到二分类任务的输出层做概率预测。

​    当输入值大于3或者小于-3时，梯度就非常接近0了，在深层网络中，这非常容易造成“梯度消失”（也就是反向传播时误差难以传递到前面一层）而使得网络很难训练。此外，sigmoid函数的均值是0.5，但是不符合我们对神经网络内数值期望为0的设想。

​    **特点：** 它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.

​    **缺点：** 缺点1：在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，而梯度消失发生的概率比较大。缺点2：Sigmoid 的 output 不是0均值（即zero-centered）。缺点3：其解析式中含有幂运算，计算机求解时相对来讲比较耗时。

**Sigmoid、Sigmoid的导数**

​    Sigmoid函数：

![image-20210619115603344](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210619115603344.png)


​    导数：

![image-20210619101022412](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210619101022412.png)

#### **②tanh函数**

![image-20210619100746926](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210619100746926.png)

![img](https://tva1.sinaimg.cn/large/006C3FgEly1gr6a83u2fsj30i008ot8x.jpg)

​    这个tanh函数又被称作双曲正切函数，可以看出它的函数范围是（-1，1）而均值为0，解决了上面sigmoid的一个问题。但是不难发现，该函数依旧没有解决梯度消失的问题。

#### **③Relu函数**

​    ReLu函数又叫线性整流单元，应该说是当前最常用的一个激活函数了，尤其是在卷积神经网络和层次较深的神经网络中。

![image-20210619100831397](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210619100831397.png)

![img](https://tva1.sinaimg.cn/large/006C3FgEly1gr6a8d3otfj30lw07qt8u.jpg)

​    ReLu将x=0处的光滑曲线替换为了折线，这使得它的计算也相对更加简单，而且有助于随机梯度下降算法收敛（有研究指出收敛加速可达6倍）。当然ReLu函数也能够缓解梯度消失的问题。

​    当然，ReLu还是有一些缺陷的，对于小于0的这部分值，梯度会迅速降为0而不再影响网络训练。这会造成部分神经元从来不被激活，也称作“死区”。这也给了ReLu函数的变种很多发挥空间。

​    优点：

1. 解决了gradient vanishing问题 (在正区间)

2. 计算速度非常快，只需要判断输入是否大于0
3. 收敛速度远快于sigmoid和tanh

#### **④Leaky ReLu**

​    将x<=0部分调整为f(x)=αx,其中α一般设为一个较小的正数如0.01或0.001。这样就将小于0部分的梯度从零提高到α，给了这些被抑制部分一定参与网络训练的可能。

![img](https://tva1.sinaimg.cn/large/006C3FgEly1gr6a8s3capj30mc08a74j.jpg)

#### **⑤PReLu**

​    参数化ReLu（Parameterised ReLu,PReLu）的形式和Leaky ReLu一样，唯一地不同是它将α视作一个可训练的参数而不是人为设定的超参数。这样，就避免了Leaky ReLu中的选定α值的问题。

#### **⑥andomized Leaky ReLu**

![img](https://tva1.sinaimg.cn/large/006C3FgEly1gr6a9ijfzdj30qo07iaa9.jpg)

#### **⑦ELU**

![img](https://tva1.sinaimg.cn/large/006C3FgEly1gr6a9tk9yuj30ky08sjrl.jpg)

​    ELU也是为解决ReLU存在的问题而提出，显然，ELU有ReLU的基本所有优点，以及：不会有Deal ReLU问题；输出的均值接近0，zero-centered。

它的一个小问题在于计算量稍大。类似于Leaky ReLU，理论上虽然好于ReLU

#### **sigmoid和relu的优缺点**

Relu优点：

1. relu函数在大于0的部分梯度为常数，所以不会产生梯度弥散现象.。而对于sigmod函数，在正负饱和区的梯度都接近于0，可能会导致梯度消失现象。
2. Relu函数的导数计算更快，所以使用梯度下降时比Sigmod收敛起来要快很多。

Relu缺点：

1. Relu死亡问题。当 x 是小于 0 的时候，那么从此所以流过这个神经元的梯度将都变成 0；这个时候这个 ReLU 单元在训练中将死亡（也就是参数无法更新），这也导致了数据多样化的丢失（因为数据一旦使得梯度为 0，也就说明这些数据已不起作用）。

Sigmod优点：

1. 具有很好的解释性，将线性函数的组合输出为0，1之间的概率。

Sigmodu缺点：

1. 激活函数计算量大，反向传播求梯度时，求导涉及除法。
2. 反向传播时，在饱和区两边导数容易为0，即容易出现梯度消失的情况，从而无法完成深层网络的训练。

#### **softmax和sigmoid在多分类任务中的优劣**

多个sigmoid与一个softmax都可以进行多分类.如果多个类别之间是互斥的，就应该使用softmax，即这个东西只可能是几个类别中的一种。如果多个类别之间不是互斥的，使用多个sigmoid。

#### **用softmax做分类函数，假如现在要对1w甚至10w类做分类会出现什么问题？**

过拟合，怎么解决，面试官让自己想(不能使用softmax，使用三元组损失)

### **损失函数**

#### 说一下smooth L1 Loss,并阐述使用smooth L1 Loss的优点**

![image-20210620223716650](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620223716650.png)

​    **Smooth L1的优点：**

​        ①相比于L1损失函数，可以收敛得更快。

​        ②相比于L2损失函数，对离群点、异常值不敏感，梯度变化相对更小，训练时不容易跑飞。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfoJbjibr9KcTicicibkytMzPl7bnlLuTz0oiaIET4ZO53rCiad3QZYTwHXPPxm3EVMn58DhI0aJ0fOTl8yA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### **L1_loss和L2_loss的区别**

​    平均绝对误差(L1 Loss): 平均绝对误差（Mean Absolute Error,MAE) 是指模型预测值f(x)和真实值y之间距离的平均值，其公式如下：

![image-20210620223821082](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620223821082.png)

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfoJbjibr9KcTicicibkytMzPl7bapG0Lyefy7DxBrHKXIgicSBFOLC9ffvKTvcKo5pKWtkZiaDfrC800icZg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​    均方误差MSE (L2 Loss):均方误差（Mean Square Error,MSE）是模型预测值f(x) 与真实样本值y 之间差值平方的平均值，其公式如下

![image-20210620223840499](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620223840499.png)

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfoJbjibr9KcTicicibkytMzPl7bsYRIquic8N5ib86qQvYH3exxr9wN71lE99284fns3bv1nUPrFaW6gBuA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### **为何分类问题用交叉熵而不用平方损失？啥是交叉熵**

![image-20210620223902298](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620223902298.png)

​    1.用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。

​    2.使用交叉熵损失是函数，误差越大参数的梯度也越大，能够快速收敛。

#### **分类中为什么交叉熵损失函数比均方误差损失函数更常用？**

​    交叉熵损失函数关于输入权重的梯度表达式与预测值与真实值的误差成正比且不含激活函数的梯度，而均方误差损失函数关于输入权重的梯度表达式中则含有，由于常用的sigmoid/tanh等激活函数存在梯度饱和区，使得MSE对权重的梯度会很小，参数w调整的慢，训练也慢，而交叉熵损失函数则不会出现此问题，其参数w会根据误差调整，训练更快，效果更好。

​    交叉熵损失函数：实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近。

![image-20210706210424759](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210706210424759.png)

```python
def cross_entropy(a,y):
    return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))
#tensorflow版
loss = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y),reduction_indices=[1]))

#numpy版
loss = np.mean(-np.sum(y_*np.log(y),axis=1))
```



#### **一张图片多个类别怎么设计损失函数，多标签分类问题**

​    多标签分类怎么解决，从损失函数角度考虑

​    分类问题名称  输出层使用激活函数 对应的损失函数

​    **二分类 sigmoid函数 二分类交叉熵损失函数（binary_crossentropy）**

​    **多分类 Softmax函数 多类别交叉熵损失函数（categorical_crossentropy）**

​    **多标签分类 sigmoid函数 二分类交叉熵损失函数（binary_crossentropy）**

​    多标签问题方法是计算一个样本各个标签的损失（输出层采用sigmoid函数），然后取平均值。把一个多标签问题，转化为了在每个标签上的二分类问题。

#### **LR的损失函数？它的导数是啥？加了正则化之后它的导数又是啥？**

​     LR的损失函数为交叉熵损失函数。

​    Logistic regression （逻辑回归）是当前业界比较常用的机器学习方法。

​    Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题，利用Logistic函数（或称为Sigmoid函数），自变量取值范围为(-INF, INF)，自变量的取值范围为(0,1)，函数形式为：

![image-20210620224027404](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620224027404.png)

### 梯度

#### **梯度爆炸，梯度消失，梯度弥散是什么，为什么会出现这种情况以及处理办法**

​    梯度弥散（梯度消失）: 通常神经网络所用的激活函数是sigmoid函数，sigmod函数容易引起梯度弥散。这个函数能将负无穷到正无穷的数映射到0和1之间，并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))表示两个0到1之间的数相乘，得到的结果就会变得很小了。神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失。

​    梯度爆炸：就是由于初始化权值过大，前面层会比后面层变化的更快，就会导致权值越来越大，梯度爆炸的现象就发生了。

####     梯度消失/爆炸是什么？

​    反向传播中由于链式求导法则的连乘，如果乘数都比较小趋于0，最终传递到网络输入层的梯度会变得很小（梯度消失），如果乘数都很大，最终的梯度也会变得很大（梯度爆炸），其实二者都是因为网络太深导致权值更新不稳定，本质上是因为梯度反向传播中的连乘效应

#### **梯度消失与梯度爆炸的产生原因**

​    梯度消失：（1）隐藏层的层数过多；（2）采用了不合适的激活函数(更容易产生梯度消失，但是也有可能产生梯度爆炸)

​    梯度爆炸：（1）隐藏层的层数过多；（2）权重的初始化值过大

#### **梯度消失与梯度爆炸的解决方案**

1. 用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。

2. 用Batch Normalization。
3.  LSTM的结构设计也可以改善RNN中的梯度消失问题。
4. 预训练+微调
5. 使用残差网络

#### **鞍点的定义**

​    目标函数在此点上的梯度（一阶导数）值为 0， 但从改点出发的一个方向是函数的极大值点，而在另一个方向是函数的极小值点。

#### **拐点怎么求？**

​    拐点，又称反曲点，在数学上指改变曲线向上或向下方向的点，直观地说拐点是使切线穿越曲线的点（即连续曲线的凹弧与凸弧的分界点）。

​    若函数y=f(x)在c点可导，且在点c一侧是凸，另一侧是凹，则称c是函数y=f(x)的拐点。

![img](https://tva1.sinaimg.cn/large/006C3FgEly1gr6aglwwnbj30aw08e0so.jpg)

### 优化算法

#### **深度学习优化学习方法（一阶、二阶）**

​    **一阶方法**：随机梯度下降（SGD）、动量（Momentum）、牛顿动量法（Nesterov动量）、AdaGrad（自适应梯度）、RMSProp（均方差传播）、Adam、Nadam。

​    **二阶方法**：牛顿法、拟牛顿法、共轭梯度法（CG）、BFGS、L-BFGS。

#### **自适应优化算法有哪些**？

​    Adagrad（累积梯度平方）、RMSProp（累积梯度平方的滑动平均）、Adam（带动量的RMSProp，即同时使用梯度的一、二阶矩）

#### **梯度下降陷入局部最优有什么解决办法**？ 

​    可以用BGD、SGD、MBGD、momentum，RMSprop，Adam等方法来避免陷入局部最优。

#### **梯度下降法原理**

​    梯度下降法又称最速下降法，是求解无约束最优化问题的一种最常用的方法，在对损失函数最小化时经常使用。梯度下降法是一种迭代算法。选取适当的初值x(0)，不断迭代，更新x的值，进行目标函数的极小化，直到收敛。由于负梯度方向时使函数值下降最快的方向，在迭代的每一步，以负梯度方向更新x的值，从而达到减少函数值的目的。

​    **损失函数**：

![image-20210620221041100](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620221041100.png)

​    其中，J(θ)是损失函数，m代表每次取多少样本进行训练，如果采用SGD进行训练，那每次随机取一组样本，m=1；如果是批处理，则m等于每次抽取作为训练样本的数量。θ是参数，对应（1式）的θ1和θ2。求出了θ1和θ2，h(x)的表达式就出来了：

![image-20210620221133173](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620221133173.png)

​    我们的目标是让损失函数J(θ)的值最小，根据梯度下降法，首先要用J(θ)对θ求偏导：

![image-20210620221149421](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620221149421.png)

由于是要最小化损失函数，所以参数θ按其负梯度方向来更新：

![image-20210620221201949](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620221201949.png)

#### **批量梯度下降（BGD）**

​    批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新

![image-20210620221224388](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620221224388.png)

```python
for i in range(nb_epochs):    
    params_grad = evaluate_gradient(loss_function, data, params)    
    params = params - learning_rate * params_grad
```

​    **优点**：（1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。

​                （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

​    **缺点**：（1）当样本数目 m 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

​               （2）不能投入新数据实时更新模型。

#### **随机梯度下降（SGD）**

​    随机梯度下降法求梯度时选取一个样本j来求梯度。

![image-20210620221354316](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620221354316.png)

```python
for i in range(nb_epochs):  
    np.random.shuffle(data)
    for example in data:
        params_grad = evaluate_gradient(loss_function , example ,params)    
        params = params - learning_rate * params_grad
```

​    **优点：**（1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。

​    **缺点：**（1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。

​               （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。

​               （3）不易于并行实现。SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。

#### **小批量梯度下降算法（mini-batch GD）**

小批量梯度下降法是是对于m个样本，我们采用x个样子来迭代，1<x<m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。

![image-20210620221527345](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620221527345.png)

```python
for i in range(nb_epochs):    
    np.random.shuffle(data)    
    for batch in get_batches(data, batch_size=50):        
        params_grad = evaluate_gradient(loss_function, batch, params)       
        params = params - learning_rate * params_grad
```

​    **优点**：

​        （1）通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。

​        （2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)

​        （3）可实现并行化。

​    **缺点(解释1)：**

​       （1）不过 Mini-batch gradient descent 不能保证很好的收敛性，learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离。（有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点。）对于非凸函数，还要避免陷于局部极小值处，或者鞍点处，因为鞍点周围的error是一样的，所有维度的梯度都接近于0，SGD 很容易被困在这里。（会在鞍点或者局部最小点震荡跳动，因为在此点处，如果是训练集全集带入即BGD，则优化会停止不动，如果是mini-batch或者SGD，每次找到的梯度都是不同的，就会发生震荡，来回跳动。）

​     （2）SGD对所有参数更新时应用同样的 learning rate，如果我们的数据是稀疏的，我们更希望对出现频率低的特征进行大一点的更新。LR会随着更新的次数逐渐变小。

​    **缺点(解释2)：**：

​    （1）batch_size的不当选择可能会带来一些问题。

#### batcha_size的选择带来的影响：

​    **在合理地范围内，增大batch_size的好处：**

​        a. 内存利用率提高了，大矩阵乘法的并行化效率提高。

​        b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

​        c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

​    **盲目增大batch_size的坏处：**

​        a. 内存利用率提高了，但是内存容量可能撑不住了。

​        b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。

​        c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

#### 梯度下降算法改进

​    **①动量梯度下降法（Momentum）**

​    Momentum 通过加入 γ*vt−1 ，可以加速 SGD， 并且抑制震荡。momentum即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。动量法做的很简单，相信之前的梯度。如果梯度方向不变，就越发更新的快，反之减弱当前梯度。r一般为0.9。

![image-20210620221940986](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620221940986.png)

​    缺点：这种情况相当于小球从山上滚下来时是在盲目地沿着坡滚，如果它能具备一些先知，例如快要上坡时，就知道需要减速了的话，适应性会更好。

​    **②Nesterov accelerated gradient法（NAG）**

​    用 θ−γv_t−1 来近似当做参数下一步会变成的值，则在计算梯度时，不是在当前位置，而是未来的位置上。仍然是动量法，只是它要求这个下降更加智能。这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性。

​    用 θ−γv_t−1 来近似当做参数下一步会变成的值，则在计算梯度时，不是在当前位置，而是未来的位置上。仍然是动量法，只是它要求这个下降更加智能。这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性。

![image-20210620222027100](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222027100.png)

​    Nesterov 的好处就是，当梯度方向快要改变的时候，它提前获得了该信息，从而减弱了这个过程，再次减少了无用的迭代。超参数设定值: 一般 γ 仍取值 0.9 左右。

​    **③Adagrad**

​    这个算法就可以对低频的参数做较大的更新，对高频的做较小的更新，也因此，对于稀疏的数据它的表现很好，很好地提高了 SGD 的鲁棒性，例如识别 Youtube 视频里面的猫，训练 GloVe word embeddings，因为它们都是需要在低频的特征上有更大的更新。

​    梯度更新规则:

![image-20210620222101468](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222101468.png)

​    其中g为：t时刻参数θ_i的梯度

![image-20210620222228452](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222228452.png)

​    如果是普通的 SGD， 那么 θ_i 在每一时刻的梯度更新公式为：

![image-20210620222241380](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222241380.png)

​    但这里的learning rate η也随t和i而变：

![image-20210620222445140](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222445140.png)

​    其中 Gt 是个对角矩阵， (i,i) 元素就是 t 时刻参数 θi 的梯度平方和。

​    Adagrad 的**优点**是减少了学习率的手动调节。超参数设定值：一般η选取0.01。

​    **缺点**：它的缺点是分母会不断积累，这样学习率就会收缩并最终会变得非常小。

​    **④Adadelta**

​    这个算法是对 Adagrad 的改进，和Adagrad相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，指数衰减平均值。

![image-20210620222537228](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222537228.png)

​    这个分母相当于梯度的均方根 root mean squared (RMS)，在数据统计分析中，将所有值平方求和，求其均值，再开平方，就得到均方根值 ，所以可以用 RMS 简写：

![image-20210620222552428](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222552428.png)

​    其中 E 的计算公式如下，t 时刻的依赖于前一时刻的平均和当前的梯度：

![image-20210620222609980](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222609980.png)

​    超参数设定值: γ 一般设定为 0.9，梯度更新规则:此外，还将学习率η换成了RMS[Δθ]，这样的话，我们甚至都不需要提前设定学习率了：

![image-20210620222734620](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222734620.png)

​    **⑤RMSprop**

​    RMSprop 是 Geoff Hinton 提出的一种自适应学习率方法。RMSprop 和 Adadelta 都是为了解决 Adagrad 学习率急剧下降问题的。

​    **梯度更新规则:**

​    RMSprop 与 Adadelta 的第一种形式相同：（使用的是指数加权平均，旨在消除梯度下降中的摆动，与Momentum的效果一样，某一维度的导数比较大，则指数加权平均就大，某一维度的导数比较小，则其指数加权平均就小，这样就保证了各维度导数都在一个量级，进而减少了摆动，允许使用一个更大的学习率η）。

![image-20210620222830587](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222830587.png)

​    超参数设定值:Hinton 建议设定 γ 为 0.9, 学习率 η 为 0.001。

**⑥Adam：Adaptive Moment Estimation**

​    Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。这个算法是另一种计算每个参数的自适应学习率的方法，相当于 RMSprop + Momentum。

​    除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的指数衰减平均值：

![image-20210620222858500](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222858500.png)

​    如果mt和vt被初始化为0向量，那它们就会向0偏置，所以做了偏差校正，通过计算偏差校正后的mt和vt来抵消这些偏差：

![image-20210620222917484](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222917484.png)

梯度更新规则:

![image-20210620222927716](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620222927716.png)

​    超参数设定值:建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8。实践表明，Adam 比其他适应性学习方法效果要好。

**Adam 和 SGD 区别：**

​    Adam = Adaptive + Momentum，顾名思义Adam集成了SGD的一阶动量和RMSProp的二阶动量。

#### 牛顿法

​    利用二阶导数，收敛速度快；但对目标函数有严格要求，必须有连续的一、二阶偏导数，计算量大。利用牛顿法求解目标函数的最小值其实是转化成求使目标函数的一阶导为0的参数值。这一转换的理论依据是，函数的极值点处的一阶导数为0.其迭代过程是在当前位置x0求该函数的切线，该切线和x轴的交点x1，作为新的x0,重复这个过程，直到交点和函数的零点重合。此时的参数值就是使得目标函数取得极值的参数值。

#### **简述梯度下降法和牛顿法的优缺点？梯度下降法和牛顿法区别**

​    1.牛顿法：是通过求解目标函数的一阶导数为0时的参数，进而求出目标函数最小值时的参数。①收敛速度很快。②海森矩阵的逆在迭代过程中不断减小，可以起到逐步减小步长的效果。③缺点：海森矩阵的逆计算复杂，代价比较大，因此有了拟牛顿法。

​    2.梯度下降法：是通过梯度方向和步长，直接求解目标函数的最小值时的参数。越接近最优值时，步长应该不断减小，否则会在最优值附近来回震荡。

### 卷积神经网络篇（包含深度学习常考）

#### **CNN的经典模型**

​    LeNet，AlexNet，VGG，GoogLeNet，ResNet，DenseNet

#### **对CNN的理解**

​    CNN＝ 数据输入层 (Input Layer)＋ {[卷积计算层（CONV Layer )＊ａ＋ReLU激励层 (ReLU Layer)]＊ｂ＋ 池化层 (Pooling Layer ) ｝*ｃ＋全连接层 (FC Layer) ＊ ｄ 。

#### **CNN和传统的全连接神经网络有什么区别？**

​    在全连接神经网络中，每相邻两层之间的节点都有边相连，于是会将每一层的全连接层中的节点组织成一列，这样方便显示连接结构。而对于卷积神经网络，相邻两层之间只有部分节点相连，为了展示每一层神经元的维度，一般会将每一层卷积层的节点组织成一个三维矩阵。全连接神经网络和卷积神经网络的唯一区别就是神经网络相邻两层的连接方式。

#### **讲一下CNN，每个层及作用**

​    卷积层：用它来进行特征提取

​    池化层：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征，

​    激活函数：是用来加入非线性因素的，因为线性模型的表达能力不够。

​    全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。

#### **为什么神经网络使用卷积层**？

​    共享参数，局部连接；

#### **使用卷积层的前提条件是什么**？

​    数据分布一致

#### **resnet相比于之前的卷积神经网络模型中，最大的改进点是什么？，解决了什么问题**

​    跳跃连接(residual block)和瓶颈层。resnet本身是一种拟合残差的结果，让网络学习任务更简单，可以有效地解决梯度弥散的问题。

​    **ResNet的特点 引入跳跃连接，有效地解决了网络过深时候梯度消失的问题，使得设计更深层次的网络变得可行**。

#### **Resnet为啥能解决梯度消失，怎么做的，能推导吗？**

​    由于每做一次卷积（包括对应的激活操作）都会浪费掉一些信息：比如卷积核参数的随机性（盲目性）、激活函数的抑制作用等等。这时，ResNet中的shortcut相当于把以前处理过的信息直接再拿到现在一并处理，起到了减损的效果。

#### **resnet第二个版本做了哪些改进，Resnet性能最好的变体是哪个，结构是怎么样的，原理是什么？**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfoJbjibr9KcTicicibkytMzPl7bMia42yWic8KqpEXyiaGcFq2hvEPb2FgnO2IdibNRJkhgAicRI09gOLJ5vog/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​        **Resnetv2：**

​        1、相比于原始的网络结构，先激活的网络中f是恒等变换，这使得模型优化更加容易。

​        2、使用了先激活输入的网络，能够减少网络过拟合。

​        **Resnet性能最好的变体是Resnext。**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfoJbjibr9KcTicicibkytMzPl7bIEm2s2r0SruzmLqZrQm7qF26J5WgFJhdDlsMsLXsNtL5ibdg0jXyomg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​    ResNeXt可以说是基于Resnet与Inception 'Split + Transfrom + Concat'而搞出的产物，结构简单、易懂又足够强大。（Inception网络使用了一种split-transform-merge思想，即先将输入切分到不同的低维度中，然后做一个特征映射，最后将结果融合到一起。但模型的泛化性不好，针对不同的任务需要设计的东西太多。）

​    ResNeXt提出了一个基数（cardinatity）的概念，用于作为模型复杂度的另外一个度量。基数（cardinatity）指的是一个block中所具有的相同分支的数目。

与 ResNet 相比，相同的参数个数，结果更好：一个 101 层的 ResNeXt 网络，和 200 层的 ResNet 准确度差不多，但是计算量只有后者的一半。

**简述InceptionV1到V4的网络、区别、改进**

- Inceptionv1的核心就是把googlenet的某一些大的卷积层换成1x1, 3x3, 5x5的小卷积，这样能够大大的减小权值参数数量。
- inception V2在输入的时候增加了batch_normal，所以他的论文名字也是叫batch_normal，加了这个以后训练起来收敛更快，学习起来自然更高效，可以减少dropout的使用。
- inception V3把googlenet里一些7x7的卷积变成了1x7和7x1的两层串联，3x3的也一样，变成了1x3和3x1，这样加速了计算，还增加了网络的非线性，减小过拟合的概率。另外，网络的输入从224改成了299.
- inception v4实际上是把原来的inception加上了resnet的方法，从一个节点能够跳过一些节点直接连入之后的一些节点，并且残差也跟着过去一个。另外就是V4把一个先1x1再3x3那步换成了先3x3再1x1.

#### **DenseNet为什么比ResNet有更强的表达能力？**

​    DenseNet在增加深度的同时，加宽每一个DenseBlock的网络宽度，能够增加网络识别特征的能力，而且由于DenseBlock的横向结构类似 Inception block的结构，使得需要计算的参数量大大降低。

#### **为什么要用1\*1卷积？**

​    增加网络的深度（加入非线性）、升维或者是降维、跨通道信息交互（channal 的变换）

#### **padding的作用**

​    ①保持边界信息，如果没有加padding的话，输入图片最边缘的像素点信息只会被卷积核操作一次，但是图像中间的像素点会被扫描到很多遍，那么就会在一定程度上降低边界信息的参考程度，但是在加入padding之后，在实际处理过程中就会从新的边界进行操作，就从一定程度上解决了这个问题。

​    ②可以利用padding对输入尺寸有差异图片进行补齐，使得输入图片尺寸一致。

​    ③在卷积神经网络的卷积层加入Padding，可以使得卷积层的输入维度和输出维度一致。④卷积神经网络的池化层加入Padding，一般都是保持边界信息和①所述一样。

#### **批标准化(Batch Normalization)**

​    可以理解为是一种数据预处理技术，使得每层网络的输入都服从（0，1）0均值，1方差分布，如果不进行BN，那么每次输入的数据分布不一致，网络训练精度自然也受影响。前向公式：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfoJbjibr9KcTicicibkytMzPl7bbf86dBu5zLVwl1MZYL4cLNj6ylMzWNHs1wut2ibU14F4SQ3B8ic6MicMQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​    Batch Norm即批规范化，目的是为了解决每批数据训练时的不规则分布给训练造成的困难，对批数据进行规范化，还可以在梯度反传时，解决梯度消失的问题。

​    Batchnorm也是一种正则的方式，可以代替其他正则方式如dropout，但通过这样的正则化，也消融了数据之间的许多差异信息。

#### **batchnorm的几个参数，可学习的参数有哪些？**

​    第四步加了两个参数γ和β，分别叫做缩放参数和平移参数，通过选择不同的γ和β可以让隐藏单元有不同的分布。这里面的γ和β可以从你的模型中学习，可以用梯度下降，Adam等算法进行更新。

#### **Batch Normalization的作用**

​    神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。而BatchNormalization的作用是**通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题**。

​    ①不仅仅极大提升了训练速度，收敛过程大大加快；

​    ②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；

​    ③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等。

#### **BN层怎么实现**

​    1.计算样本均值。

​    2.计算样本方差。

​    3.样本数据标准化处理。

​    4.进行平移和缩放处理。引入了γ和β两个参数。来训练γ和β两个参数。引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。

#### **BN一般用在网络的哪个部分啊？**

​    先卷积再BN，Batch normalization 的 batch 是批数据, 把数据分成小批小批进行 stochastic gradient descent. 而且在每批数据进行前向传递 forward propagation 的时候, 对每一层都进行 normalization 的处理

#### **BN为什么要重构**

​    恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。

#### **batchnorm训练时和测试时的区别**

​    训练阶段：首先计算均值和方差（每次训练给一个批量，计算批量的均值方差），然后归一化，然后缩放和平移。

​    测试阶段：每次只输入一张图片，这怎么计算批量的均值和方差，于是，就有了代码中下面两行，在训练的时候实现计算好mean、 var，测试的时候直接拿来用就可以了，不用计算均值和方差。

**反向传播：**

​    反向传播需要计算三个梯度值，分别是

![image-20210620224512055](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620224512055.png)

​    定义

![image-20210620224529522](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620224529522.png)

​    为从上一层传递过来的残差。

![image-20210620224600485](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620224600485.png)

​    观察缩放和移位与归一化公式，可以看到从xi到yi的链式计算过程：

![image-20210620224619972](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210620224619972.png)

#### **先加BN还是激活，有什么区别（先激活）**

​    目前在实践上，倾向于把BN放在ReLU后面。也有评测表明BN放ReLU后面效果更好。

#### **pooling如何反向传播**

​    max pooling: 下一层的梯度会原封不动地传到上一层最大值所在位置的神经元，其他位置的梯度为0；

​    average pooling: 下一层的梯度会平均地分配到上一层的对应相连区块的所有神经元。

#### **Pooling的作用和缺点**

​    增大感受野、平移不变性、降低优化难度和参数。缺点:造成梯度稀疏，丢失信息

#### **感受野的理解**：

​    一个卷积核可以映射原始输入图的区域大小。

#### **感受野的计算公式？**

$$
l*{k}=l*{k-1}+\left[\left(f*{k}-1\right) \* \prod*{i=1}^{k-1} s_{i}\right]
$$

​    其中lk−1为第k−1层对应的感受野大小，fk为第k层的卷积核大小，或者是池化层的池化尺寸大小。

#### **反向传播的原理：**

​     它的主要思想是由后一级的误差计算前一级的误差，从而极大减少运算量。

#### **各种数据的channel是指什么意思** 

​    每个卷积层中卷积核的数量

#### **卷积层和全连接层的区别**

全连接层的权重矩阵是固定的，即每一次feature map的输入过来必须都得是一定的大小，所以网络最开始的输入图像尺寸必须固定，才能保证传送到全连接层的feature map的大小跟全连接层的权重矩阵匹配。

卷积层就不需要固定大小了，因为它只是对局部区域进行窗口滑动，所以用卷积层取代全连接层成为了可能。

#### **网络权重初始化**

把w初始化为0、对w随机初始化、Xavier initialization、He initialization

### **Attention**

#### **讲下Attention的原理**

​    减少处理高维输入数据的计算负担,结构化的选取输入的子集,从而降低数据的维度。让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息,从而提高输出的质量。帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系。

#### **Attention有什么缺点**

​    Attention模块的参数都是通过label和预测值的loss反向传播进行更新，没有引入其他监督信息，因而其受到的监督有局限，容易对label过拟合。

#### **dropout的原理**

​    在进行传播的时候删除一些结点，降低网络的复杂性。

​    Dropout在卷积层的正则效果有限。相比较于全连接层，卷积层的训练参数较少，激活函数也能很好地完成特征的空间转换，因此正则化效果在卷积层不明显；
​    Dropout也过时了，能发挥其作用的地方在全连接层，可当代的深度网络中，全连接层也在慢慢被全局平均池化曾所取代，不但能减低模型尺寸，还可以提升性能。

#### **dropout训练和测试有什么区别吗？**

   Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。而在测试时，应该用整个训练好的模型，因此不需要dropout。

​    在训练过程中，从不同数量的“稀疏”网络中删除样本。在测试时，仅使用权重较小的单个未精简网络，就很容易估算出所有这些精简网络的预测结果的平均值。

### 循环神经网络方面

#### LSTM(长短期记忆)原理，其中的参数是否相同/画出LSTM的结构图/写一下LSTM的公式

​    Lstm由输入门,遗忘门,输出门和一个cell组成。第一步是决定从cell状态中丢弃什么信息,然后在决定有多少新的信息进入到cell状态中,最终基于目前的cell状态决定输出什么样的信息。
$$
\begin{array}{l}
i=\sigma\left(W*{i i} x+b*{i i}+W*{h i} h+b*{h i}\right) \
f=\sigma\left(W*{i f} x+b*{i f}+W*{h f} h+b*{h f}\right) \
g=\tanh \left(W*{i g} x+b*{i g}+W*{h g} h+b*{h g}\right) \
o=\sigma\left(W*{i o} x+b*{i o}+W*{h o} h+b*{h o}\right) \
c^{\prime}=f *c+i* g \
h^{\prime}=o * \tanh \left(c^{\prime}\right)
\end{array}
$$
![img](https://tva1.sinaimg.cn/large/006C3FgEly1grmh7nnw0sj30l0082wf3.jpg)

​    LSTM一共有三个门，输入门，遗忘门，输出门，i,f,o分别为三个门的程度参数，g是对输入的常规RNN操作。公式里可以看到LSTM的输出有两个，细胞状态c‘ 和隐状态h'，c'是经输入、遗忘门的产物，也就是当前cell本身的内容，经过输出门得到h'，就是想输出什么内容给下一单元。

#### **LSTM中有哪些激活函数**

​    LSTM中的三个门是用的**sigmoid**作为激活函数，生成候选记忆时候用的才是**tanh**，门j的激活函数如果用relu的话会有个问题，就是relu是没有饱和区域的，那么就没法起到门的作用。候选记忆用tanh是因为tanh的输出在-1~1，是0中心的，并且在0附近的梯度大，模型收敛快。

#### **LSTM这两个激活函数的作用分别是什么** 

​    sigmoid将一个实数输入映射到[0,1]范围内,tanh函数将一个实数输入映射到[-1,1]范围内;

#### **LSTM每个门的计算公式**

$$
遗忘门:
\mathrm{f}=\sigma\left(W{f}\left[h{t-1}, x{t}\right]+b{f}\right)
输
入
门
:

\mathrm{i}=\sigma\left(W{i}\left[h{t-1}, x{t}\right]+b{i}\right)
输
出
门
:

\mathrm{o}=\sigma\left(W{o}\left[h{t-1}, x{t}\right]+b{o}\right)
$$

#### 推导LSTM正向传播和单向传播过程

​    前向推导过程：
$$
Forget Gates
\begin{array}{l}
a{\phi}^{t}=\sum{i=1}^{t} w{i \phi} x{i}^{t}+\sum{h=1}^{H} w{h \phi} b{h}^{t-1}+\sum{c=1}^{C} w{c \phi} s{e}^{t-1} \
b{\phi}^{t}=f\left(a{\phi}^{t}\right)
\end{array}
C
e
l
l
s

\begin{array}{l}
a{c}^{t}=\sum{i=1}^{t} w{i c} x{i}^{t}+\sum{h=1}^{H} w{h c} b{h}^{t-1} \
s{c}^{t}=b{\phi}^{t} s{c}^{t-1}+b{4}^{t} g\left(a{e}^{t}\right)
\end{array}
O
u
t
p
u
t
G
a
t
e
s

\begin{array}{l}
a{\omega}^{t}=\sum{i=1}^{I} w{i \omega} x{i}^{t}+\sum{h=1}^{H} w{h \omega} b{h}^{t-1}+\sum{c=1}^{C} w{c \omega} s{c}^{t} \
b{\omega}^{t}=f\left(a{\omega}^{t}\right)
\end{array}
C
e
l
l
O
u
t
p
u
t
s

b{c}^{t}=b{\omega}^{t} h\left(s_{c}^{t}\right)
$$
反向推导过程：
$$
\epsilon*{e}^{t} \stackrel{\text { def }}{=} \frac{\partial \mathcal{L}}{\partial b*{c}^{t}} \quad \epsilon*{s}^{t} \stackrel{\text { det }}{=} \frac{\partial \mathcal{L}}{\partial s*{c}^{t}}
CellOutputsCellOutputs
\epsilon*{\varepsilon}^{t}=\sum*{k=1}^{K} w*{c k} \delta*{k}^{t}+\sum*{g=1}^{G} u*{c g} \delta*{g}^{t+1}
OutputGatesOutputGates
\delta*{\omega}^{t}=f^{\prime}\left(a*{\omega}^{t}\right) \sum*{c=1}^{C} h\left(s*{c}^{t}\right) \epsilon*{e}^{t}
StatesStates
\epsilon*{x}^{t}=b*{\omega}^{t} h^{\prime}\left(s*{c}^{t}\right) \epsilon*{c}^{t}+b*{\phi}^{t+1} e*{s}^{t+1}+w*{c e} \delta*{k}^{t+1}+w*{c \phi} \delta*{\phi}^{t+1}+w*{c \omega} \delta*{\omega}^{t}
CellsCells
\delta*{c}^{t}=b*{c}^{t} g^{\prime}\left(a*{c}^{t}\right) \epsilon*{s}^{t}
ForgetGatesForgetGates
\delta*{\phi}^{t}=f^{\prime}\left(a*{\phi}^{t}\right) \sum*{c=1}^{C} s*{e}^{t-1} \epsilon*{n}^{t}
InputGatesInputGates
\delta*{i}^{t}=f^{\prime}\left(a*{\imath}^{t}\right) \sum*{c=1}^{C} g\left(a*{c}^{t}\right) \epsilon*{s}^{t}
$$

#### cnn、lstm区别、文本里怎么应用 cnn和lstm各自的区别和应用场景

​    **CNN** 专门解决图像问题的，可用把它看作特征提取层，放在输入层上，最后用MLP 做分类。(**CNN可能更加适合分类问题**)

​    **RNN** 专门解决时间序列问题的，用来提取时间序列信息，放在特征提取层（如CNN）之后。

​    **LSTM**（Long Short-Term Memory）是长短期记忆网络，是一种时间递归神经网络，适合于处理和预测时间序列中间隔和延迟相对较长的重要事件。为了解决RNN中时间上的梯度消失，机器学习领域发展出了长短时记忆单元LSTM，通过门的开关实现时间上记忆功能，并防止梯度消失。

#### rnn原理 循环神经网络，为什么好?

​    循环神经网络模型（RNN）是一种节点定向连接成环的人工神经网络，是一种反馈神经网络，RNN利用内部的记忆来处理任意时序的输入序列，并且在其处理单元之间既有内部的反馈连接又有前馈连接，这使得RNN可以更加容易处理不分段的文本等。

#### 什么是RNN

​    一个序列当前的输出与前面的输出也有关,在RNN网络结构中,隐藏层的输入不仅包括输入层的输出还包含上一时刻隐藏层的输出,网络会对之前的信息进行记忆并应用于当前的输入计算中。

#### **RNN梯度消失问题,为什么LSTM和GRU可以解决此问题**

​    RNN由于网络较深,后面层的输出误差很难影响到前面层的计算,RNN的某一单元主要受它附近单元的影响。而LSTM因为可以通过阀门记忆一些长期的信息,相应的也就保留了更多的梯度。而GRU也可通过重置和更新两个阀门保留长期的记忆,也相对解决了梯度消失的问题。

#### **RNN容易梯度消失，怎么解决？**

1. 梯度裁剪（Clipping Gradient）

​        既然在BP过程中会产生梯度消失（就是偏导无限接近0，导致长时记忆无法更新），那么最简单粗暴的方法，设定阈值，当梯度小于阈值时，更新的梯度为阈值。

​        **优点：简单粗暴**

​        缺点：很难找到满意的阈值

2. LSTM（Long Short-Term Memory）

​        一定程度上模仿了长时记忆，相比于梯度裁剪，最大的优点就是，自动学习在什么时候可以将error反向传播，自动控制哪些是需要作为记忆存储在LSTM cell中。一般长时记忆模型包括写入，读取，和忘记三个过程对应到LSTM中就变成了input_gate,output_gate, forget_gate,三个门，范围在0到1之间，相当于对输入输出进行加权的学习，利用大量数据来自动学习加权的参数（即学习了哪些错误可以用BP更新参数）。具体的公式表达：

1. $$
   1. $i*{t}=\operatorname{Sigm}\left(\theta*{i x} x*{t}+\theta*{i h} h*{t-1}+b*{i}\right)$
   
   2. $o*{t}=\operatorname{Sigm}\left(\theta*{\text {ox }} x*{t}+\theta*{\text {oh }} h*{t-1}+b*{o}\right)$
   
   3. $f*{t}=\operatorname{Sigm}\left(\theta*{f x} x*{t}+\theta*{f h} h*{t-1}+b*{f}\right)$
   
   4. $g*{t}=\operatorname{Tanh}\left(\theta*{g x} x*{t}+\theta*{g h} h*{t-1}+b*{g}\right)$
   
   5. $c*{t}=i*{t} \odot g*{t}+f*{t} \odot c_{t-1}$
   
   6. $h*{t}=o*{t} \odot \operatorname{Tanh}\left(c_{t}\right)$
   $$

   优点：模型自动学习更新参数

#### **LSTM跟RNN有啥区别**，LSTM与RNN的比较

​    RNN在处理long term memory的时候存在缺陷，因此LSTM应运而生。LSTM是一种变种的RNN，它的精髓在于引入了细胞状态这样一个概念，不同于RNN只考虑最近的状态，LSTM的细胞状态会决定哪些状态应该被留下来，哪些状态应该被遗忘。

​    下面来看一些RNN和LSTM内部结构的不同：

​    **RNN**

![img](https://tva1.sinaimg.cn/large/006C3FgEly1grmh7iu1wfj30rn0b0q3a.jpg)

​    **LSTM**

![img](https://tva1.sinaimg.cn/large/006C3FgEly1grmh7g6xxfj30rt0apq3i.jpg)

​    由上面两幅图可以观察到，LSTM结构更为复杂，在RNN中，将过去的输出和当前的输入concatenate到一起，通过tanh来控制两者的输出，它只考虑最近时刻的状态。在RNN中有两个输入和一个输出。

​    而LSTM为了能记住长期的状态，在RNN的基础上增加了一路输入和一路输出，增加的这一路就是细胞状态，也就是途中最上面的一条通路。事实上整个LSTM分成了三个部分：

1. 哪些细胞状态应该被遗忘

2. 哪些新的状态应该被加入

3. 根据当前的状态和现在的输入，输出应该是什么

#### 哪些细胞状态应该被遗忘

​    这部分功能是通过sigmoid函数实现的，也就是最左边的通路。根据输入和上一时刻的输出来决定当前细胞状态是否有需要被遗忘的内容。举个例子，如果之前细胞状态中有主语，而输入中又有了主语，那么原来存在的主语就应该被遗忘。concatenate的输入和上一时刻的输出经过sigmoid函数后，越接近于0被遗忘的越多，越接近于1被遗忘的越少。

#### 哪些新的状态应该被加入

​    继续上面的例子，新进来的主语自然就是应该被加入到细胞状态的内容，同理也是靠sigmoid函数来决定应该记住哪些内容。但是值得一提的是，需要被记住的内容并不是直接concatenate的输入和上一时刻的输出，还要经过tanh，这点应该也是和RNN保持一致。并且需要注意，此处的sigmoid和前一步的sigmoid层的w和b不同，是分别训练的层。

​    细胞状态在忘记了该忘记的，记住了该记住的之后，就可以作为下一时刻的细胞状态输入了。

#### 根据当前的状态和现在的输入，输出应该是什么

​    这是最右侧的通路，也是通过sigmoid函数做门，对第二步求得的状态做tanh后的结果过滤，从而得到最终的预测结果。

​    事实上，LSTM就是在RNN的基础上，增加了对过去状态的过滤，从而可以选择哪些状态对当前更有影响，而不是简单的选择最近的状态。

​    在这之后，研究人员们实现了各种LSTM的变种网络。不变的是，通常都会用sigmoid函数做门，筛选状态或者输入。并且输出都是要经过tanh函数。具体为什么要用这两个函数，由于刚接触还不能给出一定的解释，日后理解了再补充。

#### GRU的原理

​    GRU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。

​    GRU由重置门和跟新门组成,其输入为前一时刻隐藏层的输出和当前的输入,输出为下一时刻隐藏层的信息。重置门用来计算候选隐藏层的输出,其作用是控制保留多少前一时刻的隐藏层。跟新门的作用是控制加入多少候选隐藏层的输出信息,从而得到当前隐藏层的输出。

![img](https://tva1.sinaimg.cn/large/006C3FgEly1grmh79zgutj309h06ldfu.jpg)
$$
\begin{aligned}
z*{t} &=\sigma\left(W*{z} \cdot\left[h*{t-1}, x*{t}\right]\right) \
r*{t} &=\sigma\left(W*{r} \cdot\left[h*{t-1}, x*{t}\right]\right) \
\tilde{h}*{t} &=\tanh \left(W \cdot\left[r*{t} *h\*{t-1}, x\*{t}\right]\right) \
h\*{t} &=\left(1-z\*{t}\right)* h*{t-1}+z*{t} * \tilde{h}_{t}
\end{aligned}
$$
​    GRU有两个门：更新门，输出门

#### **LSTM原理，与GRU区别**

​    LSTM算法全称为Long short-term memory，是一种特定形式的RNN（Recurrent neural network，循环神经网络），而RNN是一系列能够处理序列数据的神经网络的总称。

​    RNN在处理长期依赖（时间序列上距离较远的节点）时会遇到巨大的困难，因为计算距离较远的节点之间的联系时会涉及雅可比矩阵的多次相乘，这会带来梯度消失（经常发生）或者梯度膨胀（较少发生）的问题，这样的现象被许多学者观察到并独立研究。为了解决该问题，研究人员提出LSTM。

​    LSTM是门限RNN，其单一节点的结构如下图1所示。LSTM的巧妙之处在于通过增加输入门限，遗忘门限和输出门限，使得自循环的权重是变化的，这样一来在模型参数固定的情况下，不同时刻的积分尺度可以动态改变，从而避免了梯度消失或者梯度膨胀的问题。

![img](https://tva1.sinaimg.cn/large/006C3FgEly1grmh74vmmfj308h06o3yf.jpg)

​                                                                                                                         LSTM的CELL示意图

​    根据LSTM网络的结构，每个LSTM单元的计算公式如下图2所示，其中Ft表示遗忘门限，It表示输入门限，Ct表示前一时刻cell状态、Ct表示cell状态（这里就是循环发生的地方），Ot表示输出门限，Ht表示当前单元的输出，Ht-1表示前一时刻单元的输出。
$$
\begin{aligned}
f*{t} &=\sigma\left(W*{f} \cdot\left[h*{t-1}, x*{t}\right]+b*{f}\right) \
i*{t} &=\sigma\left(W*{i} \cdot\left[h*{t-1}, x*{t}\right]+b*{i}\right) \
\tilde{C}*{t} &=\tanh \left(W*{C} \cdot\left[h*{t-1}, x*{t}\right]+b*{C}\right) \
C*{t} &=f*{t} \* C*{t-1}+i*{t} \* \tilde{C}*{t} \
o*{t} &=\sigma\left(W*{o}\left[h*{t-1}, x*{t}\right]+b*{o}\right) \
h*{t} &=o*{t} \* \tanh \left(C*{t}\right)
\end{aligned}
$$
​    与GRU区别：1）GRU和LSTM的性能在很多任务上不分伯仲。2）GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。3）从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将hidden state 传给下一个单元，而LSTM则用memory cell 把hidden state 包装起来。

### **free anchor**

### Pytorch中的操作

#### pytorch多gpu训练机制的原理

Pytorch 的多 GPU 处理接口是 torch.nn.DataParallel(module, device_ids)，其中 module 参数是所要执行的模型，而 device_ids 则是指定并行的 GPU id 列表。

并行处理机制是，首先将模型加载到主 GPU 上，然后再将模型复制到各个指定的从 GPU 中，然后将输入数据按 batch 维度进行划分，具体来说就是每个 GPU 分配到的数据 batch 数量是总输入数据的 batch 除以指定 GPU 个数。每个 GPU 将针对各自的输入数据独立进行 forward 计算，最后将各个 GPU 的 loss 进行求和，再用反向传播更新单个 GPU 上的模型参数，再将更新后的模型参数复制到剩余指定的 GPU 中，这样就完成了一次迭代计算。

#### **PyTorch里增加张量维度和减少张量维度的函数**

扩大张量：

```python
torch.Tensor.expand(*sizes) → Tensor
```

压缩张量：

```python
torch.squeeze(input, dim=None, out=None) → Tensor
```

**nn.torch.conv2d()的参数**

```php
class torch.nn.Conv2d(in_channels,out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1,bias=True)
```

- in_channels：输入的通道数目 

- out_channels： 输出的通道数目

- kernel_size：卷积核的大小，类型为int 或者元组，当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽。

- stride： 卷积每次滑动的步长为多少，默认是 1

- padding： 设置在所有边界增加 值为 0 的边距的大小（也就是在feature map 外围增加几圈 0 ），例如当 padding =1 的时候，如果原来大小为 3 × 3 ，那么之后的大小为 5 × 5 。即在外围加了一圈 0 。
- dilation：控制卷积核之间的间距

### Tensorflow中的操作

#### **tensorflow搭建网络和训练的流程**

①训练的数据

②定义节点准备接收数据

③定义神经层：隐藏层和预测层

④定义loss表达式

⑤选择optimizer使loss达到最小

#### **TensorFlow的参数初始化机制**

tf中使用tf.constant_initializer(value)类生成一个初始值为常量value的tensor对象。tf中使用 tf.random_normal_initializer() 类来生成一组符合标准正太分布的tensor。

#### **TensorFlow** **怎么在网络结构实现一个 if 判断** 

布尔类型

#### **Tensorflow中scope的作用**

在tensorflow中使用tf.name_scope()和tf.variable_scope()函数主要是为了变量共享。

### **目标检测篇**

#### faster RCNN原理介绍，要详细画出图

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqD7NYjoQPyzL7NGg8qfcLrVq9XaiatNnzlSYt0rbY3UgF1jIzltwz0X042LxBWX3nEuzs2SbOSNnw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​    Faster R-CNN是一种两阶段（two-stage）方法,它提出的RPN网络取代了选择性搜索（Selective search）算法后使检测任务可以由神经网络端到端地完成。在结构上，Faster RCNN将特征抽取(feature extraction)，候选区域提取（Region proposal提取），边框回归（bounding box regression），分类（classification）都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。

#### RPN（Region Proposal Network）网络的作用、实现细节

​    **RPN网络的作用：**

​         RPN专门用来提取候选框，一方面RPN耗时少，另一方面RPN可以很容易结合到Fast RCNN中，成为一个整体。

​    **RPN网络的实现细节**：

​        一个特征图（Faster RCNN的公共Feature Map）经过sliding window处理，得到256维特征，对每个特征向量做两次全连接操作，一个得到2个分数，一个得到4个坐标{然后通过两次全连接得到结果2k个分数和4k个坐标[k指的是由锚点产生的K个框(K anchor boxes)]}

​        2个分数，因为RPN是提候选框，还不用判断类别，所以只要求区分是不是物体就行，那么就有两个分数，前景（物体）的分数，和背景的分数；4个坐标是指针对原图坐标的偏移，首先一定要记住是原图；

​        预先设定好共有9种组合，所以k等于9，最后我们的结果是针对这9种组合的，所以有H x W x 9个结果，也就是18个分数和36个坐标。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqD7NYjoQPyzL7NGg8qfcLrrq7J3CWJ00J5dJ191vTgXZBejDvPtkWOuY3FIHSo64wG8XWAZgZcPw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### **写一下RPN的损失函数(多任务损失:二分类损失+SmoothL1损失)**

​    训练RPN网络时，对于每个锚点我们定义了一个二分类标签（是该物体或不是）。

​    以下两种情况我们视锚点为了一个正样本标签时：

- 锚点和锚点们与标注之间的最高重叠矩形区域

- 或者锚点和标注的重叠区域指标（IOU）>0.7

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqD7NYjoQPyzL7NGg8qfcLrAJ4JIH3XPvOq1t0JkZ502bausyQlvB2nGsy2MrPqgojBBwQXRg2BzQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



#### **RPN损失中的回归损失部分输入变量是怎么计算的？**

​    (注意回归的不是坐标和宽高，而是由它们计算得到的偏移量)

#### **RPN中的anchor box是怎么选取的？**

​    滑窗的中心在原像素空间的映射点称为anchor，以此anchor为中心，生成k(paper中default k=9, 3 scales and 3 aspect ratios/不同尺寸和不同长宽比)个proposals。三个面积尺寸（128^2，256^2，512^2），然后在每个面积尺寸下，取三种不同的长宽比例（1:1,1:2,2:1）

#### **为什么提出**anchor box？

​    主要有两个原因：一个窗口只能检测一个目标、无法解决多尺度问题。

​    目前anchor box尺寸的选择主要有三种方式：人为经验选取、k-means聚类、作为超参数进行学习

#### 为什么使用不同尺寸和不同长宽比？

​    为了得到更大的交并比(IOU)。

#### 说一下RoI Pooling是怎么做的？有什么缺陷？有什么作用

​    RoI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小固定（w * h）的矩形框

​    **具体操作**：

​        （1）根据输入image，将ROI映射到feature map对应位置

​        （2）将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）

​        （3）对每个sections进行max pooling操作；

​        这样可以从不同大小的方框得到固定大小的相应的feature maps。值得一提的是，输出的feature maps的大小不取决于ROI和卷积feature maps大小。ROI pooling 最大的好处就在于极大地提高了处理速度。（在Pooling的过程中需要计算Pooling后的结果对应到feature map上所占的范围，然后在那个范围中进行取max或者取average。）

​    **优点：** 1.允许我们对CNN中的feature map进行reuse；2.可以显著加速training和testing速度；3.允许end-to-end的形式训练目标检测系统。

​    **缺点：** 由于 RoIPooling 采用的是最近邻插值（即INTER_NEAREST） ，在resize时，对于缩放后坐标不能刚好为整数的情况，采用了粗暴的舍去小数，相当于选取离目标点最近的点，损失一定的空间精度。

#### **两次整数化（量化）过程**

​    1.region proposal的xywh通常是小数，但是为了方便操作会把它整数化。

​    2.将整数化后的边界区域平均分割成 k x k 个单元，对每一个单元边界进行整数化。 //经过上述两次整数化，此时的候选框已经和最开始回归出来的位置有一定的偏差，这个偏差会影响检测或者分割的准确度

#### **怎么做的映射:** 

​    映射规则比较简单，就是把各个坐标除以“输入图片与feature map的大小的比值”

#### **ROI Pooling与ROI Align(Mask R-CNN)的区别**

​    **ROI Align:** ROI Align的思路很简单：取消量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值,从而将整个特征聚集过程转化为一个连续的操作;

​    1.遍历每一个候选区域，保持浮点数边界不做量化。

​    2.将候选区域分割成k x k个单元，每个单元的边界也不做量化。

​    3.在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。

**区别:**

​    ROI Align舍去了近似像素取整数的量化方法，改用双线性插值的方法确定特征图坐标对应于原图中的像素位置.ROI Align很好地解决了ROI Pooling操作中两次量化造成的区域不匹配(mis-alignment)的问题。

​    对于检测图片中大目标物体时，两种方案的差别不大，而如果是图片中有较多小目标物体需要检测，则优先选择RoiAlign，更精准些。

#### **RoI Align中双线性插值计算像素值的具体方法**

​    在数学上，双线性插值是有两个变量的插值函数的线性插值扩展，其核心思想是在两个方向分别进行一次线性插值。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqD7NYjoQPyzL7NGg8qfcLrURw7qMLM6iciaAG4qRib7J3t13hjbw9rLcfVVswOmAMt4Lbk5nTPf77Gg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​    假如我们想得到未知函数 f 在点 P = (x, y) 的值，假设我们已知函数 f 在 Q11 = (x1, y1)、Q12 = (x1, y2), Q21 = (x2, y1) 以及 Q22 = (x2, y2) 四个点的值。最常见的情况，f就是一个像素点的像素值。首先在 x 方向进行线性插值，得到

![image-20210621212251182](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210621212251182.png)

​    然后在 y 方向进行线性插值，得到

![image-20210621212304546](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210621212304546.png)

​    综合起来就是双线性插值最后的结果：

![image-20210621212316093](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210621212316093.png)

​    由于图像双线性插值只会用相邻的4个点，因此上述公式的分母都是1。

​    每个采样点的特征值由其相邻的4个整型特征点的像素值通过双线性差值得到。

​    **最近邻插值法**(图像的内插):在原图中最近得像素点赋值给新的像素点

#### 说一下非极大值抑制（NMS）（non maximum suppression） NMS实现细节 手写NMS代码

​    **用处**：本质是搜索局部极大值，抑制非极大值元素。

​    **原理**:NMS为非极大值抑制，用来抑制检测时冗余的框。

​    **大致算法流程**为：1.对所有预测框的置信度降序排序2.选出置信度最高的预测框，确认其为正确预测，并计算他与其他预测框的IOU 3.根据2中计算的IOU去除重叠度高的，IOU>threshold阈值就删除 4.剩下的预测框返回第1步，直到没有剩下的为止（需要注意的是：Non-Maximum Suppression一次处理一个类别，如果有N个类别，Non-Maximum Suppression就需要执行N次。）

#### **假设两个目标靠的很近，则会识别成一个bbox，会有什么问题，怎么解决？**

​    当两个目标靠的非常近时，置信度低的会被置信度高的框抑制掉，从而两个目标靠的非常近时会被识别成一个bbox。为了解决这个问题，可以使用softNMS（**基本思想**：用稍低一点的分数来代替原有的分数，而不是直接置零）

#### Faster R-CNN是如何解决正负样本不平衡的问题？

​    限制正负样本比例为1:1，如果正样本不足，就用负样本补充，这种方法后面研究工作用的不多。通常针对类别不平衡问题可以从调整样本数或修改loss weight两方面去解决，常用的方法有OHEM、OHNM、class balanced loss和Focal loss。

#### **Faster RCNN怎么筛选正负anchor**

​    我们给两种锚点分配一个正标签：

1. 具有与实际边界框的重叠最高交并比（IoU）的锚点，

2. 具有与实际边界框的重叠超过0.7 IoU的锚点。IoU比率低于0.3，我们给非正面的锚点分配一个负标签。

#### faster-rcnn中bbox回归用的是什么公式，说一下该网络是怎么回归bbox的？

![image-20210621212450235](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210621212450235.png)

​    其中x,y,w,h分别为bbox的中心点坐标，宽与高。分别是预测box、anchor box、真实box。

​    前两行是预测的box关于anchor的offset与scales，后两行是真实box与anchor的offset与scales。那回归的目的很明显，即使ti,ti*尽可能相近。回归损失函数利用的是Fast-RCNN中定义的smooth L1函数，对外点更不敏感：

![image-20210621212828113](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210621212828113.png)

![image-20210621212832649](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210621212832649.png)

​    损失函数优化权重W，使得测试时bbox经过W运算后可以得到一个较好的平移量（offsets）与尺度（scales），利用这个平移量（offsets）与尺度（scales）可在原预测bbox上微调，得到更好的预测结果。

#### **为什么要做Bounding-box regression？**

​    边框回归用来微调候选区域/框，使微调后的框更Ground Truth更接近。

#### 简述faster rcnn的前向计算过程  简述faster rcnn训练步骤

​    输入一张待检测图片->vgg16网络conv layers提取整张图片的特征，输出feature map分别输入到RPN和Fast RCNN网络开头->RPN网络得出region proposal，将这些候选框信息送入到Fast RCNN网络开头->利用候选框在之前送到的feature map提取特征，并通过ROI Pooling层得到规定大小的feature map->将这些feature map送入Fast RCNN网络中进行分类和回归坐标，最终得到需检测物体的坐标。

#### **简述faster rcnn训练步骤**

​    第一步，训练RPN，该网络用ImageNet预训练的模型初始化，并端到端微调，用于生成region proposal；

​    第二步，训练Fast R-CNN，由imageNet model初始化，利用第一步的RPN生成的region proposals作为输入数据，训练Fast R-CNN一个单独的检测网络，这时候两个网络还没有共享卷积层；

​    第三步，调优RPN，用第二步的fast-rcnn model初始化RPN再次进行训练，但固定共享的卷积层，并且只微调RPN独有的层，现在两个网络共享卷积层了；

​    第四步，调优Fast R-CNN,由第三步的RPN model初始化fast-RCNN网络，输入数据为第三步生成的proposals。保持共享的卷积层固定，微调Fast R-CNN的fc层。这样，两个网络共享相同的卷积层，构成一个统一的网络。

#### Faster rcnn有什么不足的地方吗？如何改进？

​    改进：

1. 更好的特征网络ResNet等；

2. 更精确的RPN：可以使用FPN网络架构来设计RPN网络

3. 更好的ROI分类方法：比如ROI分别在conv4和conv5上做ROI-Pooling，合并后再进行分类，这样基本不增加计算量，又能利用更高分辨率的conv4；

4. 使用softNMS代替NMS;

#### **比较FasterRCNN在RCNN系列中的改进点**  

​    RPN提取RP

#### 简要阐述一下One-Stage、Two-Stage模型

​    **One-Stage检测算法**，没有selective search产生region proposal的阶段，直接产生物体的类别概率和位置坐标，经过单次检测即可直接获得最终的检测结果。相比Two-Stage有更快的速度。代表网络有YOLO v1/v2/v3/9000,SSD,Retina-Net. （two-stage算法中的roi pooling会对目标做resize, 小目标的特征被放大，其特征轮廓也更为清晰，因此检测也更为准确）

​    **Two-Stage检测算法**将检测问题划分成两个阶段，首先是获取region proposal进行位置精修和分类阶段。相比于One-Stage,精度高，漏检率也低，但是速度较慢，代表网络有Fast rcnn，Faster rcnn，mask rcnn等。

​    **Two-Stage和One-Stage的异同**（回答的是Two-Stage先对前景背景做了筛选，再进行回归，回归效果比较好，**准度高但是相比较慢**，One-Stage是直接对特征上的点进行直接回归，优点是**速度快**，因为用了多层特征图出框可能小目标效果比较好一点（个人看法），缺点是因为正负样本失衡导致效果较差，要结合难例挖掘。）

​    **one stage在哪些具体方面检测精度不高**（ROI+default box的深层理解）（one-stage算法对小目标检测效果较差，如果所有的anchor都没有覆盖到这个目标，那么这个目标就会漏检。）

####     **Faster rcnn的两阶段训练和end-to-end训练的不一样** 

​    回答的是就是把RPN和二阶段拆开训，然后追问RPN在ENDTOEND中怎么回传，答TOTALLoss中有一阶段和二阶段的LOSS，只是回传影响的部分不一样.

#### **目标检测的发展历程，从传统到深度**

​    传统部分回答的算子结合分类器分类，简单说了一下缺陷，深度部分说了RCNN,FAST,FASTER,SSD,YOLO,FPN,MASK RCNN,Cascade RCNN，都简单的介绍了一下

#### **传统目标检测**：**主线：区域选择->特征提取->分类器**

​    传统的做目标检测的**算法基本流程**如下：

1. 使用不同尺度的滑动窗口选定图像的某一区域为候选区域；

2.  从对应的候选区域提取如Harr HOG LBP LTP等一类或者多类特征；
3.  使用Adaboost SVM 等分类算法对对应的候选区域进行分类，判断是否属于待检测的目标。

​    **缺点**：

1. 基于滑动窗口的区域选择策略没有针对性，时间复杂度高，窗口冗余
2. 手工设计的特征对于多样性的变化没有很好的鲁棒性

#### YOLOV1、YOLOV2、YOLOV3复述一遍  YOLOv1到v3的发展历程以及解决的问题

​    YOLO系列算法是一类典型的one-stage目标检测算法，其利用anchor box将分类与目标定位的回归问题结合起来，从而做到了高效、灵活和泛化性能好。

​    **YOLOv1**：YOLOv1的核心思想就是利用整张图作为网络的输入，直接在输出层回归 bounding box（边界框） 的位置及其所属的类别。

​    YOLOv1的基本思想是把一副图片，首先reshape成448×448大小（由于网络中使用了全连接层，所以图片的尺寸需固定大小输入到CNN中），然后将划分成SxS个单元格（原文中S=7），以每个格子所在位置和对应内容为基础，来预测检测框和每个框的Confidence以及每个格子预测一共C个类别的概率分数。

​    **创新点**：1. 将整张图作为网络的输入，直接在输出层回归bounding box的位置和所属的类别2. 速度快，one stage detection的开山之作

​    **损失函数设计细节**：YOLOv1对位置坐标误差，IoU误差，分类误差均使用了均方差作为损失函数。**激活函数**（最后一层全连接层用线性激活函数，其余层采用leak RELU）

​    **缺点：**

    1.  首先，每个单元格只预测2个bbox，然后每个单元格最后只取与gt_bbox的IOU高的那个最为最后的检测框，也只是说每个单元格最多只预测一个目标。
       2.   损失函数中，大物体 IOU 误差和小物体 IOU 误差对网络训练中 loss 贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的 IOU 误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。
       3.   由于输出层为全连接层，因此在检测时，YOLO 训练模型只支持与训练图像相同的输入分辨率的图片。
       4.  和two-stage方法相比，没有region proposal阶段，召回率较低

​    **YOLOv2**：YOLOv2又叫YOLO9000，其能检测超过9000种类别的物体。相比v1提高了训练图像的分辨率；引入了faster rcnn中anchor box的思想，对网络结构的设计进行了改进，输出层使用卷积层替代YOLO的全连接层，联合使用coco物体检测标注数据和imagenet物体分类标注数据训练物体检测模型。相比YOLO，YOLO9000在识别种类、精度、速度、和定位准确性等方面都有大大提升。

​    **相比于v1的改进：** 

1. Anchor: 引入了Faster R-CNN中使用的Anchor，作者通过在所有训练图像的所有边界框上运行k-means聚类来选择锚的个数和形状(k = 5，因此它找到五个最常见的目标形状) 

2. 修改了网络结构，去掉了全连接层，改成了全卷积结构。

3. 使用Batch Normalization可以从model中去掉Dropout，而不会产生过拟合。
4. 训练时引入了世界树（WordTree）结构，将检测和分类问题做成了一个统一的框架，并且提出了一种层次性联合训练方法，将ImageNet分类数据集和COCO检测数据集同时对模型训练。

​    **YOLOv3**：YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。

​    **改进点**：

1. 多尺度预测 （类FPN）：每种尺度预测3个box, anchor的设计方式仍然使用聚类，得到9个聚类中心。
2. 更好的基础分类网络（类ResNet）和分类器 darknet-53。
3. 用逻辑回归替代softmax作为分类器。

#### **yolo的预测框是什么值**

​    x,y,w,h

#### **YOLOv2中如何通过K-Means得到anchor boxes**

​    卷积神经网络具有平移不变性，且anchor boxes的位置被每个栅格固定，因此我们只需要通过k-means计算出anchor boxes的width和height即可

#### **YOLOv3框是怎么聚出来的？YOLOv3有没有很致命的问题？**

​    yolov3通过聚类的方式自定义anchor box的大小，在一定程度上，这可以提高定位的准确率。

​    **缺点：** 识别物体位置精准性差，召回率低（在每个网格中预测两个bbox这种约束方式减少了对同一目标的多次检测）（

#### **YOLO系列anchor的设计原理，kmeans的原理，anchor距离如何度量，如何改进**k-means原理：

​    K-means算法是很典型的基于距离的聚类算法，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。该算法认为簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。

​    由于从标记文件的width，height计算出的anchor boxes的width和height都是相对于整张图片的比例（w=anchor_width*****input_width/downsamples、h=anchor_height*input_height/downsamples）

#### 简要阐述一下FPN网络具体是怎么操作的  FPN网络的结构

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqD7NYjoQPyzL7NGg8qfcLrJGOAxXAia1c0bLOxkZLN251PHdtO0vLDKxtPjXCibCicgLXRuy6BStnyg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​    FPN网络直接在Faster R-CNN单网络上做修改，每个分辨率的 feature map 引入后一分辨率缩放两倍的 feature map 做 element-wise 相加的操作。通过这样的连接，每一层预测所用的 feature map 都融合了不同分辨率、不同语义强度的特征，融合的不同分辨率的 feature map 分别做对应分辨率大小的物体检测。这样保证了每一层都有合适的分辨率以及强语义（rich semantic）特征。同时，由于此方法只是在原网络基础上加上了额外的跨层连接，在实际应用中几乎不增加额外的时间和计算量。

#### **FPN的特征融合为什么是相加操作呢**？

​    假设两路输入来说，如果是通道数相同且后面带卷积的话，add等价于concat之后对应通道共享同一个卷积核。FPN里的金字塔，是希望把分辨率最小但语义最强的特征图增加分辨率，从性质上是可以用add的。如果用concat，因为分辨率小的特征通道数更多，计算量是一笔不小的开销。所以FPN里特征融合使用相加操作可以理解为是为了降低计算量。

#### **阐述一下FPN为什么能提升小目标的准确率**

​    低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略。原来多数的object detection算法都是只采用顶层特征做预测。FPN同时利用低层特征高分辨率和高层特征的高语义信息，通过融合这些不同特征层的特征达到预测的效果。并且预测是在每个融合后的特征层上单独进行的。所以可以提升小目标的准确率。

#### **基于FPN的RPN是怎么训练的**

​    在FPN的每个预测层上都接一个RPN子网，确定RPN子网的正负anchor box样本，再计算各预测层上RPN的anchor box分类和回归损失，利用BP将梯度回传更新权值

#### 简要阐述一下SSD网络

​    SSD网络的特点是对不同尺度下的feature map中的每一个点都设置一些default box,这些default box有不同的大小和横纵比例，对这些default box进行分类和边框回归的操作。SSD的核心是对固定设置的default box（不同尺度feature map中每一个空间位置都设置一组default box，这里只考虑空间位置，不考虑feature的通道个数）计算属于各类物体的概率以及坐标调整的数值。这个计算方式是对每层的feature map做卷积操作，卷积核设定为3*3，卷积核的个数是与default box个数相关。

​    **优点**：SSD的优点是运行速度超过yolo，精度在一定条件下超过faster rcnn。

​    **缺点**:是需要人工设置先验框（prior box）和min_size，max_size和长宽比（aspect_ratio）值，网络中default_box的基础大小和形状不能直接通过学习获得，而是需要手工设置，虽然使用了图像金字塔的思路，但是对小目标的recall（召回率）依然一般

#### **简述SSD网络前向是如何计算的**

1. 数据增强，获取训练样本，将训练数据统一resize到固定尺寸；

2. 通过卷积网络获取feature map：①使用的卷积网络，前半部分使用基础分类网络获取各层的feature map，这部分称为base network。②下一步计算的输入，就是上述的不同尺寸的feature map；
3. 通过卷积操作，从特征图中获取检测信息。①此处实现方式与yolo类似；②与Faster R-CNN类似，在特征图中每个点新建若干固定尺寸的anchor。检测信息包括每个anchor的信息。主要包括：confidence（代表这个anchor中是否存在物体）、分类信息以及bbox信息。

#### **SSD的致命缺点，如何改进**

​    SSD主要缺点：SSD对小目标的检测效果一般，作者认为小目标在高层没有足够的信息

#### **对小目标检测的改进**

​    可以从下面几个方面考虑：1. 增大输入尺寸2. 使用更低的特征图做检测(比如S3FD中使用更低的conv3_3检测)3. FPN(已经是检测网络的标配了)

#### 简要阐述一下RetinaNet

​    RetinaNet的作者对one-stage检测器准确率不高的问题原因进行探究，发现主要问题在于正负类别不均衡，提出Focal Loss来解决类别不平衡问题。目的是通过减少易分类样本的权重，从而使得模型在训练时更注重于难分类的样本。RetinaNet=ResNet+FPN+Two sub-network+Focal Loss; RetinaNet由backbone网络和两个子任务网络组成，backbone网络负责计算feature map，子任务网络一个负责目标分类，一个负责bbox回归，网络的loss使用Focal loss。

#### **阐述一下ssd和retinanet的区别**

​    SSD的基础网络是VGG，且SSD在使用多层feature map时只是简单的在不同层的feature map上放default box，并没有真正将低维度特征和高维度特征进行融合。且SSD网络中使用的控制正负样本数量比的方法是难样本挖掘方法，loss是分类+回归的loss。

​    而RetinaNet网络的基础网络是resnet+FPN，是真正将低维度的特征和高维度的特征进行了特征融合后再来做检测的。且控制正负样本的方法是使用Focal Loss。

#### **faster rcnn和yolo，ssd之间的区别和联系**

​    1.针对之前RCNN系列selective search的方法导致算法没有实时性，所以faster rcnn提出RPN网络来取代之前的方法，可以理解为fasterrcnn=fast rcnn+rpn网络，且rpn网络和fast rcnn的分类，回归网络共用特征提取层，这样使得引入RPN网络不会增加太多计算量。整体流程为先使用RPN网络找出可能存在object的区域，再将这些区域送入fast rcnn中进一步定位和分类。所以faster rcnn是典型的Two stage算法。因为faster rcnn中包含了两次定位，所以其精度一般高于YOLO和SSD算法，所以速度一般慢于YOLO和SSD。

​    2.YOLO算法的特点是将检测问题转换成回归问题，即YOLO直接通过回归一次既产生坐标，又产生每种类别的概率。YOLO中将每张图分成7*7的网格，每个网格默认可能属于2个object，即在一张图片上提取98个region proposal，相比于faster rcnn使用Anchor机制提取20k个anchor再从中提取最终的300个region proposal，所以faster rcnn的精度比YOLO要高，但是由于需要处理更多region proposal，所以faster rcnn的速度要比YOLO慢。

​    3.SSD相比于faster rcnn使用了多层网络特征，而不仅仅使用最后一层feature map。SSD还借鉴了YOLO算法中将检测任务转换为回归任务的思想，且SSD也借鉴了faster rcnn中的anchor机制，只是SSD的anchor不是每个位置的精调，而是类似于YOLO那样在feature map上分割出网格，在网格上产生anchor。但是SSD和YOLO不需要selective search步骤，所以SSD和YOLO同属于One-Stage算法。

#### **阐述一下Mask RCNN网络，这个网络相比于Faster RCNN网络有哪些改进的地方**

Mask rcnn网络是基于faster rcnn网络架构提出的新的目标检测网络。该网络可以在有效地完成目标检测的同时完成实例分割。Mask RCNN主要的贡献在于如下：

1. 强化了基础网络：通过ResNeXt-101+FPN用作特征提取网络，达到state-of-the-art的效果。

2. ROIAlign替换之前faster rcnn中的ROI Pooling，解决错位（Misalignment）问题。

3. 使用新的Loss Function：Mask RCNN的损失函数是分类，回归再加上mask预测的损失之和。
4. 总结来说，mask rcnn的主要贡献就是采用了ROI Align以及加了一个mask分支。

#### 分析一下SSD,YOLO,Faster rcnn等常用检测网络对小目标检测效果不好的原因

​    SSD，YOLO等单阶段多尺度算法，小目标检测需要较高的分辨率，SSD对于高分辨的低层特征没有再利用，而这些层对于检测小目标很重要。按SSD的设计思想，其实SSD对小目标应该有比较好的效果，但是需要重新精细设计SSD中的default box，比如重新设计min_sizes参数，扩大小default box的数量来cover住小目标。但是随着default box数量的增加，网络速度也会降低。YOLO网络可以理解为是强行把图片分割成7*7个网格，每个网格预测2个目标，相当于只有98个anchor，所以不管是小目标，还是大目标，YOLO的表现都不是很理想，但是由于只需处理少量的anchor，所以YOLO的速度上有很大优势。

​    Faster rcnn系列对小目标检测效果不好的原因是faster rcnn只用卷积网络的最后一层，但是卷积网络的最后一层往往feature map太小，导致之后的检测和回归无法满足要求。甚至一些小目标在最后的卷积层上直接没有特征点了。所以导致faster rcnn对小目标检测表现较差。

#### 手写计算IOU代码

​    有两个框，设第一个框的两个关键点坐标：（x1,y1）(X1,Y1)，第二个框的两个关键点坐标：（x2,y2）(X2,Y2)。以大小写来区分左上角坐标和右下角坐标。首先，要知道两个框如果有交集，一定满足下面这个公式：max(x1,x2)<=min(X1,X2) && max(y1,y2)<=min(Y1,Y2)！！！！

#### 讲一下目标检测优化的方向

​    可以从数据集下手，提升特征表征强度（backbone下手，加深加宽或者换卷积方式），RPN下手（级联，FPN，IOU NET），LOSS（行人检测领域有些问题，如重叠，可以靠修改loss提升准确度）。

#### anchor设置的意义：

​    其实就是多尺度的滑动窗口

#### 如果只能修改RPN网络的话，怎么修改可以提升网络小目标检出率

​    ①修改RPN网络的结构，比如引入FPN结构，利用多层feature map融合来提高小目标检测的精度和召回；

​    ②针对小目标重新精细设计Anchor的尺寸和形状，从而更好地对小目标进行检测；

#### 如何理解concat和add这两种常见的feature map特征融合方式

​    两者都可以理解为整合特征图信息。concat是通道数的增加;add是特征图相加，通道数不变。add是描述图像的特征下的信息量增多了，但是描述图像的维度本身并没有增加，只是每一维下的信息量在增加，这显然是对最终的图像的分类是有益的。而concatenate是通道数的合并，也就是说描述图像本身的特征数（通道数）增加了，而每一特征下的信息是没有增加。concat每个通道对应着对应的卷积核。而add形式则将对应的特征图相加，再进行下一步卷积操作，相当于加了一个先验：对应通道的特征图语义类似，从而对应的特征图共享一个卷积核（对于两路输入来说，如果是通道数相同且后面带卷积的话，add等价于concat之后对应通道共享同一个卷积核）。因此add可以认为是特殊的concat形式。但是add的计算量要比concat的计算量小得多。

#### 1阐述一下如何检测小物体

​    **小目标难以检测的原因**：分辨率低，图像模糊，携带的信息少。

​    ①借鉴FPN的思想，在FPN之前目标检测的大多数方法都是和分类一样，使用顶层的特征来进行处理。虽然这种方法只是用到了高层的语义信息，但是位置信息却没有得到，尤其在检测目标的过程中，位置信息是特别重要的，而位置信息又是主要在网络的低层。因此FPN采用了多尺度特征融合的方式，采用不同特征层特征融合之后的结果来做预测。

​    ②要让输入的分布尽可能地接近模型预训练的分布。先用ImageNet做预训练，之后使用原图上采样得到的图像来做微调，使用微调的模型来预测原图经过上采样的图像。该方法提升效果比较显著。

​    ③采用多尺度输入训练方式来训练网络；

​    ④借鉴Cascade R-CNN的设计思路，优化目标检测中Two-Stage方法中的IOU阈值。检测中的IOU阈值对于样本的选取是至关重要的，如果IOU阈值过高，会导致正样本质量很高，但是数量会很少，会出现样本比例不平衡的影响；如果IOU阈值较低，样本数量就会增加，但是样本的质量也会下降。如何选取好的IOU，对于检测结果来说很重要。

​    ⑤采用分割代替检测方法，先分割，后回归bbox来检测微小目标。

#### 阐述一下目标检测任务中的多尺度

​    输入图片的尺寸对检测模型的性能影响相当明显，事实上，多尺度是提升精度最明显的技巧之一。在基础网络部分常常会生成比原图小数十倍的特征图，导致小物体的特征描述不容易被检测网络捕捉。通过输入更大、更多尺寸的图片进行训练，能够在一定程度上提高检测模型对物体大小的鲁棒性，仅在测试阶段引入多尺度，也可享受大尺寸和多尺寸带来的增益。

​    检测网络SSD中最后一层是由多个尺度的feature map一起组成的。FPN网络中采用多尺度feature map分层融合，分层预测的方法可以提升小目标的检测效果。

#### **阐述一下如何进行多尺度训练**

**多尺度训练可以分为两个方面**:一个是图像金字塔，一个是特征金字塔

1. 人脸检测的MTCNN就是图像金字塔，使用多种分辨率的图像送到网络中识别，时间复杂度高，因为每幅图都要用多种scale去检测。

2. FPN网络属于采用了特征金字塔的网络，一次特征提取产生多个feature map即一次图像输入完成，所以时间复杂度并不会增加多少

3. faster rcnn多个anchor带来的多种尺寸的roi可以算muti scale思想的应用。

#### 如果有很长，很小，或者很宽的目标，应该如何处理目标检测中如何解决目标尺度大小不一的情况  小目标不好检测，有试过其他的方法吗？比如裁剪图像进行重叠

​    小目标不好检测的两大原因：1）数据集中包含小目标的图片比较少，导致模型在训练的时候会偏向medium和large的目标。2）小目标的面积太小了，导致包含目标的anchor比较少，这也意味着小目标被检测出的概率变小。

**改进方法：**

1. 对于数据集中含有小目标图片较少的情况，使用过度采样（oversample）的方式，即多次训练这类样本。
2. 对于第二类问题，则是对于那些包含小物体的图像，将小物体在图片中复制多分，在保证不影响其他物体的基础上，人工增加小物体在图片中出现的次数，提升被anchor包含的概率。
3. 使用FPN；
4. RPN中anchor size的设置一定要合适，这样可提高proposal的准确率。
5. 对于分辨率很低的小目标，我们可以对其所在的proposal进行超分辨率，提升小目标的特征质量，更有利于小目标的检测。

#### 检测的框角度偏移了45度，这种情况怎么处理

​    RRPN也是基于Faster R-CNN，引入RPN，它对比CTPN加入了旋转信息。CTPN只能检测水平文本，而RRPN可以检测任意方向的文本，因为CTPN的提议框是水平的，而RRPN的提议框带有旋转角度。为什么提出旋转的提议框呢？因为水平提议框在检测倾斜文本的时候会带有一些冗余（非文本部分）

### OCR篇

#### CRNN能否识别两行的文字?还是说必须一行？

​    CRNN是一种基于1D-CTC的算法，其原理决定无法识别2行或多行的文字，只能单行识别。

#### 怎么判断行文本图像是否是颠倒的？

​    有两种方案：

1. 原始图像和颠倒图像都进行识别预测，取得分较高的为识别结果。 

2. 训练一个正常图像和颠倒图像的方向分类器进行判断。

#### 目前OCR普遍是二阶段，端到端的方案在业界落地情况如何？

​    端到端在文字分布密集的业务场景，效率会比较有保证，精度的话看自己业务数据积累情况，如果行级别的识别数据积累比较多的话two-stage会比较好。

#### 印章如何识别

    1. 使用带tps的识别网络或abcnet,
       2. 使用极坐标变换将图片拉平之后使用crnn

#### 多语言的字典里是混合了不同的语种，这个是有什么讲究吗？统一到一个字典里会对精度造成多大的损失？

​    统一到一个字典里，会造成最后一层FC过大，增加模型大小。如果有特殊需求的话，可以把需要的几种语言合并字典训练模型，合并字典之后如果引入过多的形近字，可能会造成精度损失，字符平衡的问题可能也需要考虑一下。

#### 预处理部分，图片的长和宽为什么要处理成32的倍数？

​    以检测中的resnet骨干网络为例，图像输入网络之后，需要经过5次2倍降采样，共32倍，因此建议输入的图像尺寸为32的倍数。

#### 类似泰语这样的小语种，部分字会占用两个字符甚至三个字符，请问如何制作字典。

​    处理字符的时候，把多字符的当作一个字就行，字典中每行是一个字。

#### 端到端的场景文本识别方法大概分为几种？

​    端到端的场景文本识别方法大概分为2种：**基于二阶段的方法和基于字符级别的方法。**

- 基于两阶段的方法一般先检测文本块，然后提取文本块中的特征用于识别，例如ABCNet；

- 基于字符级别方法直接进行字符检测与识别，直接输出单词的文本框，字符框以及对应的字符类别，例如CharNet。

#### 二阶段的端到端的场景文本识别方法的不足有哪些？

​    这类方法一般需要设计针对ROI提取特征的方法，而ROI操作一般比较耗时。

#### 基于字符级别的端到端的场景文本识别方法的不足有哪些？

​    这类方法一方面训练时需要加入字符级别的数据，一般使用合成数据，但是合成数据和真实数据有分布Gap。另一方面，现有工作大多数假设文本阅读方向，从上到下，从左到右，没有解决文本方向预测问题。

#### AAAI 2021最新的端到端场景文本识别PGNet算法有什么特点？

​    PGNet不需要字符级别的标注，NMS操作以及ROI操作。同时提出预测文本行内的阅读顺序模块和基于图的修正模块来提升文本识别效果。该算法是百度自研，近期会在PaddleOCR开源。

#### PubTabNet 数据集关注的是什么问题？

​    PubTabNet是IBM提出的基于图片格式的表格识别数据集，包含 56.8 万张表格数据的图像，以及图像对应的 html 格式的注释。该数据集的发布推动了表格结构化算法的研发和落地应用。

#### 在识别模型中，为什么降采样残差结构的stride为(2, 1)？

​    stride为(2, 1)，表示在图像y方向（高度方向）上stride为2，x方向（宽度方向）上为1。由于待识别的文本图像通常为长方形，这样只在高度方向做下采样，尽量保留宽度方向的序列信息，避免宽度方向下采样后丢失过多的文字信息。

#### 支持空格的模型，标注数据的时候是不是要标注空格？中间几个空格都要标注出来么？

**A**：如果需要检测和识别模型，就需要在标注的时候把空格标注出来，而且在字典中增加空格对应的字符。标注过程中，如果中间几个空格标注一个就行。

#### 如果考虑支持竖排文字识别，相关的数据集如何合成？

​    竖排文字与横排文字合成方式相同，只是选择了垂直字体。合成工具推荐：[text_renderer](https://github.com/Sanster/text_renderer)

#### 训练文字识别模型，真实数据有30w，合成数据有500w，需要做样本均衡吗？

​    需要，一般需要保证一个batch中真实数据样本和合成数据样本的比例是1：1~1：3左右效果比较理想。如果合成数据过大，会过拟合到合成数据，预测效果往往不佳。还有一种**启发性**的尝试是可以先用大量合成数据训练一个base模型，然后再用真实数据微调，在一些简单场景效果也是会有提升的。

#### 请问一下，竖排文字识别时候，字的特征已经变了，这种情况在数据集和字典标注是新增一个类别还是多个角度的字共享一个类别？

​    可以根据实际场景做不同的尝试，共享一个类别是可以收敛，效果也还不错。但是如果分开训练，同类样本之间一致性更好，更容易收敛，识别效果会更优。

#### 文本行较紧密的情况下如何准确检测？

​    使用基于分割的方法，如DB，检测密集文本行时，最好收集一批数据进行训练，并且在训练时，并将生成二值图像的shrink_ratio参数调小一些。

#### 当训练数据量少时，如何获取更多的数据？

​    当训练数据量少时，可以尝试以下三种方式获取更多的数据：

1. 人工采集更多的训练数据，最直接也是最有效的方式。

2. 基于PIL和opencv基本图像处理或者变换。例如PIL中ImageFont, Image, ImageDraw三个模块将文字写到背景中，opencv的旋转仿射变换，高斯滤波等。
3. 利用数据生成算法合成数据，例如pix2pix等算法。

#### 论文《Editing Text in the Wild》中文本合成方法SRNet有什么特点？

​    SRNet是借鉴GAN中图像到图像转换、风格迁移的想法合成文本数据。不同于通用GAN的方法只选择一个分支，SRNet将文本合成任务分解为三个简单的子模块，提升合成数据的效果。这三个子模块为不带背景的文本风格迁移模块、背景抽取模块和融合模块。PaddleOCR计划将在2020年12月中旬开源基于SRNet的实用模型。

#### DBNet如果想使用多边形作为输入，数据标签格式应该如何设定？

​    如果想使用多边形作为DBNet的输入，数据标签也应该用多边形来表示。这样子可以更好得拟合弯曲文本。

#### 端到端算法PGNet使用的是什么类型的数据集呢？

​    PGNet目前可以使用四点标注数据集，也可以使用多点标注数据集（十四点），多点标注训练的效果要比四点的好，一种可以尝试的策略是先在四点数据集上训练，之后用多点数据集在此基础上继续训练。

#### 如何更换文本检测/识别的backbone？

​    无论是文字检测，还是文字识别，骨干网络的选择是预测效果和预测效率的权衡。一般，选择更大规模的骨干网络，例如ResNet101_vd，则检测或识别更准确，但预测耗时相应也会增加。而选择更小规模的骨干网络，例如MobileNetV3_small_x0_35，则预测更快，但检测或识别的准确率会大打折扣。幸运的是不同骨干网络的检测或识别效果与在ImageNet数据集图像1000分类任务效果正相关。

1. 文字检测骨干网络的替换，主要是确定类似与ResNet的4个stages，以方便集成后续的类似FPN的检测头。此外，对于文字检测问题，使用ImageNet训练的分类预训练模型，可以加速收敛和效果提升。

2. 文字识别的骨干网络的替换，需要注意网络宽高stride的下降位置。由于文本识别一般宽高比例很大，因此高度下降频率少一些，宽度下降频率多一些。

#### 文本识别训练不加LSTM是否可以收敛？

​    理论上是可以收敛的，加上LSTM模块主要是为了挖掘文字之间的序列关系，提升识别效果。对于有明显上下文语义的场景效果会比较明显。

#### 文本识别中LSTM和GRU如何选择？

​    从项目实践经验来看，序列模块采用LSTM的识别效果优于GRU，但是LSTM的计算量比GRU大一些，可以根据自己实际情况选择。

#### 对于CRNN模型，backbone采用DenseNet和ResNet_vd，哪种网络结构更好？

​    Backbone的识别效果在CRNN模型上的效果，与Imagenet 1000 图像分类任务上识别效果和效率一致。在图像分类任务上ResnNet_vd（79%+）的识别精度明显优于DenseNet（77%+），此外对于GPU，Nvidia针对ResNet系列模型做了优化，预测效率更高，所以相对而言，resnet_vd是较好选择。如果是移动端，可以优先考虑MobileNetV3系列。

#### 训练识别时，如何选择合适的网络输入shape？

​    一般高度采用32，最长宽度的选择，有两种方法：

1. 统计训练样本图像的宽高比分布。最大宽高比的选取考虑满足80%的训练样本。

2. 统计训练样本文字数目。最长字符数目的选取考虑满足80%的训练样本。然后中文字符长宽比近似认为是1，英文认为3：1，预估一个最长宽度。

#### 如何识别文字比较长的文本？

​    在中文识别模型训练时，并不是采用直接将训练样本缩放到[3,32,320]进行训练，而是先等比例缩放图像，保证图像高度为32，宽度不足320的部分补0，宽高比大于10的样本直接丢弃。预测时，如果是单张图像预测，则按上述操作直接对图像缩放，不做宽度320的限制。如果是多张图预测，则采用batch方式预测，每个batch的宽度动态变换，采用这个batch中最长宽度。

#### 识别训练时，训练集精度已经到达90了，但验证集精度一直在70，涨不上去怎么办？

​    训练集精度90，测试集70多的话，应该是过拟合了，有两个可尝试的方法：

1. 加入更多的增广方式或者调大增广prob的[概率](https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/ppocr/data/imaug/rec_img_aug.py#L341)，默认为0.4。

2. 调大系统的[l2 dcay值](https://github.com/PaddlePaddle/PaddleOCR/blob/a501603d54ff5513fc4fc760319472e59da25424/configs/rec/ch_ppocr_v1.1/rec_chinese_lite_train_v1.1.yml#L47)

#### 请问对于图片中的密集文字，有什么好的处理办法吗？

​    可以先试用预训练模型测试一下，例如DB+CRNN，判断下密集文字图片中是检测还是识别的问题，然后针对性的改善。还有一种是如果图象中密集文字较小，可以尝试增大图像分辨率，对图像进行一定范围内的拉伸，将文字稀疏化，提高识别效果。

#### 对于一些在识别时稍微模糊的文本，有没有一些图像增强的方式？

​    在人类肉眼可以识别的前提下，可以考虑图像处理中的均值滤波、中值滤波或者高斯滤波等模糊算子尝试。也可以尝试从数据扩增扰动来强化模型鲁棒性，另外新的思路有对抗性训练和超分SR思路，可以尝试借鉴。但目前业界尚无普遍认可的最优方案，建议优先在数据采集阶段增加一些限制提升图片质量。

#### 对于特定文字检测，例如身份证只检测姓名，检测指定区域文字更好，还是检测全部区域再筛选更好？

​    两个角度来说明一般检测全部区域再筛选更好。

1. 由于特定文字和非特定文字之间的视觉特征并没有很强的区分行，只检测指定区域，容易造成特定文字漏检。

2. 产品的需求可能是变化的，不排除后续对于模型需求变化的可能性（比如又需要增加一个字段），相比于训练模型，后处理的逻辑会更容易调整。

#### 如何识别带空格的英文行文本图像？

​    空格识别可以考虑以下两种方案：

1. 优化文本检测算法。检测结果在空格处将文本断开。这种方案在检测数据标注时，需要将含有空格的文本行分成好多段。

2. 优化文本识别算法。在识别字典里面引入空格字符，然后在识别的训练数据中，如果用空行，进行标注。此外，合成数据时，通过拼接训练数据，生成含有空格的文本。

#### 中英文一起识别时也可以加空格字符来训练吗

​    中文识别可以加空格当做分隔符训练，具体的效果如何没法给出直接评判，根据实际业务数据训练来判断。

#### 低像素文字或者字号比较小的文字有什么超分辨率方法吗

​    超分辨率方法分为传统方法和基于深度学习的方法。基于深度学习的方法中，比较经典的有SRCNN，另外CVPR2020也有一篇超分辨率的工作可以参考文章：Unpaired Image Super-Resolution using Pseudo-Supervision，但是没有充分的实践验证过，需要看实际场景下的效果。

#### 表格识别有什么好的模型 或者论文推荐么

​    表格目前学术界比较成熟的解决方案不多 ，可以尝试下分割的论文方案。

#### 弯曲文本有试过opencv的TPS进行弯曲校正吗？

​    opencv的tps需要标出上下边界对应的点，这个点很难通过传统方法或者深度学习方法获取。

#### 参照文档做实际项目时，是重新训练还是在官方训练的基础上进行训练？具体如何操作？

​    基于官方提供的模型，进行finetune的话，收敛会更快一些。 具体操作上，以识别模型训练为例：如果修改了字符文件，可以设置pretraind_model为官方提供的预训练模型

#### 如何根据不同的硬件平台选用不同的backbone？

​    在不同的硬件上，不同的backbone的速度优势不同，可以根据不同平台的速度-精度图来确定backbone

#### 端到端算法PGNet是否支持中文识别，速度会很慢嘛？

​    目前开源的PGNet算法模型主要是用于检测英文数字，对于中文的识别需要自己训练，大家可以使用开源的端到端中文数据集，而对于复杂文本（弯曲文本）的识别，也可以自己构造一批数据集针对进行训练，对于推理速度，可以先将模型转换为inference再进行预测，速度应该会相当可观。

#### 目前知识蒸馏有哪些主要的实践思路？

​    知识蒸馏即利用教师模型指导学生模型的训练，目前有3种主要的蒸馏思路：

1. 基于输出结果的蒸馏，即让学生模型学习教师模型的软标签（分类或者OCR识别等任务中）或者概率热度图（分割等任务中）。
2. 基于特征图的蒸馏，即让学生模型学习教师模型中间层的特征图，拟合中间层的一些特征。
3. 基于关系的蒸馏，针对不同的样本（假设个数为N），教师模型会有不同的输出，那么可以基于不同样本的输出，计算一个NxN的相关性矩阵，可以让学生模型去学习教师模型关于不同样本的相关性矩阵。

#### CTC的原理

​    CTC是计算一种损失值，主要的优点是可以对没有对齐的数据进行自动对齐。CTC是序列标注问题中的一种损失函数。

#### 如何解决OCR文本过长的问题

​    分段

#### 文本检测中，出现多个box重叠如何处理

​    NMS

### 图像处理

#### 图像滤波

​    在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制。

#### **平滑**

​    也称模糊, 是一项简单且使用频率很高的图像处理方法。平滑的一种作用就是用来减弱噪声。

#### 列出常见的线性滤波器**

​    低通滤波器 允许低频通过

​     高通滤波器 允许高频通过

​    带通滤波器 允许一定范围的频率通过

​    带阻滤波器 允许一定范围的频率通过并阻止其他的频率通过

​    全通滤波器 允许所有频率通过，只改变相位

​    陷波滤波器 阻止一个狭窄频率范围通过

#### **线性滤波与非线性滤波**

​    **线性滤波**：方框滤波 均值滤波 高斯滤波  

​    **非线性滤波**：中值滤波 双边滤波

#### **方框滤波**（boxblur函数）：

​    每一个输出像素的是内核邻域像素值的平均值得到

#### **均值滤波**（Blur函数）：

#####     **均值滤波实际上就是用均值代替原图像中的各个像素值。把每个像素都用周围的8个像素来做均值操作**

​    **原理**：在图像上，对待处理的像素给定一个模板，该模板包括了其周围的邻近像素。将模板中的全体像素的均值来替代原来的像素值的方法。

​    **方法**：对待处理的当前像素，选择一个模板，该模板为其近邻的若干像素组成，用模板中像素的均值来代替原像素值的方法。

​    **优点**：把每个像素都用周围的 8 个像素做均值操作，平滑图像速度快、算法简单。

​    **缺点**：1、在降低噪声的同时，使图像产生模糊，特别是边缘和细节处，而且模糊尺寸越大，图像模糊程度越大。

​               2、对椒盐噪声的平滑处理效果不理想。（无法去掉噪声）不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变模糊，不能很好去除噪声点。

#### **高斯滤波**（Gauss filter）

​    高斯滤波是一种线性平滑滤波，适用于消除高斯噪声，广泛应用于图像处理的减噪过程。

​    高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都由其本身和邻域内的其他像素值经过加权平均后得到。

​    高斯滤波的**具体操作**是：用一个模板（或称卷积、掩模）扫描图像中的每一个像素，用模板确定的邻域内像素的加权平均灰度值去替代模板中心像素点的值用。

​    高斯平滑滤波器对于**抑制服从正态分布的噪声**非常有效。

#### **中值滤波**（Median filter）

​    中值滤波是一种典型的非线性滤波技术，基本思想是用像素点邻域灰度值的中值来代替该像素点的灰度值，该方法在去除脉冲噪声、椒盐噪声的同时又能保留图像边缘细节。

#### **双边滤波**

​    双边滤波（Bilateral filter）是一种非线性的滤波方法，是结合图像的空间邻近度和像素值相似度的一种折衷处理，同时考虑空域信息和灰度相似性，达到保边去噪的目的。具有简单、非迭代、局部的特点。

​    用在对边缘信息重要，需要保留的图像去噪。缺点是由于双边滤波保证了边缘信息，所以其保存了过多的高频信息，对于彩色图像里的高频噪声，双边滤波器不能够干净地滤去，只能对于低频信息进行较好地滤波。

#### **维纳滤波**

​    维纳滤波是一种自适应最小均方差滤波器。维纳滤波的方法是一种统计方法，它用的最优准则是基于图像和噪声各自相关的相关矩阵，它能根据图像的局部方差调整滤波器的输出，局部方差越大，滤波器的平滑作用就越强。

#### **椒盐噪声用什么滤波？** 

中值滤波

#### 边缘检测

​    边缘检测的目的就是找到图像中亮度变化剧烈的像素点构成的集合，表现出来往往是轮廓。如果图像中边缘能够精确的测量和定位，那么，就意味着实际的物体能够被定位和测量，包括物体的面积、物体的直径、物体的形状等就能被测量。

#### **边缘检测算子**：

​    一阶：Roberts Cross算子, Prewitt算子, Sobel算子, Canny算子，罗盘算子

​    二阶：Laplacian算子，Marr-Hildreth,在梯度方向的二阶导数过零点。

#### **介绍canny边缘检测**

​    Canny边缘检测是一种非常流行的边缘检测算法，是John Canny在1986年提出的。它是一个多阶段的算法，即由多个步骤构成。

​    1. 图像降噪

2. 计算图像梯度
3. 非极大值抑
4. 阈值筛选

​    **首先，图像降噪**。我们知道梯度算子可以用于增强图像，本质上是通过增强边缘轮廓来实现的，也就是说是可以检测到边缘的。但是，它们受噪声的影响都很大。那么，我们第一步就是想到要先去除噪声，因为噪声就是灰度变化很大的地方，所以容易被识别为伪边缘。

​    **第二步，计算图像梯度，得到可能边缘**。我们在前面的关于《图像梯度》文章中有所介绍，计算图像梯度能够得到图像的边缘，因为梯度是灰度变化明显的地方，而边缘也是灰度变化明显的地方。当然这一步只能得到可能的边缘。因为灰度变化的地方可能是边缘，也可能不是边缘。这一步就有了所有可能是边缘的集合。

​    **第三步，非极大值抑制**。通常灰度变化的地方都比较集中，将局部范围内的梯度方向上，灰度变化最大的保留下来，其它的不保留，这样可以剔除掉一大部分的点。将有多个像素宽的边缘变成一个单像素宽的边缘。即“胖边缘”变成“瘦边缘”。

​    **第四步，双阈值筛选**。通过非极大值抑制后，仍然有很多的可能边缘点，进一步的设置一个双阈值，即低阈值（low），高阈值（high）。灰度变化大于high的，设置为强边缘像素，低于low的，剔除。在low和high之间的设置为弱边缘。进一步判断，如果其领域内有强边缘像素，保留，如果没有，剔除。

​    这样做的目的是只保留强边缘轮廓的话，有些边缘可能不闭合，需要从满足low和high之间的点进行补充，使得边缘尽可能的闭合。

#### **canny算子是怎么做的？简述Canny算子的计算步骤**

​    ①将彩色图像转化为灰度图；

​    ②使用高斯滤波器平滑图像；

​    ③计算图像梯度的幅值和方向；

​    ④对梯度幅值进行非极大值抑制；

​    ⑤使用双阈值进行边缘的检测和连接；Canny算子使用滞后阈值，滞后阈值需要两个阈值(高阈值和低阈值)。如果某一像素位置的幅值超过 高 阈值, 该像素被保留为边缘像素。如果某一像素位置的幅值小于 低 阈值, 该像素被排除。如果某一像素位置的幅值在两个阈值之间,该像素仅仅在连接到一个高于 高 阈值的像素时被保留。

#### **简述一下sobel算子**

​    Sobel算子是一个主要用作边缘检测的离散微分算子(discrete differentiation operator)。它Sobel算子结合了高斯平滑和微分求导，用来计算图像灰度函数的近似梯度。在图像的任何一点使用此算子，将会产生对应的梯度矢量或是其法矢量。

​    当内核大小为 3 时, 我们的Sobel内核可能产生比较明显的误差(毕竟，Sobel算子只是求取了导数的近似值而已)。为解决这一问题，OpenCV提供了Scharr 函数，但该函数仅作用于大小为3的内核。该函数的运算与Sobel函数一样快，但结果却更加精确。

#### **简述传统算法中边缘检测的一般步骤**

​    ①滤波：滤波去除噪声；

​    ②增强：增强边缘的特征；

​    ③将边缘通过某种方式提取出来，完成边缘检测。

#### **如何求边缘，45°边缘**

​    Sobel算子实现水平边缘检测、垂直边缘检测；45度、135度角边缘检测。

#### **SIFT**

​    尺度不变特征变换(Scale-invariant feature transform, SIFT)是计算机视觉中一种检测、描述和匹配图像局部特征点的方法，通过在不同的尺度空间中检测极值点或特征点(Conrner Point, Interest Point)，提取出其位置、尺度和旋转不变量，并生成特征描述子，最后用于图像的特征点匹配。

#### **SIFT特征是如何保持旋转不变性的？**

​    sift特征通过将坐标轴旋转至关键点的主方向来保持旋转不变性，关键点的主方向是通过统计关键点局部邻域内像素梯度的方向分布直方图的最大值得到的

#### **SIFT特征匹配**

​    对两幅图像中检测到的特征点，可采用特征向量的欧式距离作为特征点相似性的度量，取图像1中某个关键点，并在图像2中找到与其距离最近的两个关键点，若最近距离与次近距离的比值小于某个阈值，则认为距离最近的这一对关键点为匹配点。降低比例阈值，SIFT匹配点数量会减少，但相对而言会更加稳定。阈值ratio的取值范围一般为0.4~0.6。

#### **SIFT特征的特点**

​    SIFT是一种检测、描述、匹配图像局部特征点的算法，通过在尺度空间中检测极值点，提取位置、尺度、旋转不变量，并抽象成特征向量加以描述，最后用于图像特征点的匹配。SIFT特征对灰度、对比度变换、旋转、尺度缩放等保持不变性，对视角变化、仿射变化、噪声也具有一定的鲁棒性。但其**实时性不高**，**对边缘光滑的目标无法准确提取特征点**。

#### **SURF特征匹配**

​    加速鲁棒特征(Speed Up Robust Feature, SURF)和SIFT特征类似，同样是一个用于检测、描述、匹配图像局部特征点的特征描述子。SIFT是被广泛应用的特征点提取算法，但其实时性较差，如果不借助于硬件的加速和专用图形处理器(GPUs)的配合，很难达到实时的要求。对于一些实时应用场景，如基于特征点匹配的实时目标跟踪系统，每秒要处理数十帧的图像，需要在毫秒级完成特征点的搜索定位、特征向量的生成、特征向量的匹配以及目标锁定等工作，SIFT特征很难满足这种需求。SURF借鉴了SIFT中近似简化(DoG近似替代LoG)的思想，将Hessian矩阵的高斯二阶微分模板进行了简化，借助于积分图，使得模板对图像的滤波只需要进行几次简单的加减法运算，并且这种运算与滤波模板的尺寸无关。SURF相当于SIFT的加速改进版本，在特征点检测取得相似性能的条件下，提高了运算速度。整体来说，SUFR比SIFT在运算速度上要快数倍，综合性能更优。

#### **LBP特征**

​    局部二值模式(Local Binary Patter, LBP)是一种用来描述图像局部纹理特征的算子，LBP特征具有灰度不变性和旋转不变性等显著优点，它将图像中的各个像素与其邻域像素值进行比较，将结果保存为二进制数，并将得到的二进制比特串作为中心像素的编码值，也就是LBP特征值。LBP提供了一种衡量像素间邻域关系的特征模式，因此可以有效地提取图像的局部特征，而且由于其计算简单，可用于基于纹理分类的实时应用场景，例如目标检测、人脸识别等。

#### **图像特征提取之HOG特征**

​    方向梯度直方图(Histogram of Oriented Gradient, HOG)特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。Hog特征结合SVM分类器已经被广泛应用于图像识别中，尤其在行人检测中获得了极大的成功。

#### **简要阐述一下SIFT和SURF算法的异同点**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpH4QN9iaib3ovLhCOLRB7ueeC6yjJpL5kHv6uFzC5ibswHmmib9x119Gv1CUnZwnlNVbvmgURgF0alqA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​    **①尺度空间**：SIFT使用DoG金字塔与图像进行卷积操作，而且对图像有做降采样处理；SURF是用近似DoH金字塔(即不同尺度的box filters)与图像做卷积，借助积分图，实际操作只涉及到数次简单的加减运算，而且不改变图像大小。

​    **②特征点检测**：SIFT是先进行非极大值抑制，去除对比度低的点，再通过Hessian矩阵剔除边缘点。而SURF是计算Hessian矩阵的行列式值(DoH)，再进行非极大值抑制。

​    **③特征点主方向**：SIFT在方形邻域窗口内统计梯度方向直方图，并对梯度幅值加权，取最大峰对应的方向；SURF是在圆形区域内，计算各个扇形范围内x、y方向的Haar小波响应值，确定响应累加和值最大的扇形方向。

​    **④特征描述子**：SIFT将关键点附近的邻域划分为4×4的区域，统计每个子区域的梯度方向直方图，连接成一个4×4×8=128维的特征向量；SURF将20s×20s的邻域划分为4×4个子块，计算每个子块的Haar小波响应，并统计4个特征量，得到4×4×4=64维的特征向量。

总体来说，SURF和SIFT算法在特征点的检测取得了相似的性能，SURF借助积分图，将模板卷积操作近似转换为加减运算，在计算速度方面要优于SIFT特征。

#### **比较一下SIFT，HOG和LBP这三个特征提取算法**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfpH4QN9iaib3ovLhCOLRB7ueeEOhdvMyhh5qiav1PfASUlUTQL6c3FogfuT9iavDXLiaaE9Vr2cZicAib4ibQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### **说出几种传统算法中常用的特征检测算法**

​    ①FAST：Fast Feature Detector

​    ②STAR：Star Feature Detector

​    ③SIFT:Scale Invariant Feature Transform

​    ④SURF:Speeded UP Robust Feature 加速版的具有鲁棒性的特征检测算法

​    ⑤ORB:是Oriented Brief的简称，是brief算法的改进版，综合性能相对较好的算法。

#### **简述霍夫变换的原理**

​    使用极坐标表示一条直线，可以由参数极径和极角（r,θ） 表示。霍夫变换就采用这种表示直线的方式。即r=xcosθ+ysinθ  意味着每一对（r,θ）代表一条通过点  （x，y）的直线，如果对于一个给定点（x,y）,我们在极坐标对极径极角平面绘出通过它的直线，可以到一条正弦曲线（r>0 and 0<θ<2π）。对图像上所有点进行上述操作，如果两个不同点进行上述操作后发现曲线相交，则意味着他们通过同一条直线。

​    以上说明，一般来说，一条直线能够通过在平面θ-r上寻找交于一点的曲线数量来检测。越多曲线交于一点则意味着这个交点表示的直线由更多点组成。可以通过设置直线上点的阈值来定义多少条曲线交于一点才能被认为是检测到一条直线。

​    霍夫变换考察图像中每个点对应曲线间的交点，如果交于一点的曲线的数量超过了阈值，则可以认为这个交点所代表的参数对（r，θ）在原图像中为一条直线。

#### **简述霍夫圆变换原理**

​    从平面坐标圆上的点到极坐标转换的三个参数C(x0,y0,r)其中x0,y0是圆心，r 取一固定值时theta扫描360度，x y 跟着变化， 若多个边缘点对应的三维空间曲线交于一点，则他们在共同圆上，在圆心处有累积最大值，也可以用同样的阈值的方法来判断一个圆是否被检测到。

#### **简述opencv中主要有哪些模块？**

- Core —— 核心组件模块

​    作为核心组件，Core 做的事情肯定特别多，也比较基础。包括基本数据结构、动态数据结构、绘图函数、数组操作相关函数、辅助功能与系统函数和宏、XML/YML、聚类、与OpenGL 的交互操作。

- Imgproc 图像处理模块 

​    包括图像滤波、几何图像变换、混合图像变换、直方图、结构分析及形状描述、运动分析及目标跟踪、特征及目标检测。

- Highgui——顶层GUI及视频I/O 

​    包括用户界面、读/写图像及视频、QT新功能。

- Video——视频分析

​    包括运动分析及目标跟踪。

- Calib3d——摄像机标定及3维重建

​    包括摄像机标定及3维重建。

- Features2d——2维特征框架

​    包括特征检测与描述、特征检测提取匹配接口、关键点与匹配点绘图及对象分类。

- Objdetect——目标检测

​    包括级联分类器及SVM.

- MI——机器学习

​    包括统计模型、贝叶斯分类器、最近邻分类器、支持向量机、决策树、提升、梯度提升树、随机树、超随机树、最大期望、神经网络及机器学习数据。

- FLann——聚类及多维空间搜索

​    快速最近邻搜索及聚类。

- Gpu——计算机视觉中GPU加速

​    GPU模块及数据结构，包含图像处理与分析模块。

- Photo——计算图像

​    图像修复及去噪。

- Stitching——图像拼接

​    图像拼接顶层操作函数、旋转、自动标定、仿射变换、接缝估计、曝光补充及图像融合技术。

#### **opencv中CV_8UC3代表什么意思？**

​    8表示8位，UC--代表--unsigned int--无符号整形，3 -代表一张图片的通道数3

#### **简述opencv中的Scalar类**

​    Scalar（）表示具有4个元素的数组，在opencv中被大量用于传递像素值，比如RGB颜色值。如果用不到第四个参数，则不需要写出来，若只写三个参数，则opencv会认为只需要传递三个参数。

#### **简述.hpp和.h的区别**

​    .hpp，本质就是将.cpp的实现代码混入.h头文件当中，定义与实现都包含在同一文件，则该类的调用者只需要include该.hpp文件即可，无需再将cpp加入到project中进行编译。而实现代码将直接编译到调用者的obj文件中，不再生成单独的obj，采用hpp将大幅度减少调用project中的cpp文件数与编译次数，也不用再发布lib与dll文件，因此非常适合用来编写公用的开源库。

#### **简述一下什么是光流？**

​    光流(Optical flow or optic flow)是关于视域中的物体运动检测中的概念。用来描述相对于观察者的运动所造成的观测目标、表面或边缘的运动。

#### **简述常见的颜色系统**

​    ①RGB是最常见的颜色系统,采用人眼相似的工作机制，也被显示设备所采用。

​    ②HSV和HLS把颜色分解成色调，饱和度和亮度/明度，描述颜色更加自然，可以通过抛弃最后一个元素使算法对输入图像的光照条件不敏感。

​    ③YCrCb颜色系统在JPEG图像格式中广泛使用。

​    ④CIELab是一种在感知上均匀的颜色空间，适合用来度量两个颜色之间的距离。

#### **简述访问图像中像素的3种方法**

​    ①指针访问，C操作符[]；

​    ②迭代器iterator；

​    ③动态地址计算。

#### **简述一下图像处理中的膨胀和腐蚀操作**

​    膨胀和腐蚀都是对白色（高亮）部分进行操作的。膨胀是图像中的高亮部分进行膨胀，效果图拥有比原图更大的高亮区域，腐蚀是原图中的高亮部分被腐蚀，效果图拥有比原图更小的高亮区域。从数学原理上说，膨胀就是求局部最大值，并把这个最大值赋值给参考点指定像素，这样会使图像中高亮区域逐渐增长，腐蚀与之相反。

#### **简述开运算的操作流程和使用场景**

​    开运算就是先腐蚀后膨胀的过程。可以用来消除小物体，在纤细点处分离物体，并且在平滑较大物体的边界的同时不明显改变其面积。

#### **简述闭运算的操作流程和使用场景**

​    闭运算就是先膨胀后腐蚀的过程，闭运算能够排除小型黑洞(黑色区域)。

#### **简述形态学梯度的定义和使用场景**

​    形态学梯度是膨胀图与腐蚀图之差，对二值图进行这一操作可以将团块（blob）的边缘突出出来，可以用形态学梯度来保留物体的边缘轮廓。

#### **简述顶帽（礼帽）运算的定义和使用场景**

顶帽是原图像与开运算的结果图之差。因为开运算的结果是放大了裂缝或者局部低亮度的区域，所以从原图中减去开运算之后的图，得到的效果图突出了比原图轮廓周围区域更加明亮的区域。顶帽运算常用来分离比临近点亮一些的斑块。在一幅图像具有大幅的背景，而微小物品比较有规律的情况下，可以使用顶帽运算进行背景提取。

#### **简述黑帽运算的定义和使用场景**

​    黑帽（Black Hat）运算为闭运算结果图与原图像之差。黑帽运算后的效果图突出了比原图轮廓周围的区域更暗的区域，且这一操作和选择的核的大小相关。所以，黑帽运算用来分离比邻近点暗一些的斑块。效果图有着非常完美的轮廓。

#### **简述一下漫水填充法**

​    漫水填充法是一种用特定的颜色填充算法填充连通区域，通过设置可连通像素的上下限以及连通方式来达到不同的填充效果的方法。漫水填充经常用来标记或者分离图像的一部分进行处理或分析。简单说，就是自动选中和种子点相连的区域，接着将该区域替换成指定的颜色。

#### **简述一下仿射变换的定义**

​    仿射变换，又称仿射映射，是指在几何中，一个向量空间进行一次线性变换并接上一个平移，变换为另一个向量空间的过程。它保持了二维图形的平直性（直线变换之后依然是直线）和平行性。一个任意的仿射变换都能表示为乘以一个矩阵（线性变换）接着再加上一个向量（平移）的形式。

#### **简述一些图像金字塔的种类和区别**

​    一般情况下有两种类型的图像金字塔，分别是：**高斯金字塔**(Gaussianpyramid): 用来向下采样，主要的图像金字塔；**拉普拉斯金字塔**(Laplacianpyramid): 用来从金字塔低层图像重建上层未采样图像，在数字图像处理中也即是预测残差，可以对图像进行最大程度的还原，配合高斯金字塔一起使用。两者的简要区别：高斯金字塔用来向下降采样图像，拉普拉斯金字塔用来从金字塔低层图形中向上采样，重建一个图像可以将拉普拉斯金字塔理解为高斯金字塔的逆形式。

#### **简述凸包的定义**

​    给定二维平面上的点集，凸包就是将最外层的点连接起来构成的凸多边形，它能包含点集中所有的点。理解物体形状或轮廓的一种比较有用的方法便是计算一个物体的凸包。

#### **简述反向投影的定义和使用场景**

​    反向投影是一种记录给定图像中的像素点如何适应直方图模型像素分布的方式。简单的讲，就是首先计算某一特征的直方图模型，然后使用模型去寻找图像中存在的该特征。反向投影用于在输入图像（通常较大）中查找与特定图像（通常较小）最匹配的点或者区域，也就是定位模板图像出现在输入图像的位置。

#### **简述harris角点检测算法原理和使用场景**

​    harris角点检测时一种直接基于灰度图像的角点提取算法，稳定性高，尤其对于L型角点检测精度高。但是由于采用了高斯滤波，运算速度相对比较的慢，角点信息有丢失和位置偏移的现象，而且角点提取有聚簇现象。

#### **简述一下分水岭算法**

​    分水岭算法是一种图像区域分割法，在分割的过程中，它会把跟临近像素间的相似性作为重要的参考依据，从而将在空间位置上相近并且灰度值相近的像素点互相连接起来构成一个封闭的轮廓，封闭性是分水岭算法的一个重要特征。分水岭算法常用的操作步骤：分水岭算法常用的操作步骤：彩色图像灰度化，然后再求梯度图，最后在梯度图的基础上进行分水岭算法，求得分段图像的边缘线。

### 算法

#### **数组和链表的区别**

​    **数组**是将元素在内存中连续存放，由于每个元素占用内存相同，可以通过下标迅速访问数组中任何元素。

​    **链表**是一种上一个元素的引用指向下一个元素的存储结构，链表通过指针来连接元素与元素；

​    （1）数组是连续存储的，链表是散列存储的。数组随机访问性强（通过下标进行快速定位），所以数组的查询比链表要快，链表不能随机查找，必须从第一个开始遍历，查找效率低。

​    （2）数组插入和删除效率低（插入和删除需要移动数据），链表插入删除速度快（因为有next指针指向其下一个节点，通过改变指针的指向可以方便的增加删除元素）

#### **堆,栈,堆栈,队列**

​    堆(heap)：堆是一种经过排序的树形数据结构，每个结点都有一个值。堆通常是一个可以被看做一棵树的数组对象。堆总是满足下列性质：堆中某个节点的值总是不大于或不小于其父节点的值,堆总是一棵完全二叉树。

​    栈(stack): 它是一种具有后进先出性质的数据结构，也就是说后存放的先取，先存放的后取。

​    堆栈本身就是栈.队列是先进先出，有出口和入口。

#### **堆和栈的区别：**

​    ①堆栈空间分配区别：

​        1）、栈（操作系统）：由操作系统自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈；

​        2）、堆（操作系统）：一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收，分配方式倒是类似于链表。

​    ②堆栈缓存方式区别：

​        1）、栈使用的是一级缓存，他们通常都是被调用时处于存储空间中，调用完毕立即释放；

​        2）、堆是存放在二级缓存中，生命周期由虚拟机的垃圾回收算法来决定（并不是一旦成为孤儿对象就能被回收）。所以调用这些对象的速度要相对来得低一些。

​    堆：内存中，存储的是引用数据类型，引用数据类型无法确定大小，堆实际上是一个在内存中使用到内存中零散空间的链表结构的存储空间，堆的大小由引用类型的大小直接决定，引用类型的大小的变化直接影响到堆的变化

​    栈：是内存中存储值类型的，大小为2M，超出则会报错，内存溢出

​    堆栈数据结构区别：

​        堆（数据结构）：堆可以被看成是一棵树，如：堆排序；

​        栈（数据结构）：一种先进后出的数据结构。特点：先进后出

#### **堆和栈的访问哪个更快**

​    栈是编译时分配空间，而堆是动态分配（运行时分配空间），所以栈的速度快。

#### **快排和堆排**

​    快速排序：最常用的排序算法，速度通常也是最快的。时间复杂度：O（nlogn）

​    最坏：O（n^2） 空间复杂度：O（nlgn）  不稳定（比如 5 3 3 4 3 8 9 10 11 这个序列，在中枢元素5和3交换就会把元素3的稳定性打乱）

​    实现原理：快排主要是通过选择一个关键值作为基准值。比基准值小的都在左边序列（一般是无序的），比基准值大的都在右边（一般是无序的）。依此递归，达到总体待排序序列都有序。

#### 堆排序：

​    堆排序是指利用堆这种数据结构进行设计的一种排序算法。堆排序利用了大根堆(或小根堆)堆顶记录的关键字最大(或最小)这一特征，使得在当前无序区中选取最大(或最小)关键字的记录变得简单。

​    时间复杂度：O（n*logn）特别适用于数据量很大的场合（百万级数据）。因为快排和归并排序都是基于递归的，数据量很大的情况下容易发生堆栈溢出。排序速度略低于快排。也是一种不稳定的排序算法。

#### **介绍快排，描述一下最坏的情况**

​    时间复杂度：最好情况O(nlogn)——Partition函数每次恰好能均分序列，其递归树的深度就为.log2n.+1（.x.表示不大于x的最大整数），即仅需递归log2n次；最坏情况O（n^2）,每次划分只能将序列分为一个元素与其他元素两部分，这时的快速排序退化为冒泡排序，如果用数画出来，得到的将会是一棵单斜树，也就是说所有所有的节点只有左（右）节点的树；平均时间复杂度O(nlogn)。

​    解释一下快排的思路，时间复杂度，稳定吗？（略，不稳定） 稳定的排序都有哪些？（插，归并，冒泡） 解释一下堆排序？（不断得维护一个最大/小堆，时间复杂度nlgn）

#### **快排和堆排的优缺点和应用场景**

​    a : 时间复杂度都是o(nlogn)

​    b : 效率: 快排 >归并>堆排

​    c : 三种算法的优缺点:快排: 极端情况下排序效率很低

#### 归并

​    需要额外的内存开销，堆排序: 在快的排序算法中,相对较慢, 但应用很广.

#### **知道哪些排序算法 排序的空间复杂度 各种排序算法的原理**

​    冒泡排序、简单选择、直接插入、快速排序、堆排序、希尔排序、归并排序、基数排序。

​    冒泡排序：每当相邻的两个数比较后发现它们的排序与排序要求相反时，就将它们互换。

​    快速排序：选择一个基准元素，通常选择第一个元素或者最后一个元素，通过一趟扫描，将待排序的元素分成两部分，一部分比基准元素小，一部分大于等于基准元素，此时基准元素在其排好序后的正确位置，然后再用同样的方法递归地排序划分的两部分。

​    简单选择排序：在要排序的一组数中，选出最小的一个数与第一个位置的数交换；然后在剩下的数当中再找最小的与第二个位置的数交换，如此循环到倒数第二个数和最后一个数为止。

​    堆排序：堆排序是一种树形选择排序，是对直接排序的有效改进。

​    直接插入排序：在要排序的一组数中，假设前面（n-1）[n>=2]个数已经是排好顺序的，现在要把第n个数插到前面的有序数中，使得这n个数也是排好顺序的。如此反复循环，直到全部排好顺序。

​    希尔排序：先将要排序的一组数按某个增量d（n/2,n为要排序数的个数）分成若干组，每组记录的下标相差d，对每组中全部元素进行直接插入排序，然后再用一个较小的增量(d/2)对它进行分组，在每组中再进行直接插入排序。当增量减至1时，进行直接插入排序后，排序完成。

​    归并排序：归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把带排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。

​    基数排序：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqG5ic0dhRjzFbic3Yd4CiaZxksUkd9OUsHZo3R7dicYvJ2QKYEAFC8DLO5RtcYq8kbO51iaXx5BlGsnhg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### **二叉树、平衡二叉树、完全二叉树、满二叉树**

​    二叉树的概念：一棵二叉树是节点的一个有限集合，该集合或者为空，或者由一个根节点加上两棵左子树和右子树组成。

​    平衡二叉树，又称AVL树。它或者是一棵空树，或者是具有下列性质的二叉树：它的左子树和右子树都是平衡二叉树，且左子树和右子树的高度之差之差的绝对值不超过1.。

​    满二叉树：一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。也就是说，如果一个二叉树的层数为K，且结点总数是(2^k) -1 ，则它就是满二叉树。

​    完全二叉树：叶子节点只能分布在树的倒数第1层和倒数第二层，倒数第二层的节点必须是满的，倒数第一层的节点可以不全是满的，但是所有的节点都只能集中在树的左侧。这也说明，倒数第二层的节点肯定不会出现只有右子树，没有左子树的情况。在构建完全二叉树时，插入节点一定先插入左子树，再插入右子树。

#### **为什么要构造成二叉树，N叉树可不可以**

​    二叉树是按值来保存元素，也按值来访问元素。

#### **红黑树**

​    红黑树是一种自平衡树，它也是一颗二叉树。既然能保持平衡，说明它和 AVL 树类似，在插入或者删除时肯定有调整的过程，只不过这个调整过程并不像 AVL 树那样繁琐。为何红黑树使用得比 AVL 树更多，就是因为红黑树它的调整过程迅速且简介。

#### 红黑树有以下五个特性：

​    性质1：节点是红色或黑色

​    性质2：根是黑色

​    性质3：所有叶子都是黑色。叶子是 NIL 节点，也就是 Null 节点

​    性质4：如果一个节点是红的，则它的两个儿子都是黑的

​    性质5：从任一节点到其叶子的所有简单路径都包含相同数目的黑色节点。

#### **递归有什么缺点**

当递归层数很多的时候，容易造成内存溢出

#### **遇到哈希冲突怎么办**

​    ①开放定址法：为产生冲突的地址求得一个地址序列(),其中。其中m为表的长度,而增量有三种取值方法,线性探测再散列,平方探测再散列,随即探测再散列。

​    ②链地址法：将所有Hash地址相同的记录都链接在同一链表中，再Hash法，同时构造多个不同的Hash函数,当产生冲突时,计算另一个Hash函数地址直到不再发生冲突为止。

​    ③建立公共溢出区：将Hash表分为基本表和溢出表,若是与基本表发生冲突,都放入溢出表。

#### **跳表**

​    跳表是一个随机化的数据结构，实质就是一种可以进行二分查找的有序链表，跳表在原有的有序链表上面增加了多级索引，通过索引来实现快速查找，跳表不仅能提高搜索性能，同时也可以提高插入和删除操作的性能。

#### **动态规划和分治的区别与联系，各自适应哪些情况**

​    **动态规划**：通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。动态规划常常适用于有重叠子问题和最优子结构性质的问题。

​    **分治法**的基本思想：将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便各个击破，分而治之。

​    **共同点：**二者都要求原问题具有最优子结构性质,都是将原问题分而治之,分解成若干个规模较小(小到很容易解决的程序)的子问题.然后将子问题的解合并,形成原问题的解.

​    **不同点：**分治法将分解后的子问题看成相互独立的，通过用递归来做。

动态规划将分解后的子问题理解为相互间有联系,有重叠部分，需要记忆，通常用迭代来做。

#### **图的遍历方式**

​    从图中某一顶点出发访遍图中其余顶点，且使每一个顶点仅被访问一次，这一过程就叫做图的遍历。根据遍历路径的不同，通常有两种遍历图的方法：深度优先遍历和广度优先遍历。它们对无向图和有向图都适用。图的遍历算法是求解图的连通性问题、拓扑排序和求关键路径等算法的基础。

### python

#### **python is和==的区别**

​    is是用来判断两个变量引用的对象是否为同一个,==用于判断引用对象的值是否相等。可以通过id()函数查看引用对象的地址。

#### **list和tuple的区别？**

​    list是一种有序的集合，可以随时添加和删除其中的元素。tuple是一种有序列表，它和list非常相似。tuple一旦初始化就不能修改，而且没有append() insert()这些方法，可以获取元素但不能赋值变成另外的元素。

不同点：list是可更改的，所以，可以insert，pop等，但是tuple是不可更改的，所以没有增减函数，但是其余的查询len()，index()等函数都是一样的。

#### **Python里面的字典的key可以用list吗？可以用tuple吗？可以用set吗？**

​    一个对象能不能作为字典的key，就取决于其有没有__hash__方法。所以所有python自带类型中，除了list、dict、set和内部至少带有上述三种类型之一的tuple之外，其余的对象都能当key。

#### **讲一下yield关键字？它的作用是啥？**

​    yield是一个常用于python函数定义中的关键字，它的作用是返回一个可以用来迭代（for循环）的生成器，它的应用场景通常为一个需要返回一系列值的，含有循环的函数中。

#### **python是解释语言还是编译语言**

​    python 是解释型的编程语言

#### ** xrange与range的区别**

​    xrange 用法与 range 完全相同，所不同的是生成的不是一个list对象，而是一个生成器。

**Python里面的lambda表达式写一下，随便写一个**

​    定义一个lambda表达式，求三个数的和：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqG5ic0dhRjzFbic3Yd4CiaZxkKGDBfHiaP98kquVuX5UAiclCyicx7heBQH5Vo1Ya9W3KO3bVHFYVsaOJA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### **Python里面的三元运算符写一下**

(x = 1 if x> y else 0)

#### **python和java是解释语言还是编译语言**

​    Java编译性，python解释性。

#### **Python字典采用的是什么数据结构？**

​    使用的是key-value匹配的哈希结构

#### **Python的多线程能否用来做并行计算**？

​    不能，它有GIL锁，但可以用多进程实现并行

**Python中0.35/0.05等于多少？**

​    (python中/和//的区别)" / " 表示浮点数除法，返回浮点结果;" // " 表示整数除法,返回不大于结果的一个最大的整数

#### **python传参会改变原值吗**

​    当我们传的参数是int、字符串(string)、float、（数值型number）、元组（tuple) 时，无论函数中对其做什么操作，都不会改变函数外这个参数的值；

​    当传的是字典型(dictionary)、列表型(list)时，如果是重新对其进行赋值，则不会改变函数外参数的值，如果是对其进行操作，则会改变。即变量中存储的是引用 , 是指向真正内容的内存地址 , 对变量重新赋值 , 相当于修改了变量副本存储的内存地址 , 而这时的变量已经和函数体外的变量不是同一个了, 在函数体之外的变量 ,     依旧存储的是原本的内存地址 , 其值自然没有发生改变 。

#### **Python打开一个文件，找出某个字符串最快的方法**

​    python 字符串查找有4个方法，1 find, 2 index方法，3 rfind方法,4 rindex方法。

#### **python set的基本操作**

​    集合常用的两个场景是：1.去重（如：列表去重）；2.关系测试（如：取交集、取并集、取差集等）

### C++

#### **指针和引用的区别**

​    指针指向一块内存，它的内容是内存中的地址，而引用则是某个内存的别名，它的指向不改变。

​    本质区别：指针有自己的一块空间（地址），而引用只是一个别名。

​    使用sizeof看一个指针的大小是4（64位是8），而引用是被引用对象的大小。指针可以被初始化为NULL，而引用必须被初始化且必须是一个已有对象的引用。可以有const指针，但是没有const引用。指针在使用中可以指向其他对象，但是引用还只能是一个对象的引用，不能被改变。如果返回动态内存分配的对象或者内存，必须使用指针，引用可能引起内存泄漏。

#### **智能指针**

​    c++里面的四个智能指针: auto_ptr, shared_ptr, weak_ptr, unique_ptr

#### **多态的作用、多态机制**

​    接口的多种不同的实现方式即为多态。多态性是允许你将父对象设置成为一个或更多的他的子对象相等的技术。多态就是当发出一条命令时，不同的对象接收到同样的命令后所做出的动作是不同的。

​    **机制**：多态通常有两种实现方法：子类继承父类（extends）、类实现接口（implements）；无论是哪种方法，其核心之处就在于对父类方法的改写或对接口方法的实现，以取得在运行时不同的执行效果。要使用多态，在声明对象时就应该遵循一条法则：声明的总是父类类型或接口类型，创建的是实际类型。

​    **作用**：多态最大的用途我认为在于对设计和架构的复用。

#### **虚函数**

​    虚函数：实现类的多态性。

​    定义一个函数为虚函数，不代表函数为不被实现的函数。定义他为虚函数是为了允许用基类的指针来调用子类的这个函数。定义一个函数为纯虚函数，才代表函数没有被实现。

​    C++中的虚函数的作用主要是实现了多态的机制。基类定义虚函数，子类可以重写该函数；在派生类中对基类定义的虚函数进行重写时，需要在派生类中声明该方法为虚方法。

​    C++**里调用虚函数比调用普通函数慢：**普通函数快，因为地址在编译期间指定，单纯的寻址调用。虚函数调用时，首先找虚函数表，然后找偏移地址进行调用。

#### **虚函数表放在哪里，虚表指针什么时候初始化**

​    虚函数表和静态成员变量一样,存放在全局数据区.

​    在构造函数中进行虚表的创建和虚表指针的初始化。

#### **纯虚函数（纯虚函数在子类中没有实现会不会报错：会）**

​    纯虚函数是在基类中声明的虚函数，它在基类中没有定义，但要求任何派生类都要定义自己的实现方法。在基类中实现纯虚函数的方法是在函数原型后加 =0:

​    定义纯虚函数是为了实现一个接口，起到一个规范的作用，规范继承这个类的程序员必须实现这个函数。

#### **虚函数和纯虚函数的区别**

​    含有纯虚函数的类称为抽象类,只含有虚函数的类不能称为抽象类。虚函数可以直接被使用,也可以被子类重载以后以多态形式调用,而纯虚函数必须在子类中实现该函数才可使用,因为纯虚函数在基类中只有声明而没有定义。虚函数必须实现,对虚函数来说父类和子类都有各自的版本。

#### **继承是什么意思？**

​    继承是指一个对象直接使用另一对象的属性和方法。

​    继承机制是面向对象程序设计使代码可以复用的最重要的手段，它允许程序员在保持原有的特性基础上进行扩展，增加功能，这样产生新的类，称作是派生类。继承呈现了面向对象程序设计的层析结构，体现了由简单到复杂的认知过程。继承是类设计层次的复用。

#### **C++的继承和Java继承的区别？**

​    c++支持多重继承，java不支持多继承；

#### **虚继承**

​    当我们在继承方式前加virtual 关键字，进行虚继承，虚继承内存中会通过虚基类指针指向虚基类表，该表中的数据是为虚基类指针到基类的存储区域的偏移值。虚继承主要解决内存重复的问题，同时避免访问冲突。

#### **枚举类能否继承**

​    枚举的作用：限定“数据集”中的元素的个数(将类理解为一个集合)、即限定枚举类对象的个数。枚举类也是类，也可以有自己的成员变量，成员方法，静态方法、静态变量等，也能实现其他的接口，不能继承其他类了。

#### **类中继承的方式，为什么会有这几种方式，作用是什么**

​    public 继承：父类成员在子类中保持原有访问级别；

​    private 继承：父类成员在子类中变为私有成员；

​    protected 继承：父类中的公有成员变为保护成员，其它成员保持不变；

#### **深拷贝浅拷贝区别**

​    假设B复制了A，修改A的时候，看B是否发生变化：如果B跟着也变了，说明是浅拷贝，拿人手短！（修改堆内存中的同一个值）;如果B没有改变，说明是深拷贝，自食其力！（修改堆内存中的不同的值）。

​    浅拷贝（shallowCopy）只是增加了一个指针指向已存在的内存地址，深拷贝（deepCopy）是增加了一个指针并且申请了一个新的内存，使这个增加的指针指向这个新的内存。

​    **区别**：深复制和浅复制最根本的区别在于是否是真正获取了一个对象的复制实体，而不是引用。浅复制 —-只是拷贝了基本类型的数据，而引用类型数据，复制后也是会发生引用，我们把这种拷贝叫做“（浅复制）浅拷贝”，换句话说，浅复制仅仅是指向被复制的内存地址，如果原地址中对象被改变了，那么浅复制出来的对象也会相应改变。深复制 —-在计算机中开辟了一块新的内存地址用于存放复制的对象。

#### **拷贝和赋值的区别**

   拷贝分为:赋值,浅拷贝,深拷贝,拷贝程度一次递增；赋值:赋值就是相当于做了个软连接,所以不管你是修改了链接文联的内容还是源文件的内容,这个文件都会有所改变(相当于镜子里的自己,你动他也动).浅拷贝:只是做了部分的拷贝,何谓部分拷贝?就是在赋值的基础上减去了列表最外层的链接,其他的和赋值基本一样.深拷贝:相当于把文件复制了一份,新文件或老文件的改变都是互不相干的,完全独立于老文件。

#### **int func(int \*a, int \*b)** **和 int func(int a, int b) 的区别**

​    一个用指针做参数，一个用自变量做参数。

#### **面向对象C++的三个特性**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_jpg/gYUsOT36vfqG5ic0dhRjzFbic3Yd4CiaZxkjuZdIicVT0gTiaGTP33eHyu2b3iaVyuhia4BMa5YgnsEvHl4ghKJiaqT5zA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)
    **封装：**把客观事物封装成抽象的类，并且类可以把自己的数据和方法只让可信的类或者对象操作，对不可信的进行信息隐藏。

​    **继承：**面向对象编程 (OOP) 语言的一个主要功能就是“继承”。继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。

​    **多态：**多态性（polymorphisn）是允许你将父对象设置成为和一个或更多的他的子对象相等的技术，赋值之后，父对象就可以根据当前赋值给它的子对象的特性以不同的方式运作。简单的说，就是一句话：允许将子类类型的指针赋值给父类类型的指针。

#### **C++类构造函数和析构函数**

​    构造函数是一个特殊的公共成员函数，它在创建类对象时会自动被调用，用于构造类对象。

​    类的构造函数是类的一种特殊的成员函数，它会在每次创建类的新对象时执行。构造函数的名称与类的名称是完全相同的，并且不会返回任何类型，也不会返回 void。构造函数可用于为某些成员变量设置初始值。

​    类的析构函数是类的一种特殊的成员函数，它会在每次删除所创建的对象时执行。析构函数的名称与类的名称是完全相同的，只是在前面加了个波浪号（~）作为前缀，它不会返回任何值，也不能带有任何参数。析构函数有助于在跳出程序（比如关闭文件、释放内存等）前释放资源。

#### **构造函数和析构函数的作用**

​    构造函数的作用：用于新建对象的初始化工作。

​    析构函数的作用：用于在撤销对象前，完成一些清理工作，比如：释放内存等。

​    每当创建对象时，需要添加初始化代码时，则需要定义自己的构造函数；

​    而对象撤销时，需要自己添加清理工作的代码时，则需要定义自己的析构函数。

#### **C++中虚析构函数的作用**

​    虚析构函数是为了避免内存泄露，而且是当子类中会有指针成员变量时才会使用得到的。也就说虚析构函数使得在删除指向子类对象的基类指针时可以调用子类的析构函数达到释放子类中堆内存的目的，而防止内存泄露的。

#### **C++中malloc和new的区别**

1）malloc与free是C++/C语言的标准库函数，new/delete是C++的运算符。它们都可用于申请动态内存和释放内存。

2）对于非内部数据类型的对象而言，光用maloc/free无法满足动态对象的要求。对象在创建的同时要自动执行构造函数，对象在消亡之前要自动执行析构函数。由于malloc/free是库函数而不是运算符，不在编译器控制权限之内，不能够把执行构造函数和析构函数的任务强加于malloc/free。

3）因此C++语言需要一个能完成动态内存分配和初始化工作的运算符new，以一个能完成清理与释放内存工作的运算符delete。注意new/delete不是库函数。

4）C++程序经常要调用C函数，而C程序只能用malloc/free管理动态内存。

#### **C++中struct和class的区别**

​    对于成员访问权限以及继承方式，class默认都是private，struct默认是public；class可以用于表示模板类型，struct不行；一般来说，用到继承时常用class，没用到继承时则使用struct。

#### **C++ static、const**

​    Static：如果某个属性为整个类所共有，不属于任何一个具体对象，则采用 static 关键字来声明静态成员。

​    Const：常对象是这样的对象：它的数据成员值在对下岗的整个生存周期内不能被改变，也就是说，常对象必须进行初始化而且不能被更新！

#### **什么时候用static：**

​    需要一个数据对象为整个类而非某个对象服务,同时又力求不破坏类的封装性,即要求此成员隐藏在类的内部，对外不可见。

#### **static关键字的作用：**

​    常用来修饰变量。全局变量被static修饰后，就称之为静态全局变量；局部变量被static修饰后，就称之为静态局部变量。统称为静态变量。

#### **const和#define的区别**

​    ①编译器处理方式不同：define宏是在预处理阶段展开，const常量是编译运行阶段使用。

​    ②类型和安全检查不同：define宏没有数据类型，不做任何类型检查，仅仅是展开。const常量有具体的数据类型，在编译阶段会执行类型检查。③存储方式不同：define宏仅仅是展开，有多少地方使用，就展开多少次，不会分配内存。const常量会在内存中分配(可以是堆中也可以是栈中)。④代码调试不同：const常量可以进行调试的。define是不能进行调试的，因为在预编译阶段就已经替换掉了。

#### **C++的栈区和堆区知道吗？**

栈区是存储函数内部变量的内存区，堆区是存动态申请的内存

#### **C++里面vector、list、deque的区别**

​    vector：vector是连续内存空间，支持高效的随机存取和尾部插入删除，对其他位置插入删除不方便，对比数组是可以自由动态增加空间。(伪动态，也是构造新一片内存空间，拷贝旧空间数据到新空间，释放旧空间)

​    deque：逻辑上的连续内存空间，支持高效的随机存取和头尾部双端插入删除。在中间进行插入删除也需要进行大量的数据移动。

​    List：双向循环链表，支持高效的插入和删除，对随机存取不方便。

​    如果想要高效的存取，不在乎插入和删除的效率，选用vector.

​    如果需要大量的删除和插入操作，不关心随机存取，选用list

​    如果需要高效的存取，同时在乎首尾的插入和删除，选用deque.

#### **struct和union有什么区别**

​    ①共用体和结构体都是由多个不同的数据类型成员组成, 但在任何同一时刻, 共用体只存放了一个被选中的成员, 而结构体的所有成员都存在。

​    ②对于共用体的不同成员赋值, 将会对其它成员重写, 原来成员的值就不存在了, 而对于结构体的不同成员赋值是互不影响的。

#### ** C++中include时如何保证不重复加载头文件**

​    \#ifndef方式、#pragma once方式

#### **C++11里面auto有什么用，不给初始值可以吗**

​    C++11中对关键字auto进行了重新定义，可以让编译器根据初始值类型自动推断变量的类型。用auto声明的变量必须初始化

#### **点积和叉乘**

向量的点积：向量点积是其各个分量乘积的和

![image-20210705211643720](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705211643720.png)

几何意义：点积的结果是一个标量，等于向量大小与夹角的cos值的乘积。

![image-20210705211658486](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705211658486.png)

叉乘：

![image-20210705211707495](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210705211707495.png)

#### **C++、Java、Python的主要区别 (编译型语言和解释性语言)**

​    编译型语言：c, c++, Delphi，java,等;解释性语言：c#, python，Ruby， javascript.

#### **STL中 vector 是怎么实现的？**

​    我答了用数组实现，然后常数时间访问，内存分配。内存不够在原有基础上扩大一倍分配。

#### **python和C的区别**

​    ①语言类型：Python是一种基于解释器的语言，解释器会逐行读取代码；首先将Python编译为字节码，然后由大型C程序解释。C是一种编译语言，完整的源代码将直接编译为机器代码，由CPU直接执行。

​    ②内存管理：Python使用自动垃圾收集器进行内存管理。在C语言中，程序员必须自己进行内存管理。

​    ③应用：Python是一种通用编程语言，一个多范式。它主要支持面向对象编程，程序编程，函数编程。C是结构化编程语言。允许使用函数，选择（if / else等），迭代（循环）。它主要用于硬件相关的应用程序。

​    ④速度：Python编程语言因为历史原因，有一个GIL锁，导致其对多线程支持不够好，运行速度较慢；而C语言很快，C语言是比较底层的语言，运行效率上要优于Python。

​    ⑤复杂度不一样：在Python中，不需要声明变量类型。而在C中，必须声明变量类型。Python程序更易于学习，编写和阅读。而C程序语法比Python更难。Python中的测试和调试更容易；而在C中测试和调试更难。

#### **什么时候要进行动态内存申请？**

​    当无法事先确定对象需要使用多少内存（这些对象所需的内存大小只有在程序运行的时候才能确定）时就要申请动态内存，比如维护一个动态增长的链表或树。

#### **C++内存泄漏和野指针怎么解决**

​    内存泄漏：①访问已经释放的内存②访问没有权限的内存。

​    野指针：指向内存被释放的内存或者没有访问权限的内存的指针。

​    野指针可以使用shared_ptr和weak_ptr结合使用来尽量规避，释放内存后把指针指向NULL，防止指针在后面不小心又被解引用了。

​    内存泄漏的解决：使用智能指针。因为智能指针可以自动删除分配的内存。

#### **C++模板的作用**

​    模板（Template）指C++程序设计设计语言中采用类型作为参数的程序设计，支持通用程序设计。通常有两种形式：函数模板和类模板；

### 操作系统

#### **线程和进程的区别？**

​    **进程：** 是执行中一段程序，即一旦程序被载入到内存中并准备执行，它就是一个进程。进程是表示资源分配的基本概念，又是调度运行的基本单位，是系统中的并发执行的单位。

​    进程是程序的一次执行过程，是一个动态概念，是程序在执行过程中分配和管理资源的基本单位，每一个进程都有一个自己的地址空间，至少有 5 种基本状态，它们是：初始态，执行态，等待状态，就绪状态，终止状态。

​    **线程：**单个进程中执行中每个任务就是一个线程。线程是进程中执行运算的最小单位。

​    线程是CPU调度和分派的基本单位，它可与同属一个进程的其他的线程共享进程所拥有的全部资源。

​    一个线程只能属于一个进程，但是一个进程可以拥有多个线程。多线程处理就是允许一个进程中在同一时刻执行多个任务。

​    **相同点**：进程和线程都有ID/寄存器组、状态和优先权、信息块，创建后都可更改自己的属性，都可与父进程共享资源、都不能直接访问其他无关进程或线程的资源。

​    **联系**：线程是进程的一部分，一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。

​    **区别：** 

1. 一个程序至少有一个进程,一个进程至少有一个线程
2.  线程的划分尺度小于进程，使得多线程程序的并发性高
3. 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率
4. 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制 
5. 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。

#### **什么时候用线程什么时候进程**

​    进程与线程的选择取决以下几点：①需要频繁创建销毁的优先使用线程；因为对进程来说创建和销毁一个进程代价是很大的。②线程的切换速度快，所以在需要大量计算，切换频繁时用线程，还有耗时的操作使用线程可提高应用程序的响应。③因为对CPU系统的效率使用上线程更占优，所以可能要发展到多机分布的用进程，多核分布用线程。④并行操作时使用线程，如C/S架构的服务器端并发线程响应用户的请求。⑤需要更稳定安全时，适合选择进程；需要速度时，选择线程更好。

#### **线程有哪些状态**

​    5种基本状态，它们是：初始态，执行态，等待状态，就绪状态，终止状态。

#### **那进程间通信的方式？线程可以通信吗？**

​    进程间通信（IPC，InterProcess Communication）是指在不同进程之间传播或交换信息。IPC的方式通常有管道（包括无名管道和命名管道）、消息队列、信号量、共享存储、Socket、Streams等。其中 Socket和Streams支持不同主机上的两个进程IPC。

线程通信常用的方式有:wait/notify 等待、Volatile 内存共享、CountDownLatch 并发工具、CyclicBarrier 并发工具

#### **多线程多进程**

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/gYUsOT36vfr6BuFW1kTLun5z5Crfz5HtoAPkvuK8oFPhUKGtjOWQEFAMK4PBukdUvyvEd3JguhiafNTiap4YQr6A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### **阻塞与非阻塞**

​    **同步：**所谓同步，就是在发出一个功能调用时，在没有得到结果之前，该调用就不返回。也就是必须一件一件事做,等前一件做完了才能做下一件事。

例如普通B/S模式（同步）：提交请求->等待服务器处理->处理完毕返回 这个期间客户端浏览器不能干任何事

​    **异步：**异步的概念和同步相对。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。

​    例如 ajax请求（异步）: 请求通过事件触发->服务器处理（这是浏览器仍然可以作其他事情）->处理完毕

​    **阻塞：**阻塞调用是指调用结果返回之前，当前线程会被挂起（线程进入非可执行状态，在这个状态下，cpu不会给线程分配时间片，即线程暂停运行）。函数只有在得到结果之后才会返回。

   有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回,它还会抢占cpu去执行其他逻辑，也会主动检测io是否准备好。

   **非阻塞：**非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。

1. 同步，就是我调用一个功能，该功能没有结束前，我死等结果。
2. 异步，就是我调用一个功能，不需要知道该功能结果，该功能有结果后通知我（回调通知）
3. 阻塞，就是调用我（函数），我（函数）没有接收完数据或者没有得到结果之前，我不会返回。
4. 非阻塞，就是调用我（函数），我（函数）立即返回，通过select通知调用者

#### **五种IO模型**

1. 阻塞I/O（blocking I/O）

2. 非阻塞I/O （nonblocking I/O）
3. I/O复用(select 和poll) （I/O multiplexing）
4. 信号驱动I/O （signal driven I/O (SIGIO)）
5. 异步I/O （asynchronous I/O (the POSIX aio_functions)）

### LINUX

#### **Linux的一些常用命令**

①重启reboot

②关机poweroff、shutdown -h now

③查看本机ip信息的名称ifconfig

④vi和vim编辑器

一般模式，插入模式，底行模式

一般模式(通过按iaoIAO键)-->插入模式  插入模式(按Esc键)--> 一般模式

一般模式（通过按:键）-->底行模式  底行模式(按Esc键)--> 一般模式

底行模式中，wq = write quit 写入并退出

wq! 如果有不能保存退出的情况可以使用wq! ! 强制退出

q! = quit !强制 不写入强制退出

⑤  查看目录下的内容

ls = list

语法：

ls [目录名称]

实例：

ls 查看当前目录下的所有内容

ls /etc 查看etc目录下的所有内容（绝对路径）

目录下的所有文件

ls spring/ 当前目录下存在spring可以使用相对路径查看

ls spring/springmvc

-a 查看目录下所有的文件，包括隐藏文件

-l 以长格式显示目录下的所有文件（显示文件或者目录的详细信息）

ls -l 可以简化为 ll

-t 按更新时间倒叙排序显示目录下的内容

ls -a /etc

ls -l /etc

ls -l -t /etc  等同于 ls -lt /etc

⑥切换目录  cd = change directory

⑦删除文件  rm =remove

#### **2Linux中grep命令详解**

​    Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。

grep [options]

主要参数：grep --help可查看

-c：只输出匹配行的计数。

-i：不区分大小写。

-h：查询多文件时不显示文件名。

-l：查询多文件时只输出包含匹配字符的文件名。

-n：显示匹配行及 行号。

-s：不显示不存在或无匹配文本的错误信息。

-v：显示不包含匹配文本的所有行。

--color=auto ：可以将找到的关键词部分加上颜色的显示。

#### **按时间顺序打印出文件列表，按文件大小打印文件列表**

ls

按大小排序：[root@localhost ~]# ls -Sh

\#按时间排序:[root@localhost ~]# ls -rt

#### **Linux如何查看某进程关联的相关文件有哪些？**

lsof命令是什么？可以列出被进程所打开的文件的信息。

#### **Linux启动的过程**

Linux系统的启动过程并不是大家想象中的那么复杂，其过程可以分为5个阶段：

内核的引导。运行 init。系统初始化。建立终端 。用户登录系统。linux如何查看进程

#### **linux查看线程用哪个命令**

1.使用top命令，具体用法是 top –H  加上这个选项，top的每一行就不是显示一个进程，而是一个线程。

2.使用ps命令，具体用法是 ps –xH

这样可以查看所有存在的线程，也可以使用grep作进一步的过滤。

3.使用ps命令，具体用法是 ps -mq PID 这样可以看到指定的进程产生的线程数目。

### 手撕代码篇

#### **计算卷积网络输出尺寸**

​    卷积神经网络的计算公式为：N=(W-F+2P)/S+1 其中N：输出大小 W：输入大小 F：卷积核大小 P：填充值的大小 S：步长大小

#### **NMS**

```python
import numpy as np
def py_cpu_nms(dets, thresh):
    """Pure Python NMS baseline."""
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]  #[::-1]表示降序排序，输出为其对应序号
    keep = []                     #需要保留的bounding box
    while order.size > 0:
        i = order[0]              #取置信度最大的（即第一个）框
        keep.append(i)            #将其作为保留的框
        #以下计算置信度最大的框（order[0]）与其它所有的框（order[1:]，即第二到最后一个）框的IOU，以下都是以向量形式表示和计算
        xx1 = np.maximum(x1[i], x1[order[1:]]) #计算xmin的max,即overlap的xmin
        yy1 = np.maximum(y1[i], y1[order[1:]]) #计算ymin的max,即overlap的ymin
        xx2 = np.minimum(x2[i], x2[order[1:]]) #计算xmax的min,即overlap的xmax
        yy2 = np.minimum(y2[i], y2[order[1:]]) #计算ymax的min,即overlap的ymax
        w = np.maximum(0.0, xx2 - xx1 + 1)      #计算overlap的width
        h = np.maximum(0.0, yy2 - yy1 + 1)      #计算overlap的hight
        inter = w * h                           #计算overlap的面积
        ovr = inter / (areas[i] + areas[order[1:]] - inter) #计算并，-inter是因为交集部分加了两次。
        inds = np.where(ovr <= thresh)[0]#本轮，order仅保留IOU不大于阈值的下标
        order = order[inds + 1]                    #删除IOU大于阈值的框
return keep
```

#### **手撕SoftNMS**

```python

import numpy as np
def soft_nms(dets, sigma=0.5, Nt=0.5, method=2, threshold=0.1):
    box_len = len(dets)   # box的个数
    for i in range(box_len):
        tmpx1, tmpy1, tmpx2, tmpy2, ts = dets[i, 0], dets[i, 1], dets[i, 2], dets[i, 3], dets[i, 4]
        max_pos = i
        max_scores = ts
        # get max box
        pos = i+1
        while pos < box_len:
            if max_scores < dets[pos, 4]:
                max_scores = dets[pos, 4]
                max_pos = pos
            pos += 1
        # add max box as a detection
        dets[i, :] = dets[max_pos, :]
        # swap ith box with position of max box
        dets[max_pos, 0] = tmpx1
        dets[max_pos, 1] = tmpy1
        dets[max_pos, 2] = tmpx2
        dets[max_pos, 3] = tmpy2
        dets[max_pos, 4] = ts
        # 将置信度最高的 box 赋给临时变量
        tmpx1, tmpy1, tmpx2, tmpy2, ts = dets[i, 0], dets[i, 1], dets[i, 2], dets[i, 3], dets[i, 4]
        pos = i+1
        # NMS iterations, note that box_len changes if detection boxes fall below threshold
        while pos < box_len:
            x1, y1, x2, y2 = dets[pos, 0], dets[pos, 1], dets[pos, 2], dets[pos, 3]
            area = (x2 - x1 + 1)*(y2 - y1 + 1)
            iw = (min(tmpx2, x2) - max(tmpx1, x1) + 1)
            ih = (min(tmpy2, y2) - max(tmpy1, y1) + 1)
            if iw > 0 and ih > 0:
                overlaps = iw * ih
                ious = overlaps / ((tmpx2 - tmpx1 + 1) * (tmpy2 - tmpy1 + 1) + area - overlaps)
                if method == 1:    # 线性
                    if ious > Nt:
                        weight = 1 - ious
                    else:
                        weight = 1
                elif method == 2:  # gaussian
                    weight = np.exp(-(ious**2) / sigma)
                else:              # original NMS
                    if ious > Nt:
                        weight = 0
                    else:
                        weight = 1
                # 赋予该box新的置信度
                dets[pos, 4] = weight * dets[pos, 4]
                # 如果box得分低于阈值thresh，则通过与最后一个框交换来丢弃该框
                if dets[pos, 4] < threshold:
                    dets[pos, 0] = dets[box_len-1, 0]
                    dets[pos, 1] = dets[box_len-1, 1]
                    dets[pos, 2] = dets[box_len-1, 2]
                    dets[pos, 3] = dets[box_len-1, 3]
                    dets[pos, 4] = dets[box_len-1, 4]
                    box_len = box_len-1
                    pos = pos-1
            pos += 1
    keep = [i for i in range(box_len)]
    return keep
if __name__ == '__main__':
    dets = [[0, 0, 100, 101, 0.9], [5, 6, 90, 110, 0.7], [17, 19, 80, 120, 0.8], [10, 8, 115, 105, 0.5]]
    dets = np.array(dets)
    result = soft_nms(dets, 0.5)
    print(result)
```



#### **手写计算IOU代码**

```python

def IOU(x1,y1,X1,Y1, x2,y2,X2,Y2):
    xx = max(x1,x2)
    XX = min(X1,X2)
    yy = max(y1,y2)
    YY = min(Y1,Y2)
    m = max(0., XX-xx)
    n = max(0., YY-yy)
    Jiao = m*n
    Bing = (X1-x1)*(Y1-y1)+(X2-x2)*(Y2-y2)-Jiao
return Jiao/Bing

def bb_intersection_over_union(boxA, boxB):
    boxA = [int(x) for x in boxA]
    boxB = [int(x) for x in boxB]
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)
    iou = interArea / float(boxAArea + boxBArea - interArea)
return iou
```

#### **手写k-means**

```python
import pandas as pd
import numpy as np
import random as ran
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d # 
from sklearn.cluster import KMeans
 
def model_test():
    data = open_file("C:\\Users\\happy\\Desktop\\Iris1.csv")
    dataset = np.delete(data,-1,axis=1) #去掉最后一列
    k_means = KMeans(n_clusters=3) #构建模型
    k_means.fit(dataset)
    km4_labels = k_means.labels_
    ax = plt.subplot(projection='3d')
    ax.scatter(dataset[:,0],dataset[:,1],dataset[:,2],\
               c=km4_labels.astype(np.float))
    ax.set_zlabel('Z')  # 坐标轴
    ax.set_ylabel('Y')
    ax.set_xlabel('X')
    plt.show()
```

